# é‡æ„åçš„ server.py - ä¸»æœåŠ¡å™¨æ–‡ä»¶
# è´Ÿè´£åº”ç”¨å¯åŠ¨ã€ç”Ÿå‘½å‘¨æœŸç®¡ç†å’Œæ¨¡å—åè°ƒ

import asyncio
import os
import sys
import multiprocessing
from contextlib import asynccontextmanager

from fastapi import FastAPI

# å¯¼å…¥é‡æ„åçš„æ¨¡å—
from config import *
from logging_utils import (
    setup_server_logging, restore_original_streams,
    WebSocketConnectionManager, log_ws_manager
)
from browser_manager import (
    initialize_browser_and_page, cleanup_browser_and_page
)
from model_manager import (
    parsed_model_list, wait_for_model_list, initialize_model_manager,
    handle_initial_model_state_and_storage
)
from queue_manager import start_queue_worker, cleanup_queue_worker
from routes import setup_routes
import stream

# åˆå§‹åŒ–å…¨å±€æ—¥å¿—ç®¡ç†å™¨
if log_ws_manager is None:
    import logging_utils
    logging_utils.log_ws_manager = WebSocketConnectionManager()

import logging
logger = logging.getLogger("AIStudioProxyServer")

# å…¨å±€å˜é‡ç”¨äºç®¡ç†æµå¼ä»£ç†æœåŠ¡å™¨
stream_proxy_process = None


def start_stream_proxy_server():
    """å¯åŠ¨æµå¼ä»£ç†æœåŠ¡å™¨"""
    global stream_proxy_process

    # ä»ç¯å¢ƒå˜é‡è·å–æµå¼ä»£ç†ç«¯å£
    stream_port = int(os.environ.get('STREAM_PORT', '3120'))

    # å¦‚æœç«¯å£ä¸º0ï¼Œåˆ™ç¦ç”¨æµå¼ä»£ç†
    if stream_port == 0:
        logger.info("æµå¼ä»£ç†æœåŠ¡å™¨å·²ç¦ç”¨ (STREAM_PORT=0)")
        return False

    try:
        logger.info(f"å¯åŠ¨æµå¼ä»£ç†æœåŠ¡å™¨ (ç«¯å£: {stream_port})...")

        # è·å–ä¸Šæ¸¸ä»£ç†é…ç½®
        upstream_proxy = os.environ.get('HTTPS_PROXY') or os.environ.get('HTTP_PROXY')

        # åˆ›å»ºæµé˜Ÿåˆ—ï¼ˆå…³é”®ä¿®å¤ï¼šä¸é‡æ„å‰ä¿æŒä¸€è‡´ï¼‰
        import config
        config.STREAM_QUEUE = multiprocessing.Queue()
        logger.info("âœ… æµé˜Ÿåˆ—å·²åˆ›å»º")

        # å¯åŠ¨æµå¼ä»£ç†æœåŠ¡å™¨è¿›ç¨‹ï¼ˆä¼ é€’é˜Ÿåˆ—ï¼‰
        stream_proxy_process = multiprocessing.Process(
            target=stream.start,
            kwargs={
                'queue': config.STREAM_QUEUE,  # ä¿®å¤ï¼šä¼ é€’å®é™…çš„é˜Ÿåˆ—è€Œä¸æ˜¯None
                'port': stream_port,
                'proxy': upstream_proxy
            }
        )
        stream_proxy_process.start()

        # è®¾ç½®æµè¿›ç¨‹åˆ°é…ç½®ä¸­
        config.STREAM_PROCESS = stream_proxy_process

        logger.info(f"âœ… æµå¼ä»£ç†æœåŠ¡å™¨å·²å¯åŠ¨ (PID: {stream_proxy_process.pid}, ç«¯å£: {stream_port})")

        # æ›´æ–°é…ç½®ä»¥ä½¿ç”¨å¯åŠ¨çš„ä»£ç†æœåŠ¡å™¨
        config.PROXY_SERVER_ENV = f"http://127.0.0.1:{stream_port}/"
        config.PLAYWRIGHT_PROXY_SETTINGS = {'server': config.PROXY_SERVER_ENV}

        logger.info(f"âœ… å·²æ›´æ–° Playwright ä»£ç†é…ç½®: {config.PLAYWRIGHT_PROXY_SETTINGS}")
        logger.info(f"âœ… æµé˜Ÿåˆ—å’Œæµè¿›ç¨‹å·²æ­£ç¡®åˆå§‹åŒ–")

        return True

    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨æµå¼ä»£ç†æœåŠ¡å™¨å¤±è´¥: {e}", exc_info=True)
        return False


def stop_stream_proxy_server():
    """åœæ­¢æµå¼ä»£ç†æœåŠ¡å™¨"""
    global stream_proxy_process

    if stream_proxy_process and stream_proxy_process.is_alive():
        try:
            logger.info("æ­£åœ¨åœæ­¢æµå¼ä»£ç†æœåŠ¡å™¨...")
            stream_proxy_process.terminate()
            stream_proxy_process.join(timeout=5)

            if stream_proxy_process.is_alive():
                logger.warning("æµå¼ä»£ç†æœåŠ¡å™¨æœªåœ¨è¶…æ—¶æ—¶é—´å†…åœæ­¢ï¼Œå¼ºåˆ¶ç»ˆæ­¢...")
                stream_proxy_process.kill()
                stream_proxy_process.join()

            logger.info("âœ… æµå¼ä»£ç†æœåŠ¡å™¨å·²åœæ­¢")

        except Exception as e:
            logger.error(f"âŒ åœæ­¢æµå¼ä»£ç†æœåŠ¡å™¨æ—¶å‡ºé”™: {e}", exc_info=True)
        finally:
            stream_proxy_process = None

    # æ¸…ç†æµé˜Ÿåˆ—å’Œæµè¿›ç¨‹ï¼ˆå…³é”®ä¿®å¤ï¼šä¸é‡æ„å‰ä¿æŒä¸€è‡´ï¼‰
    import config
    if config.STREAM_QUEUE:
        try:
            # æ¸…ç©ºé˜Ÿåˆ—
            while not config.STREAM_QUEUE.empty():
                try:
                    config.STREAM_QUEUE.get_nowait()
                except:
                    break
            logger.info("âœ… æµé˜Ÿåˆ—å·²æ¸…ç©º")
        except Exception as e:
            logger.warning(f"æ¸…ç©ºæµé˜Ÿåˆ—æ—¶å‡ºé”™: {e}")
        finally:
            config.STREAM_QUEUE = None

    # æ¸…ç†æµè¿›ç¨‹å¼•ç”¨
    config.STREAM_PROCESS = None
    logger.info("âœ… æµé˜Ÿåˆ—å’Œæµè¿›ç¨‹å·²æ¸…ç†")


# --- åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç† ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""
    logger.info("=" * 50)
    logger.info("ğŸš€ AI Studio Proxy Server æ­£åœ¨å¯åŠ¨...")
    logger.info("=" * 50)
    
    # å¯åŠ¨æ—¶çš„åˆå§‹åŒ–
    original_stdout, original_stderr = None, None
    
    try:
        # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
        log_level = os.environ.get('SERVER_LOG_LEVEL', 'INFO')
        redirect_print = os.environ.get('SERVER_REDIRECT_PRINT', 'false')
        original_stdout, original_stderr = setup_server_logging(log_level, redirect_print)

        # å¯åŠ¨æµå¼ä»£ç†æœåŠ¡å™¨
        stream_success = start_stream_proxy_server()
        if stream_success:
            logger.info("âœ… æµå¼ä»£ç†æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
            # ç­‰å¾…ä¸€ä¸‹è®©ä»£ç†æœåŠ¡å™¨å®Œå…¨å¯åŠ¨
            await asyncio.sleep(2)
        else:
            logger.warning("âš ï¸ æµå¼ä»£ç†æœåŠ¡å™¨å¯åŠ¨å¤±è´¥ï¼Œå°†ä½¿ç”¨ç›´æ¥è¿æ¥æ¨¡å¼")

        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        initialize_model_manager()

        # å¯åŠ¨é˜Ÿåˆ—å·¥ä½œå™¨
        start_queue_worker()

        # åˆå§‹åŒ–æµè§ˆå™¨å’Œé¡µé¢
        browser_success = await initialize_browser_and_page()
        
        if browser_success:
            # ç­‰å¾…æ¨¡å‹åˆ—è¡¨åŠ è½½
            logger.info("ç­‰å¾…æ¨¡å‹åˆ—è¡¨åŠ è½½...")
            model_success = await wait_for_model_list(timeout=30.0)
            
            if model_success:
                logger.info(f"âœ… æ¨¡å‹åˆ—è¡¨åŠ è½½æˆåŠŸï¼Œå…± {len(parsed_model_list)} ä¸ªæ¨¡å‹")
                
                # å¤„ç†åˆå§‹æ¨¡å‹çŠ¶æ€
                await handle_initial_model_state_and_storage()
            else:
                logger.warning("âš ï¸ æ¨¡å‹åˆ—è¡¨åŠ è½½å¤±è´¥æˆ–è¶…æ—¶")
        else:
            logger.warning("âš ï¸ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥")
        
        logger.info("=" * 50)
        logger.info("âœ… AI Studio Proxy Server å¯åŠ¨å®Œæˆ")
        logger.info("=" * 50)
        
        yield  # åº”ç”¨è¿è¡ŒæœŸé—´
        
    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise
    finally:
        # å…³é—­æ—¶çš„æ¸…ç†
        logger.info("=" * 50)
        logger.info("ğŸ›‘ AI Studio Proxy Server æ­£åœ¨å…³é—­...")
        logger.info("=" * 50)
        
        try:
            # æ¸…ç†é˜Ÿåˆ—å·¥ä½œå™¨
            await cleanup_queue_worker()

            # æ¸…ç†æµè§ˆå™¨å’Œé¡µé¢
            await cleanup_browser_and_page()

            # åœæ­¢æµå¼ä»£ç†æœåŠ¡å™¨
            stop_stream_proxy_server()

            # æ¢å¤åŸå§‹æµ
            if original_stdout and original_stderr:
                restore_original_streams(original_stdout, original_stderr)

            logger.info("âœ… AI Studio Proxy Server å·²å®‰å…¨å…³é—­")
            
        except Exception as e:
            logger.error(f"âŒ å…³é—­è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


# --- FastAPI åº”ç”¨åˆ›å»º ---
def create_app() -> FastAPI:
    """åˆ›å»º FastAPI åº”ç”¨å®ä¾‹"""
    app = FastAPI(
        title="AI Studio Proxy Server",
        description="AI Studio ä»£ç†æœåŠ¡å™¨ï¼Œæä¾› OpenAI å…¼å®¹çš„ API",
        version="1.0.0",
        lifespan=lifespan
    )
    
    # è®¾ç½®è·¯ç”±
    setup_routes(app)
    
    return app


# --- åº”ç”¨å®ä¾‹ ---
app = create_app()


if __name__ == "__main__":
    import uvicorn
    
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    host = os.environ.get('SERVER_HOST', '127.0.0.1')
    port = int(os.environ.get('SERVER_PORT', '8000'))
    
    print(f"å¯åŠ¨æœåŠ¡å™¨: {host}:{port}")
    
    uvicorn.run(
        "server:app",
        host=host,
        port=port,
        reload=False,
        log_level="warning"  # ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„æ—¥å¿—ç³»ç»Ÿ
    )
