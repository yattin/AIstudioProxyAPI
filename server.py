import asyncio
import multiprocessing
import random
import time
import json
from typing import List, Optional, Dict, Any, Union, AsyncGenerator, Tuple, Callable, Set
import os

from contextlib import asynccontextmanager
import sys

import logging
import logging.handlers

from asyncio import Queue, Lock, Future, Task, Event

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse, StreamingResponse, FileResponse
from fastapi import WebSocket, WebSocketDisconnect
from pydantic import BaseModel
from playwright.async_api import Page as AsyncPage, Browser as AsyncBrowser, Playwright as AsyncPlaywright, Error as PlaywrightAsyncError, expect as expect_async, BrowserContext as AsyncBrowserContext, Locator, TimeoutError
from playwright.async_api import async_playwright
from urllib.parse import urljoin, urlparse
import uuid
import datetime
import aiohttp
import stream
import queue


# --- stream queue ---
STREAM_QUEUE:Optional[multiprocessing.Queue] = None
STREAM_PROCESS = None

STREAM_TIMEOUT_LOG_STATE = {
    "consecutive_timeouts": 0,
    "last_error_log_time": 0.0, # ä½¿ç”¨ time.monotonic()
    "suppress_until_time": 0.0, # ä½¿ç”¨ time.monotonic()
    "max_initial_errors": 3,
    "warning_interval_after_suppress": 60.0, # seconds
    "suppress_duration_after_initial_burst": 300.0, # seconds
}

# --- å…¨å±€æ·»åŠ æ ‡è®°å¸¸é‡ ---
USER_INPUT_START_MARKER_SERVER = "__USER_INPUT_START__"
USER_INPUT_END_MARKER_SERVER = "__USER_INPUT_END__"

# --- å…¨å±€æ—¥å¿—æ§åˆ¶é…ç½® ---
DEBUG_LOGS_ENABLED = os.environ.get('DEBUG_LOGS_ENABLED', 'false').lower() in ('true', '1', 'yes')
TRACE_LOGS_ENABLED = os.environ.get('TRACE_LOGS_ENABLED', 'false').lower() in ('true', '1', 'yes')

# --- Configuration ---
AI_STUDIO_URL_PATTERN = 'aistudio.google.com/'
RESPONSE_COMPLETION_TIMEOUT = 300000 # 5 minutes total timeout (in ms)
INITIAL_WAIT_MS_BEFORE_POLLING = 500 # ms, initial wait before polling for response completion
POLLING_INTERVAL = 300 # ms
POLLING_INTERVAL_STREAM = 180 # ms
SILENCE_TIMEOUT_MS = 40000 # ms
POST_SPINNER_CHECK_DELAY_MS = 500
FINAL_STATE_CHECK_TIMEOUT_MS = 1500
POST_COMPLETION_BUFFER = 700
CLEAR_CHAT_VERIFY_TIMEOUT_MS = 5000
CLEAR_CHAT_VERIFY_INTERVAL_MS = 400
CLICK_TIMEOUT_MS = 5000
CLIPBOARD_READ_TIMEOUT_MS = 5000
PSEUDO_STREAM_DELAY = 0.01
EDIT_MESSAGE_BUTTON_SELECTOR = 'ms-chat-turn:last-child .actions-container button.toggle-edit-button'
MESSAGE_TEXTAREA_SELECTOR = 'ms-chat-turn:last-child ms-text-chunk ms-autosize-textarea'
FINISH_EDIT_BUTTON_SELECTOR = 'ms-chat-turn:last-child .actions-container button.toggle-edit-button[aria-label="Stop editing"]'

AUTH_PROFILES_DIR = os.path.join(os.path.dirname(__file__), 'auth_profiles')
ACTIVE_AUTH_DIR = os.path.join(AUTH_PROFILES_DIR, 'active')
SAVED_AUTH_DIR = os.path.join(AUTH_PROFILES_DIR, 'saved')
LOG_DIR = os.path.join(os.path.dirname(__file__), 'logs')
APP_LOG_FILE_PATH = os.path.join(LOG_DIR, 'app.log')

# --- å…¨å±€ä»£ç†è®¾ç½® ---

PROXY_SERVER_ENV = "http://127.0.0.1:3120/"
STREAM_PROXY_SERVER_ENV = os.environ.get('HTTPS_PROXY') or os.environ.get('HTTP_PROXY')

NO_PROXY_ENV = os.environ.get('NO_PROXY')
AUTO_SAVE_AUTH = os.environ.get('AUTO_SAVE_AUTH', '').lower() in ('1', 'true', 'yes')
AUTH_SAVE_TIMEOUT = int(os.environ.get('AUTH_SAVE_TIMEOUT', '30'))

PLAYWRIGHT_PROXY_SETTINGS: Optional[Dict[str, str]] = None
if PROXY_SERVER_ENV:
    PLAYWRIGHT_PROXY_SETTINGS = {'server': PROXY_SERVER_ENV}
    if NO_PROXY_ENV:
        PLAYWRIGHT_PROXY_SETTINGS['bypass'] = NO_PROXY_ENV.replace(',', ';')

# --- Constants ---
MODEL_NAME = 'AI-Studio_Camoufox-Proxy'
CHAT_COMPLETION_ID_PREFIX = 'chatcmpl-'
MODELS_ENDPOINT_URL_CONTAINS = "MakerSuiteService/ListModels"
DEFAULT_FALLBACK_MODEL_ID = "no model list"

# --- Selectors ---
PROMPT_TEXTAREA_SELECTOR = 'ms-prompt-input-wrapper ms-autosize-textarea textarea'
INPUT_SELECTOR = PROMPT_TEXTAREA_SELECTOR
INPUT_SELECTOR2 = PROMPT_TEXTAREA_SELECTOR
SUBMIT_BUTTON_SELECTOR = 'button[aria-label="Run"].run-button'
RESPONSE_CONTAINER_SELECTOR = 'ms-chat-turn .chat-turn-container.model'
RESPONSE_TEXT_SELECTOR = 'ms-cmark-node.cmark-node'
LOADING_SPINNER_SELECTOR = 'button[aria-label="Run"].run-button svg .stoppable-spinner'
OVERLAY_SELECTOR = 'div.cdk-overlay-backdrop'
WAIT_FOR_ELEMENT_TIMEOUT_MS = 10000 # Timeout for waiting for elements like overlays
ERROR_TOAST_SELECTOR = 'div.toast.warning, div.toast.error'
CLEAR_CHAT_BUTTON_SELECTOR = 'button[data-test-clear="outside"][aria-label="Clear chat"]'
CLEAR_CHAT_CONFIRM_BUTTON_SELECTOR = 'button.mdc-button:has-text("Continue")'
MORE_OPTIONS_BUTTON_SELECTOR = 'div.actions-container div ms-chat-turn-options div > button'
COPY_MARKDOWN_BUTTON_SELECTOR = 'button.mat-mdc-menu-item:nth-child(4)'
COPY_MARKDOWN_BUTTON_SELECTOR_ALT = 'div[role="menu"] button:has-text("Copy Markdown")'
MAX_OUTPUT_TOKENS_SELECTOR = 'input[aria-label="Maximum output tokens"]'
STOP_SEQUENCE_INPUT_SELECTOR = 'input[aria-label="Add stop token"]'
MAT_CHIP_REMOVE_BUTTON_SELECTOR = 'mat-chip-set mat-chip-row button[aria-label*="Remove"]'
TOP_P_INPUT_SELECTOR = 'div.settings-item-column:has(h3:text-is("Top P")) input[type="number"].slider-input'
TEMPERATURE_INPUT_SELECTOR = 'div[data-test-id="temperatureSliderContainer"] input[type="number"].slider-input'


# --- Global State ---
playwright_manager: Optional[AsyncPlaywright] = None
browser_instance: Optional[AsyncBrowser] = None
page_instance: Optional[AsyncPage] = None
is_playwright_ready = False
is_browser_connected = False
is_page_ready = False
is_initializing = False

global_model_list_raw_json: Optional[List[Any]] = None
parsed_model_list: List[Dict[str, Any]] = []
model_list_fetch_event = asyncio.Event()

current_ai_studio_model_id: Optional[str] = None
model_switching_lock: Optional[Lock] = None

excluded_model_ids: Set[str] = set()
EXCLUDED_MODELS_FILENAME = "excluded_models.txt"

request_queue: Optional[Queue] = None
processing_lock: Optional[Lock] = None
worker_task: Optional[Task] = None

page_params_cache: Dict[str, Any] = {}
params_cache_lock: Optional[Lock] = None

logger = logging.getLogger("AIStudioProxyServer")
log_ws_manager = None

# --- æ—¥å¿—è®¾ç½®å‡½æ•° ---
def setup_server_logging(log_level_name: str = "INFO", redirect_print_str: str = "false"):
    global logger, log_ws_manager
    log_level = getattr(logging, log_level_name.upper(), logging.INFO)
    redirect_print = redirect_print_str.lower() in ('true', '1', 'yes')
    os.makedirs(LOG_DIR, exist_ok=True)
    os.makedirs(ACTIVE_AUTH_DIR, exist_ok=True)
    os.makedirs(SAVED_AUTH_DIR, exist_ok=True)
    file_log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - [%(name)s:%(funcName)s:%(lineno)d] - %(message)s')
    if logger.hasHandlers():
        logger.handlers.clear()
    logger.setLevel(log_level)
    logger.propagate = False
    if os.path.exists(APP_LOG_FILE_PATH):
        try:
            os.remove(APP_LOG_FILE_PATH)
        except OSError as e:
            print(f"è­¦å‘Š (setup_server_logging): å°è¯•ç§»é™¤æ—§çš„ app.log æ–‡ä»¶ '{APP_LOG_FILE_PATH}' å¤±è´¥: {e}ã€‚å°†ä¾èµ– mode='w' è¿›è¡Œæˆªæ–­ã€‚", file=sys.__stderr__)
    file_handler = logging.handlers.RotatingFileHandler(
        APP_LOG_FILE_PATH, maxBytes=5*1024*1024, backupCount=5, encoding='utf-8', mode='w'
    )
    file_handler.setFormatter(file_log_formatter)
    logger.addHandler(file_handler)
    if log_ws_manager is None:
        print("ä¸¥é‡è­¦å‘Š (setup_server_logging): log_ws_manager æœªåˆå§‹åŒ–ï¼WebSocket æ—¥å¿—åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚", file=sys.__stderr__)
    else:
        ws_handler = WebSocketLogHandler(log_ws_manager)
        ws_handler.setLevel(logging.INFO)
        logger.addHandler(ws_handler)
    console_server_log_formatter = logging.Formatter('%(asctime)s - %(levelname)s [SERVER] - %(message)s')
    console_handler = logging.StreamHandler(sys.stderr)
    console_handler.setFormatter(console_server_log_formatter)
    console_handler.setLevel(log_level)
    logger.addHandler(console_handler)
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    if redirect_print:
        print("--- æ³¨æ„ï¼šserver.py æ­£åœ¨å°†å…¶ print è¾“å‡ºé‡å®šå‘åˆ°æ—¥å¿—ç³»ç»Ÿ (æ–‡ä»¶ã€WebSocket å’Œæ§åˆ¶å°è®°å½•å™¨) ---", file=original_stderr)
        stdout_redirect_logger = logging.getLogger("AIStudioProxyServer.stdout")
        stdout_redirect_logger.setLevel(logging.INFO)
        stdout_redirect_logger.propagate = True
        sys.stdout = StreamToLogger(stdout_redirect_logger, logging.INFO)
        stderr_redirect_logger = logging.getLogger("AIStudioProxyServer.stderr")
        stderr_redirect_logger.setLevel(logging.ERROR)
        stderr_redirect_logger.propagate = True
        sys.stderr = StreamToLogger(stderr_redirect_logger, logging.ERROR)
    else:
        print("--- server.py çš„ print è¾“å‡ºæœªè¢«é‡å®šå‘åˆ°æ—¥å¿—ç³»ç»Ÿ (å°†ä½¿ç”¨åŸå§‹ stdout/stderr) ---", file=original_stderr)
    logging.getLogger("uvicorn").setLevel(logging.WARNING)
    logging.getLogger("uvicorn.error").setLevel(logging.INFO)
    logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
    logging.getLogger("websockets").setLevel(logging.WARNING)
    logging.getLogger("playwright").setLevel(logging.WARNING)
    logging.getLogger("asyncio").setLevel(logging.ERROR) # <-- æ·»åŠ æ­¤è¡Œ
    logger.info("=" * 5 + " AIStudioProxyServer æ—¥å¿—ç³»ç»Ÿå·²åœ¨ lifespan ä¸­åˆå§‹åŒ– " + "=" * 5)
    logger.info(f"æ—¥å¿—çº§åˆ«è®¾ç½®ä¸º: {logging.getLevelName(log_level)}")
    logger.info(f"æ—¥å¿—æ–‡ä»¶è·¯å¾„: {APP_LOG_FILE_PATH}")
    logger.info(f"æ§åˆ¶å°æ—¥å¿—å¤„ç†å™¨å·²æ·»åŠ ã€‚")
    logger.info(f"Print é‡å®šå‘ (ç”± SERVER_REDIRECT_PRINT ç¯å¢ƒå˜é‡æ§åˆ¶): {'å¯ç”¨' if redirect_print else 'ç¦ç”¨'}")
    return original_stdout, original_stderr

def restore_original_streams(original_stdout, original_stderr):
    sys.stdout = original_stdout
    sys.stderr = original_stderr
    print("å·²æ¢å¤ server.py çš„åŸå§‹ stdout å’Œ stderr æµã€‚", file=sys.__stderr__)

# --- StreamToLogger, WebSocketConnectionManager, WebSocketLogHandler ---
class StreamToLogger:
    def __init__(self, logger_instance, log_level=logging.INFO):
        self.logger = logger_instance
        self.log_level = log_level
        self.linebuf = ''

    def write(self, buf):
        try:
            temp_linebuf = self.linebuf + buf
            self.linebuf = ''
            for line in temp_linebuf.splitlines(True):
                if line.endswith(('\n', '\r')):
                    self.logger.log(self.log_level, line.rstrip())
                else:
                    self.linebuf += line
        except Exception as e:
            print(f"StreamToLogger é”™è¯¯: {e}", file=sys.__stderr__)

    def flush(self):
        try:
            if self.linebuf != '':
                self.logger.log(self.log_level, self.linebuf.rstrip())
            self.linebuf = ''
        except Exception as e:
            print(f"StreamToLogger Flush é”™è¯¯: {e}", file=sys.__stderr__)

    def isatty(self):
        return False

class WebSocketConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}

    async def connect(self, client_id: str, websocket: WebSocket):
        await websocket.accept()
        self.active_connections[client_id] = websocket
        logger.info(f"WebSocket æ—¥å¿—å®¢æˆ·ç«¯å·²è¿æ¥: {client_id}")
        try:
            await websocket.send_text(json.dumps({
                "type": "connection_status",
                "status": "connected",
                "message": "å·²è¿æ¥åˆ°å®æ—¶æ—¥å¿—æµã€‚",
                "timestamp": datetime.datetime.now().isoformat()
            }))
        except Exception as e:
            logger.warning(f"å‘ WebSocket å®¢æˆ·ç«¯ {client_id} å‘é€æ¬¢è¿æ¶ˆæ¯å¤±è´¥: {e}")

    def disconnect(self, client_id: str):
        if client_id in self.active_connections:
            del self.active_connections[client_id]
            logger.info(f"WebSocket æ—¥å¿—å®¢æˆ·ç«¯å·²æ–­å¼€: {client_id}")

    async def broadcast(self, message: str):
        if not self.active_connections:
            return
        disconnected_clients = []
        active_conns_copy = list(self.active_connections.items())
        for client_id, connection in active_conns_copy:
            try:
                await connection.send_text(message)
            except WebSocketDisconnect:
                logger.info(f"[WS Broadcast] å®¢æˆ·ç«¯ {client_id} åœ¨å¹¿æ’­æœŸé—´æ–­å¼€è¿æ¥ã€‚")
                disconnected_clients.append(client_id)
            except RuntimeError as e:
                 if "Connection is closed" in str(e):
                     logger.info(f"[WS Broadcast] å®¢æˆ·ç«¯ {client_id} çš„è¿æ¥å·²å…³é—­ã€‚")
                     disconnected_clients.append(client_id)
                 else:
                     logger.error(f"å¹¿æ’­åˆ° WebSocket {client_id} æ—¶å‘ç”Ÿè¿è¡Œæ—¶é”™è¯¯: {e}")
                     disconnected_clients.append(client_id)
            except Exception as e:
                logger.error(f"å¹¿æ’­åˆ° WebSocket {client_id} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                disconnected_clients.append(client_id)
        if disconnected_clients:
             for client_id_to_remove in disconnected_clients:
                 self.disconnect(client_id_to_remove)

class WebSocketLogHandler(logging.Handler):
    def __init__(self, manager: WebSocketConnectionManager):
        super().__init__()
        self.manager = manager
        self.formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    def emit(self, record: logging.LogRecord):
        if self.manager and self.manager.active_connections:
            try:
                log_entry_str = self.format(record)
                try:
                     current_loop = asyncio.get_running_loop()
                     current_loop.create_task(self.manager.broadcast(log_entry_str))
                except RuntimeError:
                     pass
            except Exception as e:
                print(f"WebSocketLogHandler é”™è¯¯: å¹¿æ’­æ—¥å¿—å¤±è´¥ - {e}", file=sys.__stderr__)

# --- Pydantic Models ---
class FunctionCall(BaseModel):
    name: str
    arguments: str

class ToolCall(BaseModel):
    id: str
    type: str = "function"
    function: FunctionCall

class MessageContentItem(BaseModel):
    type: str
    text: Optional[str] = None

class Message(BaseModel):
    role: str
    content: Union[str, List[MessageContentItem], None] = None
    name: Optional[str] = None
    tool_calls: Optional[List[ToolCall]] = None
    tool_call_id: Optional[str] = None

class ChatCompletionRequest(BaseModel):
    messages: List[Message]
    model: Optional[str] = MODEL_NAME
    stream: Optional[bool] = False
    temperature: Optional[float] = None
    max_output_tokens: Optional[int] = None
    stop: Optional[Union[str, List[str]]] = None
    top_p: Optional[float] = None

# --- Custom Exception ---
class ClientDisconnectedError(Exception):
    pass

# --- Helper Functions ---
def prepare_combined_prompt(messages: List[Message], req_id: str) -> str:
    # Using logger instead of print
    logger.info(f"[{req_id}] (å‡†å¤‡æç¤º) æ­£åœ¨ä» {len(messages)} æ¡æ¶ˆæ¯å‡†å¤‡ç»„åˆæç¤º (åŒ…æ‹¬å†å²)ã€‚")
    combined_parts = []
    system_prompt_content: Optional[str] = None
    processed_system_message_indices: Set[int] = set()
    for i, msg in enumerate(messages):
        if msg.role == 'system':
            if isinstance(msg.content, str) and msg.content.strip():
                system_prompt_content = msg.content.strip()
                processed_system_message_indices.add(i)
                logger.info(f"[{req_id}] (å‡†å¤‡æç¤º) åœ¨ç´¢å¼• {i} æ‰¾åˆ°å¹¶ä½¿ç”¨ç³»ç»Ÿæç¤º: '{system_prompt_content[:80]}...'")
                system_instr_prefix = "ç³»ç»ŸæŒ‡ä»¤:\n"
                combined_parts.append(f"{system_instr_prefix}{system_prompt_content}")
            else:
                logger.info(f"[{req_id}] (å‡†å¤‡æç¤º) åœ¨ç´¢å¼• {i} å¿½ç•¥éå­—ç¬¦ä¸²æˆ–ç©ºçš„ç³»ç»Ÿæ¶ˆæ¯ã€‚")
                processed_system_message_indices.add(i)
            break
    role_map_ui = {"user": "ç”¨æˆ·", "assistant": "åŠ©æ‰‹", "system": "ç³»ç»Ÿ", "tool": "å·¥å…·"}
    turn_separator = "\n---\n"
    for i, msg in enumerate(messages):
        if i in processed_system_message_indices:
            continue
        if msg.role == 'system':
            logger.info(f"[{req_id}] (å‡†å¤‡æç¤º) è·³è¿‡åœ¨ç´¢å¼• {i} çš„åç»­ç³»ç»Ÿæ¶ˆæ¯ã€‚")
            continue
        if combined_parts:
            combined_parts.append(turn_separator)
        role_prefix_ui = f"{role_map_ui.get(msg.role, msg.role.capitalize())}:\n"
        current_turn_parts = [role_prefix_ui]
        content_str = ""
        if isinstance(msg.content, str):
            content_str = msg.content.strip()
        elif isinstance(msg.content, list):
            text_parts = []
            for item_model in msg.content:
                if isinstance(item_model, dict):
                    item_type = item_model.get('type')
                    if item_type == 'text' and isinstance(item_model.get('text'), str):
                        text_parts.append(item_model['text'])
                    else:
                        logger.warning(f"[{req_id}] (å‡†å¤‡æç¤º) è­¦å‘Š: åœ¨ç´¢å¼• {i} çš„æ¶ˆæ¯ä¸­å¿½ç•¥éæ–‡æœ¬æˆ–æœªçŸ¥ç±»å‹çš„ content item: ç±»å‹={item_type}")
                elif isinstance(item_model, MessageContentItem):
                    if item_model.type == 'text' and isinstance(item_model.text, str):
                        text_parts.append(item_model.text)
                    else:
                        logger.warning(f"[{req_id}] (å‡†å¤‡æç¤º) è­¦å‘Š: åœ¨ç´¢å¼• {i} çš„æ¶ˆæ¯ä¸­å¿½ç•¥éæ–‡æœ¬æˆ–æœªçŸ¥ç±»å‹çš„ content item: ç±»å‹={item_model.type}")
            content_str = "\n".join(text_parts).strip()
        elif msg.content is None and msg.role == 'assistant' and hasattr(msg, 'tool_calls') and msg.tool_calls:
            pass
        elif msg.content is None and msg.role == 'tool':
             logger.warning(f"[{req_id}] (å‡†å¤‡æç¤º) è­¦å‘Š: è§’è‰² 'tool' åœ¨ç´¢å¼• {i} çš„ content ä¸º Noneï¼Œè¿™é€šå¸¸ä¸ç¬¦åˆé¢„æœŸã€‚")
        else:
            logger.warning(f"[{req_id}] (å‡†å¤‡æç¤º) è­¦å‘Š: è§’è‰² {msg.role} åœ¨ç´¢å¼• {i} çš„å†…å®¹ç±»å‹æ„å¤– ({type(msg.content)}) æˆ–ä¸º Noneã€‚å°†å°è¯•è½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²ã€‚")
            content_str = str(msg.content or "").strip()
        if content_str:
            current_turn_parts.append(content_str)
        if msg.role == 'assistant' and hasattr(msg, 'tool_calls') and msg.tool_calls:
            if content_str:
                current_turn_parts.append("\n")
            tool_call_visualizations = []
            if msg.tool_calls:
                for tool_call in msg.tool_calls:
                    if isinstance(tool_call, dict) and tool_call.get('type') == 'function':
                        function_call = tool_call.get('function')
                        if isinstance(function_call, dict):
                            func_name = function_call.get('name')
                            func_args_str = function_call.get('arguments')
                            try:
                                parsed_args = json.loads(func_args_str if func_args_str else '{}')
                                formatted_args = json.dumps(parsed_args, indent=2, ensure_ascii=False)
                            except (json.JSONDecodeError, TypeError):
                                formatted_args = func_args_str if func_args_str is not None else "{}"
                            tool_call_visualizations.append(
                                f"è¯·æ±‚è°ƒç”¨å‡½æ•°: {func_name}\nå‚æ•°:\n{formatted_args}"
                            )
            if tool_call_visualizations:
                current_turn_parts.append("\n".join(tool_call_visualizations))
        if msg.role == 'tool' and hasattr(msg, 'tool_call_id') and msg.tool_call_id:
            if hasattr(msg, 'name') and msg.name and content_str:
                pass
            elif not content_str:
                 logger.warning(f"[{req_id}] (å‡†å¤‡æç¤º) è­¦å‘Š: è§’è‰² 'tool' (ID: {msg.tool_call_id}, Name: {getattr(msg, 'name', 'N/A')}) åœ¨ç´¢å¼• {i} çš„ content ä¸ºç©ºï¼Œè¿™é€šå¸¸è¡¨ç¤ºå‡½æ•°æ‰§è¡Œæ— å­—ç¬¦ä¸²è¾“å‡ºæˆ–ç»“æœæœªæä¾›ã€‚")
        if len(current_turn_parts) > 1 or (msg.role == 'assistant' and hasattr(msg, 'tool_calls') and msg.tool_calls):
            combined_parts.append("".join(current_turn_parts))
        elif not combined_parts and not current_turn_parts:
            logger.info(f"[{req_id}] (å‡†å¤‡æç¤º) è·³è¿‡è§’è‰² {msg.role} åœ¨ç´¢å¼• {i} çš„ç©ºæ¶ˆæ¯ (ä¸”æ— å·¥å…·è°ƒç”¨)ã€‚")
        elif len(current_turn_parts) == 1 and not combined_parts:
             logger.info(f"[{req_id}] (å‡†å¤‡æç¤º) è·³è¿‡è§’è‰² {msg.role} åœ¨ç´¢å¼• {i} çš„ç©ºæ¶ˆæ¯ (åªæœ‰å‰ç¼€)ã€‚")
    final_prompt = "".join(combined_parts)
    if final_prompt:
        final_prompt += "\n"
    preview_text = final_prompt[:300].replace('\n', '\\n')
    logger.info(f"[{req_id}] (å‡†å¤‡æç¤º) ç»„åˆæç¤ºé•¿åº¦: {len(final_prompt)}ã€‚é¢„è§ˆ: '{preview_text}...'")
    return final_prompt

def validate_chat_request(messages: List[Message], req_id: str) -> Dict[str, Optional[str]]:
    if not messages:
        raise ValueError(f"[{req_id}] æ— æ•ˆè¯·æ±‚: 'messages' æ•°ç»„ç¼ºå¤±æˆ–ä¸ºç©ºã€‚")
    if not any(msg.role != 'system' for msg in messages):
        raise ValueError(f"[{req_id}] æ— æ•ˆè¯·æ±‚: æœªæ‰¾åˆ°ç”¨æˆ·æˆ–åŠ©æ‰‹æ¶ˆæ¯ã€‚")
    logger.info(f"[{req_id}] (æ ¡éªŒ) å¯¹ {len(messages)} æ¡æ¶ˆæ¯çš„åŸºæœ¬æ ¡éªŒé€šè¿‡ã€‚")
    return {}

async def get_raw_text_content(response_element: Locator, previous_text: str, req_id: str) -> str:
    raw_text = previous_text
    try:
        await response_element.wait_for(state='attached', timeout=1000)
        pre_element = response_element.locator('pre').last
        pre_found_and_visible = False
        try:
            await pre_element.wait_for(state='visible', timeout=250)
            pre_found_and_visible = True
        except PlaywrightAsyncError: pass
        if pre_found_and_visible:
            try:
                raw_text = await pre_element.inner_text(timeout=500)
            except PlaywrightAsyncError as pre_err:
                if DEBUG_LOGS_ENABLED:
                    error_message_first_line = pre_err.message.split('\n')[0]
                    logger.warning(f"[{req_id}] ä»å¯è§çš„ <pre> è·å– innerText å¤±è´¥: {error_message_first_line}")
                try:
                     raw_text = await response_element.inner_text(timeout=1000)
                except PlaywrightAsyncError as e_parent:
                     if DEBUG_LOGS_ENABLED:
                         logger.warning(f"[{req_id}] åœ¨ <pre> è·å–å¤±è´¥åï¼Œä»çˆ¶å…ƒç´ è·å– inner_text å¤±è´¥: {e_parent}ã€‚è¿”å›å…ˆå‰æ–‡æœ¬ã€‚")
                     raw_text = previous_text
        else:
            try:
                 raw_text = await response_element.inner_text(timeout=1500)
            except PlaywrightAsyncError as e_parent:
                 if DEBUG_LOGS_ENABLED:
                     logger.warning(f"[{req_id}] ä»çˆ¶å…ƒç´ è·å– inner_text å¤±è´¥ (æ—  pre å…ƒç´ ): {e_parent}ã€‚è¿”å›å…ˆå‰æ–‡æœ¬ã€‚")
                 raw_text = previous_text
        if raw_text and isinstance(raw_text, str):
            replacements = {
                "": ""
            }
            cleaned_text = raw_text
            found_junk = False
            for junk, replacement in replacements.items():
                if junk in cleaned_text:
                    cleaned_text = cleaned_text.replace(junk, replacement)
                    found_junk = True
            if found_junk:
                cleaned_text = "\n".join([line.strip() for line in cleaned_text.splitlines() if line.strip()])
                if DEBUG_LOGS_ENABLED:
                     logger.debug(f"[{req_id}] (æ¸…ç†) å·²ç§»é™¤å“åº”æ–‡æœ¬ä¸­çš„å·²çŸ¥UIå…ƒç´ ã€‚")
                raw_text = cleaned_text
        return raw_text
    except PlaywrightAsyncError:
        return previous_text
    except Exception as e_general:
         logger.warning(f"[{req_id}] getRawTextContent ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯: {e_general}ã€‚è¿”å›å…ˆå‰æ–‡æœ¬ã€‚")
         return previous_text

def generate_sse_chunk(delta: str, req_id: str, model: str) -> str:
    chunk = {
        "id": f"{CHAT_COMPLETION_ID_PREFIX}{req_id}-{int(time.time())}-{random.randint(100, 999)}",
        "object": "chat.completion.chunk", "created": int(time.time()), "model": model,
        "choices": [{"index": 0, "delta": {"content": delta}, "finish_reason": None}]
    }
    return f"data: {json.dumps(chunk)}\n\n"

def generate_sse_stop_chunk(req_id: str, model: str, reason: str = "stop") -> str:
    chunk = {
        "id": f"{CHAT_COMPLETION_ID_PREFIX}{req_id}-{int(time.time())}-{random.randint(100, 999)}",
        "object": "chat.completion.chunk", "created": int(time.time()), "model": model,
        "choices": [{"index": 0, "delta": {}, "finish_reason": reason}]
    }
    return f"data: {json.dumps(chunk)}\n\n"

def generate_sse_error_chunk(message: str, req_id: str, error_type: str = "server_error") -> str:
    error_payload = {"error": {"message": f"[{req_id}] {message}", "type": error_type}}
    return f"data: {json.dumps(error_payload)}\n\n"

async def _initialize_page_logic(browser: AsyncBrowser):
    logger.info("--- åˆå§‹åŒ–é¡µé¢é€»è¾‘ (è¿æ¥åˆ°ç°æœ‰æµè§ˆå™¨) ---")
    temp_context: Optional[AsyncBrowserContext] = None
    storage_state_path_to_use: Optional[str] = None
    launch_mode = os.environ.get('LAUNCH_MODE', 'debug')
    logger.info(f"   æ£€æµ‹åˆ°å¯åŠ¨æ¨¡å¼: {launch_mode}")
    loop = asyncio.get_running_loop()
    if launch_mode == 'headless' or launch_mode == 'virtual_headless':
        auth_filename = os.environ.get('ACTIVE_AUTH_JSON_PATH')
        if auth_filename:
            constructed_path = auth_filename
            if os.path.exists(constructed_path):
                storage_state_path_to_use = constructed_path
                logger.info(f"   æ— å¤´æ¨¡å¼å°†ä½¿ç”¨çš„è®¤è¯æ–‡ä»¶: {constructed_path}")
            else:
                logger.error(f"{launch_mode} æ¨¡å¼è®¤è¯æ–‡ä»¶æ— æ•ˆæˆ–ä¸å­˜åœ¨: '{constructed_path}'")
                raise RuntimeError(f"{launch_mode} æ¨¡å¼è®¤è¯æ–‡ä»¶æ— æ•ˆ: '{constructed_path}'")
        else:
            logger.error(f"{launch_mode} æ¨¡å¼éœ€è¦ ACTIVE_AUTH_JSON_PATH ç¯å¢ƒå˜é‡ï¼Œä½†æœªè®¾ç½®æˆ–ä¸ºç©ºã€‚")
            raise RuntimeError(f"{launch_mode} æ¨¡å¼éœ€è¦ ACTIVE_AUTH_JSON_PATHã€‚")
    elif launch_mode == 'debug':
        logger.info(f"   è°ƒè¯•æ¨¡å¼: å°è¯•ä»ç¯å¢ƒå˜é‡ ACTIVE_AUTH_JSON_PATH åŠ è½½è®¤è¯æ–‡ä»¶...")
        auth_filepath_from_env = os.environ.get('ACTIVE_AUTH_JSON_PATH')
        if auth_filepath_from_env and os.path.exists(auth_filepath_from_env):
            storage_state_path_to_use = auth_filepath_from_env
            logger.info(f"   è°ƒè¯•æ¨¡å¼å°†ä½¿ç”¨çš„è®¤è¯æ–‡ä»¶ (æ¥è‡ªç¯å¢ƒå˜é‡): {storage_state_path_to_use}")
        elif auth_filepath_from_env:
            logger.warning(f"   è°ƒè¯•æ¨¡å¼ä¸‹ç¯å¢ƒå˜é‡ ACTIVE_AUTH_JSON_PATH æŒ‡å‘çš„æ–‡ä»¶ä¸å­˜åœ¨: '{auth_filepath_from_env}'ã€‚ä¸åŠ è½½è®¤è¯æ–‡ä»¶ã€‚")
        else:
            logger.info("   è°ƒè¯•æ¨¡å¼ä¸‹æœªé€šè¿‡ç¯å¢ƒå˜é‡æä¾›è®¤è¯æ–‡ä»¶ã€‚å°†ä½¿ç”¨æµè§ˆå™¨å½“å‰çŠ¶æ€ã€‚")
    elif launch_mode == "direct_debug_no_browser":
        logger.info("   direct_debug_no_browser æ¨¡å¼ï¼šä¸åŠ è½½ storage_stateï¼Œä¸è¿›è¡Œæµè§ˆå™¨æ“ä½œã€‚")
    else:
        logger.warning(f"   âš ï¸ è­¦å‘Š: æœªçŸ¥çš„å¯åŠ¨æ¨¡å¼ '{launch_mode}'ã€‚ä¸åŠ è½½ storage_stateã€‚")
    try:
        logger.info("åˆ›å»ºæ–°çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡...")
        context_options: Dict[str, Any] = {'viewport': {'width': 460, 'height': 800}}
        if storage_state_path_to_use:
            context_options['storage_state'] = storage_state_path_to_use
            logger.info(f"   (ä½¿ç”¨ storage_state='{os.path.basename(storage_state_path_to_use)}')")
        else:
            logger.info("   (ä¸ä½¿ç”¨ storage_state)")
        if PLAYWRIGHT_PROXY_SETTINGS:
            context_options['proxy'] = PLAYWRIGHT_PROXY_SETTINGS
            logger.info(f"   (æµè§ˆå™¨ä¸Šä¸‹æ–‡å°†ä½¿ç”¨ä»£ç†: {PLAYWRIGHT_PROXY_SETTINGS['server']})")
        else:
            logger.info("   (æµè§ˆå™¨ä¸Šä¸‹æ–‡ä¸ä½¿ç”¨æ˜¾å¼ä»£ç†é…ç½®)")
        context_options['ignore_https_errors'] = True
        logger.info("   (æµè§ˆå™¨ä¸Šä¸‹æ–‡å°†å¿½ç•¥ HTTPS é”™è¯¯)")
        temp_context = await browser.new_context(**context_options)
        found_page: Optional[AsyncPage] = None
        pages = temp_context.pages
        target_url_base = f"https://{AI_STUDIO_URL_PATTERN}"
        target_full_url = f"{target_url_base}prompts/new_chat"
        login_url_pattern = 'accounts.google.com'
        current_url = ""
        for p_iter in pages:
            try:
                page_url_to_check = p_iter.url
                if not p_iter.is_closed() and target_url_base in page_url_to_check and "/prompts/" in page_url_to_check:
                    found_page = p_iter
                    current_url = page_url_to_check
                    logger.info(f"   æ‰¾åˆ°å·²æ‰“å¼€çš„ AI Studio é¡µé¢: {current_url}")
                    if found_page:
                        logger.info(f"   ä¸ºå·²å­˜åœ¨çš„é¡µé¢ {found_page.url} æ·»åŠ æ¨¡å‹åˆ—è¡¨å“åº”ç›‘å¬å™¨ã€‚")
                        found_page.on("response", _handle_model_list_response)
                    break
            except PlaywrightAsyncError as pw_err_url:
                logger.warning(f"   æ£€æŸ¥é¡µé¢ URL æ—¶å‡ºç° Playwright é”™è¯¯: {pw_err_url}")
            except AttributeError as attr_err_url:
                logger.warning(f"   æ£€æŸ¥é¡µé¢ URL æ—¶å‡ºç°å±æ€§é”™è¯¯: {attr_err_url}")
            except Exception as e_url_check:
                logger.warning(f"   æ£€æŸ¥é¡µé¢ URL æ—¶å‡ºç°å…¶ä»–æœªé¢„æœŸé”™è¯¯: {e_url_check} (ç±»å‹: {type(e_url_check).__name__})")
        if not found_page:
            logger.info(f"-> æœªæ‰¾åˆ°åˆé€‚çš„ç°æœ‰é¡µé¢ï¼Œæ­£åœ¨æ‰“å¼€æ–°é¡µé¢å¹¶å¯¼èˆªåˆ° {target_full_url}...")
            found_page = await temp_context.new_page()
            if found_page:
                logger.info(f"   ä¸ºæ–°åˆ›å»ºçš„é¡µé¢æ·»åŠ æ¨¡å‹åˆ—è¡¨å“åº”ç›‘å¬å™¨ (å¯¼èˆªå‰)ã€‚")
                found_page.on("response", _handle_model_list_response)
            try:
                await found_page.goto(target_full_url, wait_until="domcontentloaded", timeout=90000)
                current_url = found_page.url
                logger.info(f"-> æ–°é¡µé¢å¯¼èˆªå°è¯•å®Œæˆã€‚å½“å‰ URL: {current_url}")
            except Exception as new_page_nav_err:
                await save_error_snapshot("init_new_page_nav_fail")
                error_str = str(new_page_nav_err)
                if "NS_ERROR_NET_INTERRUPT" in error_str:
                    logger.error("\n" + "="*30 + " ç½‘ç»œå¯¼èˆªé”™è¯¯æç¤º " + "="*30)
                    logger.error(f"âŒ å¯¼èˆªåˆ° '{target_full_url}' å¤±è´¥ï¼Œå‡ºç°ç½‘ç»œä¸­æ–­é”™è¯¯ (NS_ERROR_NET_INTERRUPT)ã€‚")
                    logger.error("   è¿™é€šå¸¸è¡¨ç¤ºæµè§ˆå™¨åœ¨å°è¯•åŠ è½½é¡µé¢æ—¶è¿æ¥è¢«æ„å¤–æ–­å¼€ã€‚")
                    logger.error("   å¯èƒ½çš„åŸå› åŠæ’æŸ¥å»ºè®®:")
                    logger.error("     1. ç½‘ç»œè¿æ¥: è¯·æ£€æŸ¥ä½ çš„æœ¬åœ°ç½‘ç»œè¿æ¥æ˜¯å¦ç¨³å®šï¼Œå¹¶å°è¯•åœ¨æ™®é€šæµè§ˆå™¨ä¸­è®¿é—®ç›®æ ‡ç½‘å€ã€‚")
                    logger.error("     2. AI Studio æœåŠ¡: ç¡®è®¤ aistudio.google.com æœåŠ¡æœ¬èº«æ˜¯å¦å¯ç”¨ã€‚")
                    logger.error("     3. é˜²ç«å¢™/ä»£ç†/VPN: æ£€æŸ¥æœ¬åœ°é˜²ç«å¢™ã€æ€æ¯’è½¯ä»¶ã€ä»£ç†æˆ– VPN è®¾ç½®ã€‚")
                    logger.error("     4. Camoufox æœåŠ¡: ç¡®è®¤ launch_camoufox.py è„šæœ¬æ˜¯å¦æ­£å¸¸è¿è¡Œã€‚")
                    logger.error("     5. ç³»ç»Ÿèµ„æºé—®é¢˜: ç¡®ä¿ç³»ç»Ÿæœ‰è¶³å¤Ÿçš„å†…å­˜å’Œ CPU èµ„æºã€‚")
                    logger.error("="*74 + "\n")
                raise RuntimeError(f"å¯¼èˆªæ–°é¡µé¢å¤±è´¥: {new_page_nav_err}") from new_page_nav_err
        if login_url_pattern in current_url:
            if launch_mode == 'headless':
                logger.error("æ— å¤´æ¨¡å¼ä¸‹æ£€æµ‹åˆ°é‡å®šå‘è‡³ç™»å½•é¡µé¢ï¼Œè®¤è¯å¯èƒ½å·²å¤±æ•ˆã€‚è¯·æ›´æ–°è®¤è¯æ–‡ä»¶ã€‚")
                raise RuntimeError("æ— å¤´æ¨¡å¼è®¤è¯å¤±è´¥ï¼Œéœ€è¦æ›´æ–°è®¤è¯æ–‡ä»¶ã€‚")
            else:
                print(f"\n{'='*20} éœ€è¦æ“ä½œ {'='*20}", flush=True)
                login_prompt = "   æ£€æµ‹åˆ°å¯èƒ½éœ€è¦ç™»å½•ã€‚å¦‚æœæµè§ˆå™¨æ˜¾ç¤ºç™»å½•é¡µé¢ï¼Œè¯·åœ¨æµè§ˆå™¨çª—å£ä¸­å®Œæˆ Google ç™»å½•ï¼Œç„¶ååœ¨æ­¤å¤„æŒ‰ Enter é”®ç»§ç»­..."
                print(USER_INPUT_START_MARKER_SERVER, flush=True)
                await loop.run_in_executor(None, input, login_prompt)
                print(USER_INPUT_END_MARKER_SERVER, flush=True)
                logger.info("   ç”¨æˆ·å·²æ“ä½œï¼Œæ­£åœ¨æ£€æŸ¥ç™»å½•çŠ¶æ€...")
                try:
                    await found_page.wait_for_url(f"**/{AI_STUDIO_URL_PATTERN}**", timeout=180000)
                    current_url = found_page.url
                    if login_url_pattern in current_url:
                        logger.error("æ‰‹åŠ¨ç™»å½•å°è¯•åï¼Œé¡µé¢ä¼¼ä¹ä»åœç•™åœ¨ç™»å½•é¡µé¢ã€‚")
                        raise RuntimeError("æ‰‹åŠ¨ç™»å½•å°è¯•åä»åœ¨ç™»å½•é¡µé¢ã€‚")
                    logger.info("   âœ… ç™»å½•æˆåŠŸï¼è¯·ä¸è¦æ“ä½œæµè§ˆå™¨çª—å£ï¼Œç­‰å¾…åç»­æç¤ºã€‚")
                    print("\n" + "="*50, flush=True)
                    print("   ã€ç”¨æˆ·äº¤äº’ã€‘éœ€è¦æ‚¨çš„è¾“å…¥!", flush=True)
                    save_auth_prompt = "   æ˜¯å¦è¦å°†å½“å‰çš„æµè§ˆå™¨è®¤è¯çŠ¶æ€ä¿å­˜åˆ°æ–‡ä»¶ï¼Ÿ (y/N): "
                    should_save_auth_choice = ''
                    if AUTO_SAVE_AUTH and launch_mode == 'debug':
                        logger.info("   è‡ªåŠ¨ä¿å­˜è®¤è¯æ¨¡å¼å·²å¯ç”¨ï¼Œå°†è‡ªåŠ¨ä¿å­˜è®¤è¯çŠ¶æ€...")
                        should_save_auth_choice = 'y'
                    else:
                        print(USER_INPUT_START_MARKER_SERVER, flush=True)
                        try:
                            auth_save_input_future = loop.run_in_executor(None, input, save_auth_prompt)
                            should_save_auth_choice = await asyncio.wait_for(auth_save_input_future, timeout=AUTH_SAVE_TIMEOUT)
                        except asyncio.TimeoutError:
                            print(f"   è¾“å…¥ç­‰å¾…è¶…æ—¶({AUTH_SAVE_TIMEOUT}ç§’)ã€‚é»˜è®¤ä¸ä¿å­˜è®¤è¯çŠ¶æ€ã€‚", flush=True)
                            should_save_auth_choice = 'n'
                        finally:
                            print(USER_INPUT_END_MARKER_SERVER, flush=True)
                    if should_save_auth_choice.strip().lower() == 'y':
                        os.makedirs(SAVED_AUTH_DIR, exist_ok=True)
                        default_auth_filename = f"auth_state_{int(time.time())}.json"
                        print(USER_INPUT_START_MARKER_SERVER, flush=True)
                        filename_prompt_str = f"   è¯·è¾“å…¥ä¿å­˜çš„æ–‡ä»¶å (é»˜è®¤ä¸º: {default_auth_filename}): "
                        chosen_auth_filename = ''
                        try:
                            filename_input_future = loop.run_in_executor(None, input, filename_prompt_str)
                            chosen_auth_filename = await asyncio.wait_for(filename_input_future, timeout=AUTH_SAVE_TIMEOUT)
                        except asyncio.TimeoutError:
                            print(f"   è¾“å…¥æ–‡ä»¶åç­‰å¾…è¶…æ—¶({AUTH_SAVE_TIMEOUT}ç§’)ã€‚å°†ä½¿ç”¨é»˜è®¤æ–‡ä»¶å: {default_auth_filename}", flush=True)
                        finally:
                            print(USER_INPUT_END_MARKER_SERVER, flush=True)
                        final_auth_filename = chosen_auth_filename.strip() or default_auth_filename
                        if not final_auth_filename.endswith(".json"):
                            final_auth_filename += ".json"
                        auth_save_path = os.path.join(SAVED_AUTH_DIR, final_auth_filename)
                        try:
                            await temp_context.storage_state(path=auth_save_path)
                            print(f"   âœ… è®¤è¯çŠ¶æ€å·²æˆåŠŸä¿å­˜åˆ°: {auth_save_path}", flush=True)
                        except Exception as save_state_err:
                            logger.error(f"   âŒ ä¿å­˜è®¤è¯çŠ¶æ€å¤±è´¥: {save_state_err}", exc_info=True)
                            print(f"   âŒ ä¿å­˜è®¤è¯çŠ¶æ€å¤±è´¥: {save_state_err}", flush=True)
                    else:
                        print("   å¥½çš„ï¼Œä¸ä¿å­˜è®¤è¯çŠ¶æ€ã€‚", flush=True)
                    print("="*50 + "\n", flush=True)
                except Exception as wait_login_err:
                    await save_error_snapshot("init_login_wait_fail")
                    logger.error(f"ç™»å½•æç¤ºåæœªèƒ½æ£€æµ‹åˆ° AI Studio URL æˆ–ä¿å­˜çŠ¶æ€æ—¶å‡ºé”™: {wait_login_err}", exc_info=True)
                    raise RuntimeError(f"ç™»å½•æç¤ºåæœªèƒ½æ£€æµ‹åˆ° AI Studio URL: {wait_login_err}") from wait_login_err
        elif target_url_base not in current_url or "/prompts/" not in current_url:
            await save_error_snapshot("init_unexpected_page")
            logger.error(f"åˆå§‹å¯¼èˆªåé¡µé¢ URL æ„å¤–: {current_url}ã€‚æœŸæœ›åŒ…å« '{target_url_base}' å’Œ '/prompts/'ã€‚")
            raise RuntimeError(f"åˆå§‹å¯¼èˆªåå‡ºç°æ„å¤–é¡µé¢: {current_url}ã€‚")
        logger.info(f"-> ç¡®è®¤å½“å‰ä½äº AI Studio å¯¹è¯é¡µé¢: {current_url}")
        await found_page.bring_to_front()
        try:
            input_wrapper_locator = found_page.locator('ms-prompt-input-wrapper')
            await expect_async(input_wrapper_locator).to_be_visible(timeout=35000)
            await expect_async(found_page.locator(INPUT_SELECTOR)).to_be_visible(timeout=10000)
            logger.info("-> âœ… æ ¸å¿ƒè¾“å…¥åŒºåŸŸå¯è§ã€‚")
            model_name_locator = found_page.locator('mat-select[data-test-ms-model-selector] div.model-option-content span.gmat-body-medium')
            try:
                model_name_on_page = await model_name_locator.first.inner_text(timeout=5000)
                logger.info(f"-> ğŸ¤– é¡µé¢æ£€æµ‹åˆ°çš„å½“å‰æ¨¡å‹: {model_name_on_page}")
            except PlaywrightAsyncError as e:
                logger.error(f"è·å–æ¨¡å‹åç§°æ—¶å‡ºé”™ (model_name_locator): {e}")
                raise
            result_page_instance = found_page
            result_page_ready = True
            logger.info(f"âœ… é¡µé¢é€»è¾‘åˆå§‹åŒ–æˆåŠŸã€‚")
            return result_page_instance, result_page_ready
        except Exception as input_visible_err:
             await save_error_snapshot("init_fail_input_timeout")
             logger.error(f"é¡µé¢åˆå§‹åŒ–å¤±è´¥ï¼šæ ¸å¿ƒè¾“å…¥åŒºåŸŸæœªåœ¨é¢„æœŸæ—¶é—´å†…å˜ä¸ºå¯è§ã€‚æœ€åçš„ URL æ˜¯ {found_page.url}", exc_info=True)
             raise RuntimeError(f"é¡µé¢åˆå§‹åŒ–å¤±è´¥ï¼šæ ¸å¿ƒè¾“å…¥åŒºåŸŸæœªåœ¨é¢„æœŸæ—¶é—´å†…å˜ä¸ºå¯è§ã€‚æœ€åçš„ URL æ˜¯ {found_page.url}") from input_visible_err
    except Exception as e_init_page:
        logger.critical(f"âŒ é¡µé¢é€»è¾‘åˆå§‹åŒ–æœŸé—´å‘ç”Ÿä¸¥é‡æ„å¤–é”™è¯¯: {e_init_page}", exc_info=True)
        if temp_context:
            try:
                logger.info(f"   å°è¯•å…³é—­ä¸´æ—¶çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡ due to initialization error.")
                await temp_context.close()
                logger.info("   âœ… ä¸´æ—¶æµè§ˆå™¨ä¸Šä¸‹æ–‡å·²å…³é—­ã€‚")
            except Exception as close_err:
                 logger.warning(f"   âš ï¸ å…³é—­ä¸´æ—¶æµè§ˆå™¨ä¸Šä¸‹æ–‡æ—¶å‡ºé”™: {close_err}")
        await save_error_snapshot("init_unexpected_error")
        raise RuntimeError(f"é¡µé¢åˆå§‹åŒ–æ„å¤–é”™è¯¯: {e_init_page}") from e_init_page

async def _close_page_logic():
    global page_instance, is_page_ready
    logger.info("--- è¿è¡Œé¡µé¢é€»è¾‘å…³é—­ --- ")
    if page_instance and not page_instance.is_closed():
        try:
            await page_instance.close()
            logger.info("   âœ… é¡µé¢å·²å…³é—­")
        except PlaywrightAsyncError as pw_err:
            logger.warning(f"   âš ï¸ å…³é—­é¡µé¢æ—¶å‡ºç°Playwrighté”™è¯¯: {pw_err}")
        except asyncio.TimeoutError as timeout_err:
            logger.warning(f"   âš ï¸ å…³é—­é¡µé¢æ—¶è¶…æ—¶: {timeout_err}")
        except Exception as other_err:
            logger.error(f"   âš ï¸ å…³é—­é¡µé¢æ—¶å‡ºç°æ„å¤–é”™è¯¯: {other_err} (ç±»å‹: {type(other_err).__name__})", exc_info=True)
    page_instance = None
    is_page_ready = False
    logger.info("é¡µé¢é€»è¾‘çŠ¶æ€å·²é‡ç½®ã€‚")
    return None, False

async def _handle_model_list_response(response: Any):
    global global_model_list_raw_json, parsed_model_list, model_list_fetch_event, logger, MODELS_ENDPOINT_URL_CONTAINS, DEBUG_LOGS_ENABLED, excluded_model_ids
    if MODELS_ENDPOINT_URL_CONTAINS in response.url and response.ok:
        logger.info(f"æ•è·åˆ°æ½œåœ¨çš„æ¨¡å‹åˆ—è¡¨å“åº”æ¥è‡ª: {response.url} (çŠ¶æ€: {response.status})")
        try:
            data = await response.json()
            models_array_container = None
            if isinstance(data, list) and data:
                if isinstance(data[0], list) and data[0] and isinstance(data[0][0], list):
                    logger.info("æ£€æµ‹åˆ°ä¸‰å±‚åˆ—è¡¨ç»“æ„ data[0][0] is list. models_array_container è®¾ç½®ä¸º data[0]ã€‚")
                    models_array_container = data[0]
                elif isinstance(data[0], list) and data[0] and isinstance(data[0][0], str):
                    logger.info("æ£€æµ‹åˆ°ä¸¤å±‚åˆ—è¡¨ç»“æ„ data[0][0] is str. models_array_container è®¾ç½®ä¸º dataã€‚")
                    models_array_container = data
                elif isinstance(data[0], dict):
                    logger.info("æ£€æµ‹åˆ°æ ¹åˆ—è¡¨ï¼Œå…ƒç´ ä¸ºå­—å…¸ã€‚ç›´æ¥ä½¿ç”¨ data ä½œä¸º models_array_containerã€‚")
                    models_array_container = data
                else:
                    logger.warning(f"æœªçŸ¥çš„åˆ—è¡¨åµŒå¥—ç»“æ„ã€‚data[0] ç±»å‹: {type(data[0]) if data else 'N/A'}ã€‚data[0] é¢„è§ˆ: {str(data[0])[:200] if data else 'N/A'}")
            elif isinstance(data, dict):
                if 'data' in data and isinstance(data['data'], list):
                    models_array_container = data['data']
                elif 'models' in data and isinstance(data['models'], list):
                    models_array_container = data['models']
                else:
                    for key, value in data.items():
                        if isinstance(value, list) and len(value) > 0 and isinstance(value[0], (dict, list)):
                            models_array_container = value
                            logger.info(f"æ¨¡å‹åˆ—è¡¨æ•°æ®åœ¨ '{key}' é”®ä¸‹é€šè¿‡å¯å‘å¼æœç´¢æ‰¾åˆ°ã€‚")
                            break
                    if models_array_container is None:
                        logger.warning("åœ¨å­—å…¸å“åº”ä¸­æœªèƒ½è‡ªåŠ¨å®šä½æ¨¡å‹åˆ—è¡¨æ•°ç»„ã€‚")
                        if not model_list_fetch_event.is_set(): model_list_fetch_event.set()
                        return
            else:
                logger.warning(f"æ¥æ”¶åˆ°çš„æ¨¡å‹åˆ—è¡¨æ•°æ®æ—¢ä¸æ˜¯åˆ—è¡¨ä¹Ÿä¸æ˜¯å­—å…¸: {type(data)}")
                if not model_list_fetch_event.is_set(): model_list_fetch_event.set()
                return
            if models_array_container is not None:
                new_parsed_list = []
                for entry_in_container in models_array_container:
                    model_fields_list = None
                    if isinstance(entry_in_container, dict):
                        potential_id = entry_in_container.get('id', entry_in_container.get('model_id', entry_in_container.get('modelId')))
                        if potential_id: model_fields_list = entry_in_container
                        else: model_fields_list = list(entry_in_container.values())
                    elif isinstance(entry_in_container, list):
                        model_fields_list = entry_in_container
                    else:
                        logger.debug(f"Skipping entry of unknown type: {type(entry_in_container)}")
                        continue
                    if not model_fields_list:
                        logger.debug("Skipping entry because model_fields_list is empty or None.")
                        continue
                    model_id_path_str = None
                    display_name_candidate = ""
                    description_candidate = "N/A"
                    default_max_output_tokens_val = None
                    default_top_p_val = None
                    default_temperature_val = 1.0
                    supported_max_output_tokens_val = None
                    current_model_id_for_log = "UnknownModelYet"
                    try:
                        if isinstance(model_fields_list, list):
                            if not (len(model_fields_list) > 0 and isinstance(model_fields_list[0], (str, int, float))):
                                logger.debug(f"Skipping list-based model_fields due to invalid first element: {str(model_fields_list)[:100]}")
                                continue
                            model_id_path_str = str(model_fields_list[0])
                            current_model_id_for_log = model_id_path_str.split('/')[-1] if model_id_path_str and '/' in model_id_path_str else model_id_path_str
                            display_name_candidate = str(model_fields_list[3]) if len(model_fields_list) > 3 else ""
                            description_candidate = str(model_fields_list[4]) if len(model_fields_list) > 4 else "N/A"
                            if len(model_fields_list) > 6 and model_fields_list[6] is not None:
                                try:
                                    val_int = int(model_fields_list[6])
                                    default_max_output_tokens_val = val_int
                                    supported_max_output_tokens_val = val_int
                                except (ValueError, TypeError):
                                    logger.warning(f"æ¨¡å‹ {current_model_id_for_log}: æ— æ³•å°†åˆ—è¡¨ç´¢å¼•6çš„å€¼ '{model_fields_list[6]}' è§£æä¸º max_output_tokensã€‚")
                            if len(model_fields_list) > 9 and model_fields_list[9] is not None:
                                try:
                                    raw_top_p = float(model_fields_list[9])
                                    if not (0.0 <= raw_top_p <= 1.0):
                                        logger.warning(f"æ¨¡å‹ {current_model_id_for_log}: åŸå§‹ top_på€¼ {raw_top_p} (æ¥è‡ªåˆ—è¡¨ç´¢å¼•9) è¶…å‡º [0,1] èŒƒå›´ï¼Œå°†è£å‰ªã€‚")
                                        default_top_p_val = max(0.0, min(1.0, raw_top_p))
                                    else:
                                        default_top_p_val = raw_top_p
                                except (ValueError, TypeError):
                                    logger.warning(f"æ¨¡å‹ {current_model_id_for_log}: æ— æ³•å°†åˆ—è¡¨ç´¢å¼•9çš„å€¼ '{model_fields_list[9]}' è§£æä¸º top_pã€‚")
                        elif isinstance(model_fields_list, dict):
                            model_id_path_str = str(model_fields_list.get('id', model_fields_list.get('model_id', model_fields_list.get('modelId'))))
                            current_model_id_for_log = model_id_path_str.split('/')[-1] if model_id_path_str and '/' in model_id_path_str else model_id_path_str
                            display_name_candidate = str(model_fields_list.get('displayName', model_fields_list.get('display_name', model_fields_list.get('name', ''))))
                            description_candidate = str(model_fields_list.get('description', "N/A"))
                            mot_parsed = model_fields_list.get('maxOutputTokens', model_fields_list.get('defaultMaxOutputTokens', model_fields_list.get('outputTokenLimit')))
                            if mot_parsed is not None:
                                try:
                                    val_int = int(mot_parsed)
                                    default_max_output_tokens_val = val_int
                                    supported_max_output_tokens_val = val_int
                                except (ValueError, TypeError):
                                     logger.warning(f"æ¨¡å‹ {current_model_id_for_log}: æ— æ³•å°†å­—å…¸å€¼ '{mot_parsed}' è§£æä¸º max_output_tokensã€‚")
                            top_p_parsed = model_fields_list.get('topP', model_fields_list.get('defaultTopP'))
                            if top_p_parsed is not None:
                                try:
                                    raw_top_p = float(top_p_parsed)
                                    if not (0.0 <= raw_top_p <= 1.0):
                                        logger.warning(f"æ¨¡å‹ {current_model_id_for_log}: åŸå§‹ top_på€¼ {raw_top_p} (æ¥è‡ªå­—å…¸) è¶…å‡º [0,1] èŒƒå›´ï¼Œå°†è£å‰ªã€‚")
                                        default_top_p_val = max(0.0, min(1.0, raw_top_p))
                                    else:
                                        default_top_p_val = raw_top_p
                                except (ValueError, TypeError):
                                    logger.warning(f"æ¨¡å‹ {current_model_id_for_log}: æ— æ³•å°†å­—å…¸å€¼ '{top_p_parsed}' è§£æä¸º top_pã€‚")
                            temp_parsed = model_fields_list.get('temperature', model_fields_list.get('defaultTemperature'))
                            if temp_parsed is not None:
                                try: default_temperature_val = float(temp_parsed)
                                except (ValueError, TypeError):
                                    logger.warning(f"æ¨¡å‹ {current_model_id_for_log}: æ— æ³•å°†å­—å…¸å€¼ '{temp_parsed}' è§£æä¸º temperatureã€‚")
                        else:
                            logger.debug(f"Skipping entry because model_fields_list is not list or dict: {type(model_fields_list)}")
                            continue
                    except Exception as e_parse_fields:
                        logger.error(f"è§£ææ¨¡å‹å­—æ®µæ—¶å‡ºé”™ for entry {str(entry_in_container)[:100]}: {e_parse_fields}")
                        continue
                    if model_id_path_str and model_id_path_str.lower() != "none":
                        simple_model_id_str = model_id_path_str.split('/')[-1] if '/' in model_id_path_str else model_id_path_str
                        if simple_model_id_str in excluded_model_ids:
                            logger.info(f"æ¨¡å‹ '{simple_model_id_str}' åœ¨æ’é™¤åˆ—è¡¨ excluded_model_ids ä¸­ï¼Œå·²è·³è¿‡ã€‚")
                            continue
                        final_display_name_str = display_name_candidate if display_name_candidate else simple_model_id_str.replace("-", " ").title()
                        model_entry_dict = {
                            "id": simple_model_id_str, "object": "model", "created": int(time.time()),
                            "owned_by": "ai_studio", "display_name": final_display_name_str,
                            "description": description_candidate, "raw_model_path": model_id_path_str,
                            "default_temperature": default_temperature_val,
                            "default_max_output_tokens": default_max_output_tokens_val,
                            "supported_max_output_tokens": supported_max_output_tokens_val,
                            "default_top_p": default_top_p_val
                        }
                        new_parsed_list.append(model_entry_dict)
                    else:
                        logger.debug(f"Skipping entry due to invalid model_id_path: {model_id_path_str} from entry {str(entry_in_container)[:100]}")
                if new_parsed_list:
                    parsed_model_list = sorted(new_parsed_list, key=lambda m: m.get('display_name', '').lower())
                    global_model_list_raw_json = json.dumps({"data": parsed_model_list, "object": "list"})
                    if DEBUG_LOGS_ENABLED:
                        log_output = f"æˆåŠŸè§£æå’Œæ›´æ–°æ¨¡å‹åˆ—è¡¨ã€‚æ€»å…±è§£ææ¨¡å‹æ•°: {len(parsed_model_list)}.\n"
                        for i, item in enumerate(parsed_model_list[:min(3, len(parsed_model_list))]):
                            log_output += f"  Model {i+1}: ID={item.get('id')}, Name={item.get('display_name')}, Temp={item.get('default_temperature')}, MaxTokDef={item.get('default_max_output_tokens')}, MaxTokSup={item.get('supported_max_output_tokens')}, TopP={item.get('default_top_p')}\n"
                        logger.info(log_output)
                    if not model_list_fetch_event.is_set(): model_list_fetch_event.set()
                elif not parsed_model_list:
                    logger.warning("è§£æåæ¨¡å‹åˆ—è¡¨ä»ç„¶ä¸ºç©ºã€‚")
                    if not model_list_fetch_event.is_set(): model_list_fetch_event.set()
            else:
                logger.warning("models_array_container ä¸º Noneï¼Œæ— æ³•è§£ææ¨¡å‹åˆ—è¡¨ã€‚")
                if not model_list_fetch_event.is_set(): model_list_fetch_event.set()
        except json.JSONDecodeError as json_err:
            logger.error(f"è§£ææ¨¡å‹åˆ—è¡¨JSONå¤±è´¥: {json_err}. å“åº” (å‰500å­—): {await response.text()[:500]}")
        except Exception as e_handle_list_resp:
            logger.exception(f"å¤„ç†æ¨¡å‹åˆ—è¡¨å“åº”æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e_handle_list_resp}")
        finally:
            if not model_list_fetch_event.is_set():
                logger.info("å¤„ç†æ¨¡å‹åˆ—è¡¨å“åº”ç»“æŸï¼Œå¼ºåˆ¶è®¾ç½® model_list_fetch_eventã€‚")
                model_list_fetch_event.set()

async def signal_camoufox_shutdown():
    logger.info("   å°è¯•å‘é€å…³é—­ä¿¡å·åˆ° Camoufox æœåŠ¡å™¨ (æ­¤åŠŸèƒ½å¯èƒ½å·²ç”±çˆ¶è¿›ç¨‹å¤„ç†)...")
    ws_endpoint = os.environ.get('CAMOUFOX_WS_ENDPOINT')
    if not ws_endpoint:
        logger.warning("   âš ï¸ æ— æ³•å‘é€å…³é—­ä¿¡å·ï¼šæœªæ‰¾åˆ° CAMOUFOX_WS_ENDPOINT ç¯å¢ƒå˜é‡ã€‚")
        return
    if not browser_instance or not browser_instance.is_connected():
        logger.warning("   âš ï¸ æµè§ˆå™¨å®ä¾‹å·²æ–­å¼€æˆ–æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å…³é—­ä¿¡å·å‘é€ã€‚")
        return
    try:
        await asyncio.sleep(0.2)
        logger.info("   âœ… (æ¨¡æ‹Ÿ) å…³é—­ä¿¡å·å·²å¤„ç†ã€‚")
    except Exception as e:
        logger.error(f"   âš ï¸ å‘é€å…³é—­ä¿¡å·è¿‡ç¨‹ä¸­æ•è·å¼‚å¸¸: {e}", exc_info=True)

# --- Lifespan Context Manager ---
@asynccontextmanager
async def lifespan(app_param: FastAPI):
    global playwright_manager, browser_instance, page_instance, worker_task
    global is_playwright_ready, is_browser_connected, is_page_ready, is_initializing
    global logger, log_ws_manager, model_list_fetch_event, current_ai_studio_model_id, excluded_model_ids
    global request_queue, processing_lock, model_switching_lock, page_params_cache, params_cache_lock
    true_original_stdout, true_original_stderr = sys.stdout, sys.stderr
    global STREAM_QUEUE ,STREAM_PROCESS, PROXY_SERVER_ENV, STREAM_PROXY_SERVER_ENV, STREAM_PORT, PROXY_SERVER_ENV
    global PLAYWRIGHT_PROXY_SETTINGS
    initial_stdout_before_redirect, initial_stderr_before_redirect = sys.stdout, sys.stderr

    if log_ws_manager is None:
        log_ws_manager = WebSocketConnectionManager()
    log_level_env = os.environ.get('SERVER_LOG_LEVEL', 'INFO')
    redirect_print_env = os.environ.get('SERVER_REDIRECT_PRINT', 'false')
    initial_stdout_before_redirect, initial_stderr_before_redirect = setup_server_logging(
        log_level_name=log_level_env,
        redirect_print_str=redirect_print_env
    )

    PROXY_SERVER_ENV = "http://127.0.0.1:3120/"
    STREAM_PROXY_SERVER_ENV = os.environ.get('HTTPS_PROXY') or os.environ.get('HTTP_PROXY')

    STREAM_PORT = os.environ.get('STREAM_PORT')
    if STREAM_PORT == '0':
        PROXY_SERVER_ENV = os.environ.get('HTTPS_PROXY') or os.environ.get('HTTP_PROXY')
    elif STREAM_PORT is not None:
        PROXY_SERVER_ENV = f"http://127.0.0.1:{STREAM_PORT}/"

    PLAYWRIGHT_PROXY_SETTINGS = None
    if PROXY_SERVER_ENV:
        PLAYWRIGHT_PROXY_SETTINGS = {'server': PROXY_SERVER_ENV}
        if NO_PROXY_ENV:
            PLAYWRIGHT_PROXY_SETTINGS['bypass'] = NO_PROXY_ENV.replace(',', ';')

    if STREAM_PORT != '0':
        logger.info(f"STREAM ä»£ç†å¯åŠ¨ä¸­ï¼Œç«¯å£: {STREAM_PORT}")
        STREAM_QUEUE = multiprocessing.Queue()
        if STREAM_PORT is None:
            port = 3120
        else:
            port = int(STREAM_PORT)
        logger.info(f"STREAM ä»£ç†ä½¿ç”¨ä¸Šæ¸¸ä»£ç†æœåŠ¡å™¨ï¼š{STREAM_PROXY_SERVER_ENV}")
        STREAM_PROCESS = multiprocessing.Process(target=stream.start, args=(STREAM_QUEUE, port, STREAM_PROXY_SERVER_ENV))
        STREAM_PROCESS.start()
        logger.info("STREAM ä»£ç†å¯åŠ¨å®Œæ¯•")
    else:
        logger.info("STREAM ä»£ç†å·²ç¦ç”¨")

    request_queue = asyncio.Queue()
    processing_lock = asyncio.Lock()
    model_switching_lock = asyncio.Lock()
    model_list_fetch_event = asyncio.Event()
    params_cache_lock = asyncio.Lock()
    if PLAYWRIGHT_PROXY_SETTINGS:
        logger.info(f"--- ä»£ç†é…ç½®æ£€æµ‹åˆ° (ç”± server.py çš„ lifespan è®°å½•) ---")
        logger.info(f"   å°†ä½¿ç”¨ä»£ç†æœåŠ¡å™¨: {PLAYWRIGHT_PROXY_SETTINGS['server']}")
        if 'bypass' in PLAYWRIGHT_PROXY_SETTINGS:
            logger.info(f"   ç»•è¿‡ä»£ç†çš„ä¸»æœº: {PLAYWRIGHT_PROXY_SETTINGS['bypass']}")
        logger.info(f"-----------------------")
    else:
        logger.info("--- æœªæ£€æµ‹åˆ° HTTP_PROXY æˆ– HTTPS_PROXY ç¯å¢ƒå˜é‡ï¼Œä¸ä½¿ç”¨ä»£ç† (ç”± server.py çš„ lifespan è®°å½•) ---")
    load_excluded_models(EXCLUDED_MODELS_FILENAME)
    is_initializing = True
    logger.info("\n" + "="*60 + "\n          ğŸš€ AI Studio Proxy Server (FastAPI App Lifespan) ğŸš€\n" + "="*60)
    logger.info(f"FastAPI åº”ç”¨ç”Ÿå‘½å‘¨æœŸ: å¯åŠ¨ä¸­...")
    try:
        logger.info(f"   å¯åŠ¨ Playwright...")
        playwright_manager = await async_playwright().start()
        is_playwright_ready = True
        logger.info(f"   âœ… Playwright å·²å¯åŠ¨ã€‚")
        ws_endpoint = os.environ.get('CAMOUFOX_WS_ENDPOINT')
        launch_mode = os.environ.get('LAUNCH_MODE', 'unknown')
        if not ws_endpoint:
            if launch_mode == "direct_debug_no_browser":
                logger.warning("CAMOUFOX_WS_ENDPOINT æœªè®¾ç½®ï¼Œä½† LAUNCH_MODE è¡¨æ˜ä¸éœ€è¦æµè§ˆå™¨ã€‚è·³è¿‡æµè§ˆå™¨è¿æ¥ã€‚")
                is_browser_connected = False
                is_page_ready = False
                model_list_fetch_event.set()
            else:
                logger.error("æœªæ‰¾åˆ° CAMOUFOX_WS_ENDPOINT ç¯å¢ƒå˜é‡ã€‚Playwright å°†æ— æ³•è¿æ¥åˆ°æµè§ˆå™¨ã€‚")
                raise ValueError("CAMOUFOX_WS_ENDPOINT ç¯å¢ƒå˜é‡ç¼ºå¤±ã€‚")
        else:
            logger.info(f"   è¿æ¥åˆ° Camoufox æœåŠ¡å™¨ (æµè§ˆå™¨ WebSocket ç«¯ç‚¹) äº: {ws_endpoint}")
            try:
                browser_instance = await playwright_manager.firefox.connect(ws_endpoint, timeout=30000)
                is_browser_connected = True
                logger.info(f"   âœ… å·²è¿æ¥åˆ°æµè§ˆå™¨å®ä¾‹: ç‰ˆæœ¬ {browser_instance.version}")
                temp_page_instance, temp_is_page_ready = await _initialize_page_logic(browser_instance)
                if temp_page_instance and temp_is_page_ready:
                    page_instance = temp_page_instance
                    is_page_ready = temp_is_page_ready
                    await _handle_initial_model_state_and_storage(page_instance)
                else:
                    is_page_ready = False
                    if not model_list_fetch_event.is_set(): model_list_fetch_event.set()
            except Exception as connect_err:
                logger.error(f"æœªèƒ½è¿æ¥åˆ° Camoufox æœåŠ¡å™¨ (æµè§ˆå™¨) æˆ–åˆå§‹åŒ–é¡µé¢å¤±è´¥: {connect_err}", exc_info=True)
                if launch_mode != "direct_debug_no_browser":
                    raise RuntimeError(f"æœªèƒ½è¿æ¥åˆ° Camoufox æˆ–åˆå§‹åŒ–é¡µé¢: {connect_err}") from connect_err
                else:
                    is_browser_connected = False
                    is_page_ready = False
                    if not model_list_fetch_event.is_set(): model_list_fetch_event.set()
        if is_page_ready and is_browser_connected and not model_list_fetch_event.is_set():
            logger.info("ç­‰å¾…æ¨¡å‹åˆ—è¡¨æ•è· (æœ€å¤šç­‰å¾…15ç§’)...")
            try:
                await asyncio.wait_for(model_list_fetch_event.wait(), timeout=15.0)
                if model_list_fetch_event.is_set():
                    logger.info("æ¨¡å‹åˆ—è¡¨äº‹ä»¶å·²è§¦å‘ã€‚")
                else:
                    logger.warning("æ¨¡å‹åˆ—è¡¨äº‹ä»¶ç­‰å¾…åä»æœªè®¾ç½®ã€‚")
            except asyncio.TimeoutError:
                logger.warning("ç­‰å¾…æ¨¡å‹åˆ—è¡¨æ•è·è¶…æ—¶ã€‚å°†ä½¿ç”¨é»˜è®¤æˆ–ç©ºåˆ—è¡¨ã€‚")
            finally:
                if not model_list_fetch_event.is_set():
                    model_list_fetch_event.set()
        elif not (is_page_ready and is_browser_connected):
             if not model_list_fetch_event.is_set(): model_list_fetch_event.set()
        if (is_page_ready and is_browser_connected) or launch_mode == "direct_debug_no_browser":
             logger.info(f"   å¯åŠ¨è¯·æ±‚å¤„ç† Worker...")
             worker_task = asyncio.create_task(queue_worker())
             logger.info(f"   âœ… è¯·æ±‚å¤„ç† Worker å·²å¯åŠ¨ã€‚")
        elif launch_mode == "direct_debug_no_browser":
            logger.warning("æµè§ˆå™¨å’Œé¡µé¢æœªå°±ç»ª (direct_debug_no_browser æ¨¡å¼)ï¼Œè¯·æ±‚å¤„ç† Worker æœªå¯åŠ¨ã€‚API å¯èƒ½åŠŸèƒ½å—é™ã€‚")
        else:
             logger.error("é¡µé¢æˆ–æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨ Workerã€‚")
             if not model_list_fetch_event.is_set(): model_list_fetch_event.set()
             raise RuntimeError("é¡µé¢æˆ–æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨ Workerã€‚")
        logger.info(f"âœ… FastAPI åº”ç”¨ç”Ÿå‘½å‘¨æœŸ: å¯åŠ¨å®Œæˆã€‚æœåŠ¡å·²å°±ç»ªã€‚")
        is_initializing = False
        yield
    except Exception as startup_err:
        logger.critical(f"âŒ FastAPI åº”ç”¨ç”Ÿå‘½å‘¨æœŸ: å¯åŠ¨æœŸé—´å‘ç”Ÿä¸¥é‡é”™è¯¯: {startup_err}", exc_info=True)
        if not model_list_fetch_event.is_set(): model_list_fetch_event.set()
        if worker_task and not worker_task.done(): worker_task.cancel()
        if browser_instance and browser_instance.is_connected():
            try: await browser_instance.close()
            except: pass
        if playwright_manager:
            try: await playwright_manager.stop()
            except: pass
        raise RuntimeError(f"åº”ç”¨ç¨‹åºå¯åŠ¨å¤±è´¥: {startup_err}") from startup_err
    finally:
        logger.info("STREAM ä»£ç†å…³é—­ä¸­")
        STREAM_PROCESS.terminate()

        is_initializing = False
        logger.info(f"\nFastAPI åº”ç”¨ç”Ÿå‘½å‘¨æœŸ: å…³é—­ä¸­...")
        if worker_task and not worker_task.done():
             logger.info(f"   æ­£åœ¨å–æ¶ˆè¯·æ±‚å¤„ç† Worker...")
             worker_task.cancel()
             try:
                 await asyncio.wait_for(worker_task, timeout=5.0)
                 logger.info(f"   âœ… è¯·æ±‚å¤„ç† Worker å·²åœæ­¢/å–æ¶ˆã€‚")
             except asyncio.TimeoutError: logger.warning(f"   âš ï¸ Worker ç­‰å¾…è¶…æ—¶ã€‚")
             except asyncio.CancelledError: logger.info(f"   âœ… è¯·æ±‚å¤„ç† Worker å·²ç¡®è®¤å–æ¶ˆã€‚")
             except Exception as wt_err: logger.error(f"   âŒ ç­‰å¾… Worker åœæ­¢æ—¶å‡ºé”™: {wt_err}", exc_info=True)
        if page_instance and not page_instance.is_closed():
            try:
                logger.info("Lifespan æ¸…ç†ï¼šç§»é™¤æ¨¡å‹åˆ—è¡¨å“åº”ç›‘å¬å™¨ã€‚")
                page_instance.remove_listener("response", _handle_model_list_response)
            except Exception as e:
                logger.debug(f"Lifespan æ¸…ç†ï¼šç§»é™¤ç›‘å¬å™¨æ—¶å‘ç”Ÿéä¸¥é‡é”™è¯¯æˆ–ç›‘å¬å™¨æœ¬ä¸å­˜åœ¨: {e}")
        if page_instance:
            await _close_page_logic()
        if browser_instance:
            logger.info(f"   æ­£åœ¨å…³é—­ä¸æµè§ˆå™¨å®ä¾‹çš„è¿æ¥...")
            try:
                if browser_instance.is_connected():
                    await browser_instance.close()
                    logger.info(f"   âœ… æµè§ˆå™¨è¿æ¥å·²å…³é—­ã€‚")
                else: logger.info(f"   â„¹ï¸ æµè§ˆå™¨å…ˆå‰å·²æ–­å¼€è¿æ¥ã€‚")
            except Exception as close_err: logger.error(f"   âŒ å…³é—­æµè§ˆå™¨è¿æ¥æ—¶å‡ºé”™: {close_err}", exc_info=True)
            finally: browser_instance = None; is_browser_connected = False; is_page_ready = False
        if playwright_manager:
            logger.info(f"   åœæ­¢ Playwright...")
            try:
                await playwright_manager.stop()
                logger.info(f"   âœ… Playwright å·²åœæ­¢ã€‚")
            except Exception as stop_err: logger.error(f"   âŒ åœæ­¢ Playwright æ—¶å‡ºé”™: {stop_err}", exc_info=True)
            finally: playwright_manager = None; is_playwright_ready = False
        restore_original_streams(initial_stdout_before_redirect, initial_stderr_before_redirect)
        restore_original_streams(true_original_stdout, true_original_stderr)
        logger.info(f"âœ… FastAPI åº”ç”¨ç”Ÿå‘½å‘¨æœŸ: å…³é—­å®Œæˆã€‚")

# --- FastAPI App å®šä¹‰ ---
app = FastAPI(
    title="AI Studio Proxy Server (é›†æˆæ¨¡å¼)",
    description="é€šè¿‡ Playwrightä¸ AI Studio äº¤äº’çš„ä»£ç†æœåŠ¡å™¨ã€‚",
    version="0.6.0-integrated",
    lifespan=lifespan
)

# --- API Endpoints ---
@app.get("/", response_class=FileResponse)
async def read_index():
    index_html_path = os.path.join(os.path.dirname(__file__), "index.html")
    if not os.path.exists(index_html_path):
        logger.error(f"index.html not found at {index_html_path}")
        raise HTTPException(status_code=404, detail="index.html not found")
    return FileResponse(index_html_path)

@app.get("/webui.css")
async def get_css():
    css_path = os.path.join(os.path.dirname(__file__), "webui.css")
    if not os.path.exists(css_path):
        logger.error(f"webui.css not found at {css_path}")
        raise HTTPException(status_code=404, detail="webui.css not found")
    return FileResponse(css_path, media_type="text/css")

@app.get("/webui.js")
async def get_js():
    js_path = os.path.join(os.path.dirname(__file__), "webui.js")
    if not os.path.exists(js_path):
        logger.error(f"webui.js not found at {js_path}")
        raise HTTPException(status_code=404, detail="webui.js not found")
    return FileResponse(js_path, media_type="application/javascript")

@app.get("/api/info")
async def get_api_info(request: Request):
    server_port = request.url.port
    if not server_port and hasattr(request.app.state, 'server_port'):
        server_port = request.app.state.server_port
    if not server_port:
        server_port = os.environ.get('SERVER_PORT_INFO', '8000')
    host = request.headers.get('host') or f"127.0.0.1:{server_port}"
    scheme = request.headers.get('x-forwarded-proto', 'http')
    base_url = f"{scheme}://{host}"
    api_base = f"{base_url}/v1"
    effective_model_name = current_ai_studio_model_id if current_ai_studio_model_id else MODEL_NAME
    return JSONResponse(content={
        "model_name": effective_model_name,
        "api_base_url": api_base,
        "server_base_url": base_url,
        "api_key_required": False,
        "message": "API Key is not required."
    })

@app.get("/health")
async def health_check():
    is_worker_running = bool(worker_task and not worker_task.done())
    launch_mode = os.environ.get('LAUNCH_MODE', 'unknown')
    browser_page_critical = launch_mode != "direct_debug_no_browser"
    core_ready_conditions = [not is_initializing, is_playwright_ready]
    if browser_page_critical:
        core_ready_conditions.extend([is_browser_connected, is_page_ready])
    is_core_ready = all(core_ready_conditions)
    status_val = "OK" if is_core_ready and is_worker_running else "Error"
    q_size = request_queue.qsize() if request_queue else -1
    status_message_parts = []
    if is_initializing: status_message_parts.append("åˆå§‹åŒ–è¿›è¡Œä¸­")
    if not is_playwright_ready: status_message_parts.append("Playwright æœªå°±ç»ª")
    if browser_page_critical:
        if not is_browser_connected: status_message_parts.append("æµè§ˆå™¨æœªè¿æ¥")
        if not is_page_ready: status_message_parts.append("é¡µé¢æœªå°±ç»ª")
    if not is_worker_running: status_message_parts.append("Worker æœªè¿è¡Œ")
    status = {
        "status": status_val,
        "message": "",
        "details": {
            "playwrightReady": is_playwright_ready,
            "browserConnected": is_browser_connected,
            "pageReady": is_page_ready,
            "initializing": is_initializing,
            "workerRunning": is_worker_running,
            "queueLength": q_size,
            "launchMode": launch_mode,
            "browserAndPageCritical": browser_page_critical
        }
    }
    if status_val == "OK":
        status["message"] = f"æœåŠ¡è¿è¡Œä¸­;é˜Ÿåˆ—é•¿åº¦: {q_size}ã€‚"
        return JSONResponse(content=status, status_code=200)
    else:
        status["message"] = f"æœåŠ¡ä¸å¯ç”¨;é—®é¢˜: {(', '.join(status_message_parts) if status_message_parts else 'æœªçŸ¥åŸå› ')}. é˜Ÿåˆ—é•¿åº¦: {q_size}."
        return JSONResponse(content=status, status_code=503)

@app.get("/v1/models")
async def list_models():
    logger.info("[API] æ”¶åˆ° /v1/models è¯·æ±‚ã€‚")
    if not model_list_fetch_event.is_set() and page_instance and not page_instance.is_closed():
        logger.info("/v1/models: æ¨¡å‹åˆ—è¡¨äº‹ä»¶æœªè®¾ç½®æˆ–åˆ—è¡¨ä¸ºç©ºï¼Œå°è¯•é¡µé¢åˆ·æ–°ä»¥è§¦å‘æ•è·...")
        try:
            listener_attached = False
            if hasattr(page_instance, '_events') and "response" in page_instance._events:
                for handler_slot_or_func in page_instance._events["response"]:
                    actual_handler = getattr(handler_slot_or_func, 'handler', handler_slot_or_func)
                    if actual_handler == _handle_model_list_response:
                        listener_attached = True
                        break
            if not listener_attached:
                logger.info("/v1/models: å“åº”ç›‘å¬å™¨ä¼¼ä¹ä¸å­˜åœ¨æˆ–å·²è¢«ç§»é™¤ï¼Œå°è¯•é‡æ–°æ·»åŠ ã€‚")
                page_instance.on("response", _handle_model_list_response)
            await page_instance.reload(wait_until="domcontentloaded", timeout=20000)
            logger.info(f"é¡µé¢å·²åˆ·æ–°ã€‚ç­‰å¾…æ¨¡å‹åˆ—è¡¨äº‹ä»¶ (æœ€å¤š10ç§’)...")
            await asyncio.wait_for(model_list_fetch_event.wait(), timeout=10.0)
        except asyncio.TimeoutError:
            logger.warning("/v1/models: åˆ·æ–°åç­‰å¾…æ¨¡å‹åˆ—è¡¨äº‹ä»¶è¶…æ—¶ã€‚")
        except PlaywrightAsyncError as reload_err:
            logger.error(f"/v1/models: åˆ·æ–°é¡µé¢å¤±è´¥: {reload_err}")
        except Exception as e:
            logger.error(f"/v1/models: å°è¯•è§¦å‘æ¨¡å‹åˆ—è¡¨æ•è·æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            if not model_list_fetch_event.is_set():
                logger.info("/v1/models: å°è¯•æ•è·åï¼Œå¼ºåˆ¶è®¾ç½®æ¨¡å‹åˆ—è¡¨äº‹ä»¶ã€‚")
                model_list_fetch_event.set()
    if parsed_model_list:
        final_model_list = [m for m in parsed_model_list if m.get("id") not in excluded_model_ids]
        logger.info(f"è¿”å›è¿‡æ»¤åçš„ {len(final_model_list)} ä¸ªæ¨¡å‹ (åŸç¼“å­˜ {len(parsed_model_list)} ä¸ª)ã€‚æ’é™¤çš„æœ‰: {excluded_model_ids.intersection(set(m.get('id') for m in parsed_model_list))}")
        return {"object": "list", "data": final_model_list}
    else:
        logger.warning("æ¨¡å‹åˆ—è¡¨ä¸ºç©ºæˆ–æœªæˆåŠŸè·å–ã€‚è¿”å›é»˜è®¤åå¤‡æ¨¡å‹ã€‚")
        fallback_model_obj = {
            "id": DEFAULT_FALLBACK_MODEL_ID,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "camoufox-proxy-fallback",
            "display_name": DEFAULT_FALLBACK_MODEL_ID.replace("-", " ").title(),
            "description": "Default fallback model.",
            "raw_model_path": f"models/{DEFAULT_FALLBACK_MODEL_ID}"
        }
        return {"object": "list", "data": [fallback_model_obj]}

# --- Helper: Detect Error ---
async def detect_and_extract_page_error(page: AsyncPage, req_id: str) -> Optional[str]:
    error_toast_locator = page.locator(ERROR_TOAST_SELECTOR).last
    try:
        await error_toast_locator.wait_for(state='visible', timeout=500)
        message_locator = error_toast_locator.locator('span.content-text')
        error_message = await message_locator.text_content(timeout=500)
        if error_message:
             logger.error(f"[{req_id}]    æ£€æµ‹åˆ°å¹¶æå–é”™è¯¯æ¶ˆæ¯: {error_message}")
             return error_message.strip()
        else:
             logger.warning(f"[{req_id}]    æ£€æµ‹åˆ°é”™è¯¯æç¤ºæ¡†ï¼Œä½†æ— æ³•æå–æ¶ˆæ¯ã€‚")
             return "æ£€æµ‹åˆ°é”™è¯¯æç¤ºæ¡†ï¼Œä½†æ— æ³•æå–ç‰¹å®šæ¶ˆæ¯ã€‚"
    except PlaywrightAsyncError: return None
    except Exception as e:
        logger.warning(f"[{req_id}]    æ£€æŸ¥é¡µé¢é”™è¯¯æ—¶å‡ºé”™: {e}")
        return None

# --- Snapshot Helper ---
async def save_error_snapshot(error_name: str = 'error'):
    name_parts = error_name.split('_')
    req_id = name_parts[-1] if len(name_parts) > 1 and len(name_parts[-1]) == 7 else None
    base_error_name = error_name if not req_id else '_'.join(name_parts[:-1])
    log_prefix = f"[{req_id}]" if req_id else "[æ— è¯·æ±‚ID]"
    page_to_snapshot = page_instance
    if not browser_instance or not browser_instance.is_connected() or not page_to_snapshot or page_to_snapshot.is_closed():
        logger.warning(f"{log_prefix} æ— æ³•ä¿å­˜å¿«ç…§ ({base_error_name})ï¼Œæµè§ˆå™¨/é¡µé¢ä¸å¯ç”¨ã€‚")
        return
    logger.info(f"{log_prefix} å°è¯•ä¿å­˜é”™è¯¯å¿«ç…§ ({base_error_name})...")
    timestamp = int(time.time() * 1000)
    error_dir = os.path.join(os.path.dirname(__file__), 'errors_py')
    try:
        os.makedirs(error_dir, exist_ok=True)
        filename_suffix = f"{req_id}_{timestamp}" if req_id else f"{timestamp}"
        filename_base = f"{base_error_name}_{filename_suffix}"
        screenshot_path = os.path.join(error_dir, f"{filename_base}.png")
        html_path = os.path.join(error_dir, f"{filename_base}.html")
        try:
            await page_to_snapshot.screenshot(path=screenshot_path, full_page=True, timeout=15000)
            logger.info(f"{log_prefix}   å¿«ç…§å·²ä¿å­˜åˆ°: {screenshot_path}")
        except Exception as ss_err:
            logger.error(f"{log_prefix}   ä¿å­˜å±å¹•æˆªå›¾å¤±è´¥ ({base_error_name}): {ss_err}")
        try:
            content = await page_to_snapshot.content()
            f = None
            try:
                f = open(html_path, 'w', encoding='utf-8')
                f.write(content)
                logger.info(f"{log_prefix}   HTML å·²ä¿å­˜åˆ°: {html_path}")
            except Exception as write_err:
                logger.error(f"{log_prefix}   ä¿å­˜ HTML å¤±è´¥ ({base_error_name}): {write_err}")
            finally:
                if f:
                    try:
                        f.close()
                        logger.debug(f"{log_prefix}   HTML æ–‡ä»¶å·²æ­£ç¡®å…³é—­")
                    except Exception as close_err:
                        logger.error(f"{log_prefix}   å…³é—­ HTML æ–‡ä»¶æ—¶å‡ºé”™: {close_err}")
        except Exception as html_err:
            logger.error(f"{log_prefix}   è·å–é¡µé¢å†…å®¹å¤±è´¥ ({base_error_name}): {html_err}")
    except Exception as dir_err:
        logger.error(f"{log_prefix}   åˆ›å»ºé”™è¯¯ç›®å½•æˆ–ä¿å­˜å¿«ç…§æ—¶å‘ç”Ÿå…¶ä»–é”™è¯¯ ({base_error_name}): {dir_err}")

# --- Get response via Edit Button ---
async def get_response_via_edit_button(
    page: AsyncPage,
    req_id: str,
    check_client_disconnected: Callable
) -> Optional[str]:
    logger.info(f"[{req_id}] (Helper) å°è¯•é€šè¿‡ç¼–è¾‘æŒ‰é’®è·å–å“åº”...")
    last_message_container = page.locator('ms-chat-turn').last
    edit_button = last_message_container.get_by_label("Edit")
    finish_edit_button = last_message_container.get_by_label("Stop editing")
    autosize_textarea_locator = last_message_container.locator('ms-autosize-textarea')
    actual_textarea_locator = autosize_textarea_locator.locator('textarea')
    try:
        logger.info(f"[{req_id}]   - å°è¯•æ‚¬åœæœ€åä¸€æ¡æ¶ˆæ¯ä»¥æ˜¾ç¤º 'Edit' æŒ‰é’®...")
        try:
            # å¯¹æ¶ˆæ¯å®¹å™¨æ‰§è¡Œæ‚¬åœæ“ä½œ
            await last_message_container.hover(timeout=CLICK_TIMEOUT_MS / 2) # ä½¿ç”¨ä¸€åŠçš„ç‚¹å‡»è¶…æ—¶ä½œä¸ºæ‚¬åœè¶…æ—¶
            await asyncio.sleep(0.3) # ç­‰å¾…æ‚¬åœæ•ˆæœç”Ÿæ•ˆ
            check_client_disconnected("ç¼–è¾‘å“åº” - æ‚¬åœå: ")
        except Exception as hover_err:
            logger.warning(f"[{req_id}]   - (get_response_via_edit_button) æ‚¬åœæœ€åä¸€æ¡æ¶ˆæ¯å¤±è´¥ (å¿½ç•¥): {type(hover_err).__name__}")
            # å³ä½¿æ‚¬åœå¤±è´¥ï¼Œä¹Ÿç»§ç»­å°è¯•åç»­æ“ä½œï¼ŒPlaywrightçš„expect_asyncå¯èƒ½ä¼šå¤„ç†
        
        logger.info(f"[{req_id}]   - å®šä½å¹¶ç‚¹å‡» 'Edit' æŒ‰é’®...")
        try:
            await expect_async(edit_button).to_be_visible(timeout=CLICK_TIMEOUT_MS)
            check_client_disconnected("ç¼–è¾‘å“åº” - 'Edit' æŒ‰é’®å¯è§å: ")
            await edit_button.click(timeout=CLICK_TIMEOUT_MS)
            logger.info(f"[{req_id}]   - 'Edit' æŒ‰é’®å·²ç‚¹å‡»ã€‚")
        except Exception as edit_btn_err:
            logger.error(f"[{req_id}]   - 'Edit' æŒ‰é’®ä¸å¯è§æˆ–ç‚¹å‡»å¤±è´¥: {edit_btn_err}")
            await save_error_snapshot(f"edit_response_edit_button_failed_{req_id}")
            return None
        check_client_disconnected("ç¼–è¾‘å“åº” - ç‚¹å‡» 'Edit' æŒ‰é’®å: ")
        await asyncio.sleep(0.3)
        check_client_disconnected("ç¼–è¾‘å“åº” - ç‚¹å‡» 'Edit' æŒ‰é’®åå»¶æ—¶å: ")
        logger.info(f"[{req_id}]   - ä»æ–‡æœ¬åŒºåŸŸè·å–å†…å®¹...")
        response_content = None
        textarea_failed = False
        try:
            await expect_async(autosize_textarea_locator).to_be_visible(timeout=CLICK_TIMEOUT_MS)
            check_client_disconnected("ç¼–è¾‘å“åº” - autosize-textarea å¯è§å: ")
            try:
                data_value_content = await autosize_textarea_locator.get_attribute("data-value")
                check_client_disconnected("ç¼–è¾‘å“åº” - get_attribute data-value å: ")
                if data_value_content is not None:
                    response_content = str(data_value_content)
                    logger.info(f"[{req_id}]   - ä» data-value è·å–å†…å®¹æˆåŠŸã€‚")
            except Exception as data_val_err:
                logger.warning(f"[{req_id}]   - è·å– data-value å¤±è´¥: {data_val_err}")
                check_client_disconnected("ç¼–è¾‘å“åº” - get_attribute data-value é”™è¯¯å: ")
            if response_content is None:
                logger.info(f"[{req_id}]   - data-value è·å–å¤±è´¥æˆ–ä¸ºNoneï¼Œå°è¯•ä»å†…éƒ¨ textarea è·å– input_value...")
                try:
                    await expect_async(actual_textarea_locator).to_be_visible(timeout=CLICK_TIMEOUT_MS/2)
                    input_val_content = await actual_textarea_locator.input_value(timeout=CLICK_TIMEOUT_MS/2)
                    check_client_disconnected("ç¼–è¾‘å“åº” - input_value å: ")
                    if input_val_content is not None:
                        response_content = str(input_val_content)
                        logger.info(f"[{req_id}]   - ä» input_value è·å–å†…å®¹æˆåŠŸã€‚")
                except Exception as input_val_err:
                     logger.warning(f"[{req_id}]   - è·å– input_value ä¹Ÿå¤±è´¥: {input_val_err}")
                     check_client_disconnected("ç¼–è¾‘å“åº” - input_value é”™è¯¯å: ")
            if response_content is not None:
                response_content = response_content.strip()
                content_preview = response_content[:100].replace('\\n', '\\\\n')
                logger.info(f"[{req_id}]   - âœ… æœ€ç»ˆè·å–å†…å®¹ (é•¿åº¦={len(response_content)}): '{content_preview}...'")
            else:
                logger.warning(f"[{req_id}]   - æ‰€æœ‰æ–¹æ³• (data-value, input_value) å†…å®¹è·å–å‡å¤±è´¥æˆ–è¿”å› Noneã€‚")
                textarea_failed = True
        except Exception as textarea_err:
            logger.error(f"[{req_id}]   - å®šä½æˆ–å¤„ç†æ–‡æœ¬åŒºåŸŸæ—¶å¤±è´¥: {textarea_err}")
            textarea_failed = True
            response_content = None
            check_client_disconnected("ç¼–è¾‘å“åº” - è·å–æ–‡æœ¬åŒºåŸŸé”™è¯¯å: ")
        if not textarea_failed:
            logger.info(f"[{req_id}]   - å®šä½å¹¶ç‚¹å‡» 'Stop editing' æŒ‰é’®...")
            try:
                await expect_async(finish_edit_button).to_be_visible(timeout=CLICK_TIMEOUT_MS)
                check_client_disconnected("ç¼–è¾‘å“åº” - 'Stop editing' æŒ‰é’®å¯è§å: ")
                await finish_edit_button.click(timeout=CLICK_TIMEOUT_MS)
                logger.info(f"[{req_id}]   - 'Stop editing' æŒ‰é’®å·²ç‚¹å‡»ã€‚")
            except Exception as finish_btn_err:
                logger.warning(f"[{req_id}]   - 'Stop editing' æŒ‰é’®ä¸å¯è§æˆ–ç‚¹å‡»å¤±è´¥: {finish_btn_err}")
                await save_error_snapshot(f"edit_response_finish_button_failed_{req_id}")
            check_client_disconnected("ç¼–è¾‘å“åº” - ç‚¹å‡» 'Stop editing' å: ")
            await asyncio.sleep(0.2)
            check_client_disconnected("ç¼–è¾‘å“åº” - ç‚¹å‡» 'Stop editing' åå»¶æ—¶å: ")
        else:
             logger.info(f"[{req_id}]   - è·³è¿‡ç‚¹å‡» 'Stop editing' æŒ‰é’®ï¼Œå› ä¸ºæ–‡æœ¬åŒºåŸŸè¯»å–å¤±è´¥ã€‚")
        return response_content
    except ClientDisconnectedError:
        logger.info(f"[{req_id}] (Helper Edit) å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ã€‚")
        raise
    except Exception as e:
        logger.exception(f"[{req_id}] é€šè¿‡ç¼–è¾‘æŒ‰é’®è·å–å“åº”è¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯")
        await save_error_snapshot(f"edit_response_unexpected_error_{req_id}")
        return None

# --- Get response via Copy Button ---
async def get_response_via_copy_button(
    page: AsyncPage,
    req_id: str,
    check_client_disconnected: Callable
) -> Optional[str]:
    logger.info(f"[{req_id}] (Helper) å°è¯•é€šè¿‡å¤åˆ¶æŒ‰é’®è·å–å“åº”...")
    last_message_container = page.locator('ms-chat-turn').last
    more_options_button = last_message_container.get_by_label("Open options")
    copy_markdown_button = page.get_by_role("menuitem", name="Copy markdown")
    try:
        logger.info(f"[{req_id}]   - å°è¯•æ‚¬åœæœ€åä¸€æ¡æ¶ˆæ¯ä»¥æ˜¾ç¤ºé€‰é¡¹...")
        await last_message_container.hover(timeout=CLICK_TIMEOUT_MS)
        check_client_disconnected("å¤åˆ¶å“åº” - æ‚¬åœå: ")
        await asyncio.sleep(0.5)
        check_client_disconnected("å¤åˆ¶å“åº” - æ‚¬åœåå»¶æ—¶å: ")
        logger.info(f"[{req_id}]   - å·²æ‚¬åœã€‚")
        logger.info(f"[{req_id}]   - å®šä½å¹¶ç‚¹å‡» 'æ›´å¤šé€‰é¡¹' æŒ‰é’®...")
        try:
            await expect_async(more_options_button).to_be_visible(timeout=CLICK_TIMEOUT_MS)
            check_client_disconnected("å¤åˆ¶å“åº” - æ›´å¤šé€‰é¡¹æŒ‰é’®å¯è§å: ")
            await more_options_button.click(timeout=CLICK_TIMEOUT_MS)
            logger.info(f"[{req_id}]   - 'æ›´å¤šé€‰é¡¹' å·²ç‚¹å‡» (é€šè¿‡ get_by_label)ã€‚")
        except Exception as more_opts_err:
            logger.error(f"[{req_id}]   - 'æ›´å¤šé€‰é¡¹' æŒ‰é’® (é€šè¿‡ get_by_label) ä¸å¯è§æˆ–ç‚¹å‡»å¤±è´¥: {more_opts_err}")
            await save_error_snapshot(f"copy_response_more_options_failed_{req_id}")
            return None
        check_client_disconnected("å¤åˆ¶å“åº” - ç‚¹å‡»æ›´å¤šé€‰é¡¹å: ")
        await asyncio.sleep(0.5)
        check_client_disconnected("å¤åˆ¶å“åº” - ç‚¹å‡»æ›´å¤šé€‰é¡¹åå»¶æ—¶å: ")
        logger.info(f"[{req_id}]   - å®šä½å¹¶ç‚¹å‡» 'å¤åˆ¶ Markdown' æŒ‰é’®...")
        copy_success = False
        try:
            await expect_async(copy_markdown_button).to_be_visible(timeout=CLICK_TIMEOUT_MS)
            check_client_disconnected("å¤åˆ¶å“åº” - å¤åˆ¶æŒ‰é’®å¯è§å: ")
            await copy_markdown_button.click(timeout=CLICK_TIMEOUT_MS, force=True)
            copy_success = True
            logger.info(f"[{req_id}]   - å·²ç‚¹å‡» 'å¤åˆ¶ Markdown' (é€šè¿‡ get_by_role)ã€‚")
        except Exception as copy_err:
            logger.error(f"[{req_id}]   - 'å¤åˆ¶ Markdown' æŒ‰é’® (é€šè¿‡ get_by_role) ç‚¹å‡»å¤±è´¥: {copy_err}")
            await save_error_snapshot(f"copy_response_copy_button_failed_{req_id}")
            return None
        if not copy_success:
             logger.error(f"[{req_id}]   - æœªèƒ½ç‚¹å‡» 'å¤åˆ¶ Markdown' æŒ‰é’®ã€‚")
             return None
        check_client_disconnected("å¤åˆ¶å“åº” - ç‚¹å‡»å¤åˆ¶æŒ‰é’®å: ")
        await asyncio.sleep(0.5)
        check_client_disconnected("å¤åˆ¶å“åº” - ç‚¹å‡»å¤åˆ¶æŒ‰é’®åå»¶æ—¶å: ")
        logger.info(f"[{req_id}]   - æ­£åœ¨è¯»å–å‰ªè´´æ¿å†…å®¹...")
        try:
            clipboard_content = await page.evaluate('navigator.clipboard.readText()')
            check_client_disconnected("å¤åˆ¶å“åº” - è¯»å–å‰ªè´´æ¿å: ")
            if clipboard_content:
                content_preview = clipboard_content[:100].replace('\n', '\\\\n')
                logger.info(f"[{req_id}]   - âœ… æˆåŠŸè·å–å‰ªè´´æ¿å†…å®¹ (é•¿åº¦={len(clipboard_content)}): '{content_preview}...'")
                return clipboard_content
            else:
                logger.error(f"[{req_id}]   - å‰ªè´´æ¿å†…å®¹ä¸ºç©ºã€‚")
                return None
        except Exception as clipboard_err:
            if "clipboard-read" in str(clipboard_err):
                 logger.error(f"[{req_id}]   - è¯»å–å‰ªè´´æ¿å¤±è´¥: å¯èƒ½æ˜¯æƒé™é—®é¢˜ã€‚é”™è¯¯: {clipboard_err}")
            else:
                 logger.error(f"[{req_id}]   - è¯»å–å‰ªè´´æ¿å¤±è´¥: {clipboard_err}")
            await save_error_snapshot(f"copy_response_clipboard_read_failed_{req_id}")
            return None
    except ClientDisconnectedError:
        logger.info(f"[{req_id}] (Helper Copy) å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ã€‚")
        raise
    except Exception as e:
        logger.exception(f"[{req_id}] å¤åˆ¶å“åº”è¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯")
        await save_error_snapshot(f"copy_response_unexpected_error_{req_id}")
        return None

# --- Wait for Response Completion ---
async def _wait_for_response_completion(
    page: AsyncPage,
    prompt_textarea_locator: Locator,
    submit_button_locator: Locator,
    edit_button_locator: Locator,
    req_id: str,
    check_client_disconnected_func: Callable,
    current_chat_id: Optional[str],
    timeout_ms=RESPONSE_COMPLETION_TIMEOUT,
    initial_wait_ms=INITIAL_WAIT_MS_BEFORE_POLLING
) -> bool:
    logger.info(f"[{req_id}] (WaitV3) å¼€å§‹ç­‰å¾…å“åº”å®Œæˆ... (è¶…æ—¶: {timeout_ms}ms)")
    await asyncio.sleep(initial_wait_ms / 1000) # Initial brief wait
    
    start_time = time.time()
    wait_timeout_ms_short = 3000 # 3 seconds for individual element checks
    
    consecutive_empty_input_submit_disabled_count = 0
    
    while True:
        if check_client_disconnected_func(current_chat_id, req_id):
            logger.info(f"[{req_id}] (WaitV3) å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ï¼Œä¸­æ­¢ç­‰å¾…ã€‚")
            return False

        current_time_elapsed_ms = (time.time() - start_time) * 1000
        if current_time_elapsed_ms > timeout_ms:
            logger.error(f"[{req_id}] (WaitV3) ç­‰å¾…å“åº”å®Œæˆè¶…æ—¶ ({timeout_ms}ms)ã€‚")
            await save_error_snapshot(f"wait_completion_v3_overall_timeout_{req_id}")
            return False

        if check_client_disconnected_func(current_chat_id, req_id): return False

        # --- ä¸»è¦æ¡ä»¶: è¾“å…¥æ¡†ç©º & æäº¤æŒ‰é’®ç¦ç”¨ ---
        is_input_empty = await prompt_textarea_locator.input_value() == ""
        is_submit_disabled = False
        try:
            is_submit_disabled = await submit_button_locator.is_disabled(timeout=wait_timeout_ms_short)
        except TimeoutError:
            logger.warning(f"[{req_id}] (WaitV3) æ£€æŸ¥æäº¤æŒ‰é’®æ˜¯å¦ç¦ç”¨è¶…æ—¶ã€‚ä¸ºæœ¬æ¬¡æ£€æŸ¥å‡å®šå…¶æœªç¦ç”¨ã€‚")
        
        if check_client_disconnected_func(current_chat_id, req_id): return False

        if is_input_empty and is_submit_disabled:
            consecutive_empty_input_submit_disabled_count += 1
            if DEBUG_LOGS_ENABLED:
                logger.debug(f"[{req_id}] (WaitV3) ä¸»è¦æ¡ä»¶æ»¡è¶³: è¾“å…¥æ¡†ç©ºï¼Œæäº¤æŒ‰é’®ç¦ç”¨ (è®¡æ•°: {consecutive_empty_input_submit_disabled_count})ã€‚")

            # --- æœ€ç»ˆç¡®è®¤: ç¼–è¾‘æŒ‰é’®å¯è§ ---
            try:
                if await edit_button_locator.is_visible(timeout=wait_timeout_ms_short):
                    logger.info(f"[{req_id}] (WaitV3) âœ… å“åº”å®Œæˆ: è¾“å…¥æ¡†ç©ºï¼Œæäº¤æŒ‰é’®ç¦ç”¨ï¼Œç¼–è¾‘æŒ‰é’®å¯è§ã€‚")
                    return True # æ˜ç¡®å®Œæˆ
            except TimeoutError:
                if DEBUG_LOGS_ENABLED:
                    logger.debug(f"[{req_id}] (WaitV3) ä¸»è¦æ¡ä»¶æ»¡è¶³åï¼Œæ£€æŸ¥ç¼–è¾‘æŒ‰é’®å¯è§æ€§è¶…æ—¶ã€‚")
            
            if check_client_disconnected_func(current_chat_id, req_id): return False

            # å¯å‘å¼å®Œæˆ: å¦‚æœä¸»è¦æ¡ä»¶æŒç»­æ»¡è¶³ï¼Œä½†ç¼–è¾‘æŒ‰é’®ä»æœªå‡ºç°
            if consecutive_empty_input_submit_disabled_count >= 3: # ä¾‹å¦‚ï¼Œå¤§çº¦ 1.5ç§’ (3 * 0.5ç§’è½®è¯¢)
                logger.warning(f"[{req_id}] (WaitV3) å“åº”å¯èƒ½å·²å®Œæˆ (å¯å‘å¼): è¾“å…¥æ¡†ç©ºï¼Œæäº¤æŒ‰é’®ç¦ç”¨ï¼Œä½†åœ¨ {consecutive_empty_input_submit_disabled_count} æ¬¡æ£€æŸ¥åç¼–è¾‘æŒ‰é’®ä»æœªå‡ºç°ã€‚å‡å®šå®Œæˆã€‚åç»­è‹¥å†…å®¹è·å–å¤±è´¥ï¼Œå¯èƒ½ä¸æ­¤æœ‰å…³ã€‚")
                # ä¸å†åœ¨æ­¤å¤„ä¿å­˜å¿«ç…§: await save_error_snapshot(f"wait_completion_v3_heuristic_no_edit_{req_id}")
                return True # å¯å‘å¼å®Œæˆ
        else: # ä¸»è¦æ¡ä»¶ (è¾“å…¥æ¡†ç©º & æäº¤æŒ‰é’®ç¦ç”¨) æœªæ»¡è¶³
            consecutive_empty_input_submit_disabled_count = 0 # é‡ç½®è®¡æ•°å™¨
            if DEBUG_LOGS_ENABLED:
                reasons = []
                if not is_input_empty: reasons.append("è¾“å…¥æ¡†éç©º")
                if not is_submit_disabled: reasons.append("æäº¤æŒ‰é’®éç¦ç”¨")
                logger.debug(f"[{req_id}] (WaitV3) ä¸»è¦æ¡ä»¶æœªæ»¡è¶³ ({', '.join(reasons)}). ç»§ç»­è½®è¯¢...")

        await asyncio.sleep(0.5) # è½®è¯¢é—´éš”

# --- Get Final Response Content ---
async def _get_final_response_content(
    page: AsyncPage,
    req_id: str,
    check_client_disconnected: Callable
) -> Optional[str]:
    logger.info(f"[{req_id}] (Helper GetContent) å¼€å§‹è·å–æœ€ç»ˆå“åº”å†…å®¹...")
    response_content = await get_response_via_edit_button(
        page, req_id, check_client_disconnected
    )
    if response_content is not None:
        logger.info(f"[{req_id}] (Helper GetContent) âœ… æˆåŠŸé€šè¿‡ç¼–è¾‘æŒ‰é’®è·å–å†…å®¹ã€‚")
        return response_content
    logger.warning(f"[{req_id}] (Helper GetContent) ç¼–è¾‘æŒ‰é’®æ–¹æ³•å¤±è´¥æˆ–è¿”å›ç©ºï¼Œå›é€€åˆ°å¤åˆ¶æŒ‰é’®æ–¹æ³•...")
    response_content = await get_response_via_copy_button(
        page, req_id, check_client_disconnected
    )
    if response_content is not None:
        logger.info(f"[{req_id}] (Helper GetContent) âœ… æˆåŠŸé€šè¿‡å¤åˆ¶æŒ‰é’®è·å–å†…å®¹ã€‚")
        return response_content
    logger.error(f"[{req_id}] (Helper GetContent) æ‰€æœ‰è·å–å“åº”å†…å®¹çš„æ–¹æ³•å‡å¤±è´¥ã€‚")
    await save_error_snapshot(f"get_content_all_methods_failed_{req_id}")
    return None

# --- Queue Worker ---
async def queue_worker():
    logger.info("--- é˜Ÿåˆ— Worker å·²å¯åŠ¨ ---")
    was_last_request_streaming = False
    last_request_completion_time = 0
    while True:
        request_item = None; result_future = None; req_id = "UNKNOWN"; completion_event = None
        try:
            queue_size = request_queue.qsize()
            if queue_size > 0:
                checked_count = 0
                items_to_requeue = []
                processed_ids = set()
                while checked_count < queue_size and checked_count < 10:
                    try:
                        item = request_queue.get_nowait()
                        item_req_id = item.get("req_id", "unknown")
                        if item_req_id in processed_ids:
                             items_to_requeue.append(item)
                             continue
                        processed_ids.add(item_req_id)
                        if not item.get("cancelled", False):
                            item_http_request = item.get("http_request")
                            if item_http_request:
                                try:
                                    if await item_http_request.is_disconnected():
                                        logger.info(f"[{item_req_id}] (Worker Queue Check) æ£€æµ‹åˆ°å®¢æˆ·ç«¯å·²æ–­å¼€ï¼Œæ ‡è®°ä¸ºå–æ¶ˆã€‚")
                                        item["cancelled"] = True
                                        item_future = item.get("result_future")
                                        if item_future and not item_future.done():
                                            item_future.set_exception(HTTPException(status_code=499, detail=f"[{item_req_id}] Client disconnected while queued."))
                                except Exception as check_err:
                                    logger.error(f"[{item_req_id}] (Worker Queue Check) Error checking disconnect: {check_err}")
                        items_to_requeue.append(item)
                        checked_count += 1
                    except asyncio.QueueEmpty:
                        break
                for item in items_to_requeue:
                    await request_queue.put(item)
            request_item = await request_queue.get()
            req_id = request_item["req_id"]
            request_data = request_item["request_data"]
            http_request = request_item["http_request"]
            result_future = request_item["result_future"]
            if request_item.get("cancelled", False):
                logger.info(f"[{req_id}] (Worker) è¯·æ±‚å·²å–æ¶ˆï¼Œè·³è¿‡ã€‚")
                if not result_future.done(): result_future.set_exception(HTTPException(status_code=499, detail=f"[{req_id}] è¯·æ±‚å·²è¢«ç”¨æˆ·å–æ¶ˆ"))
                request_queue.task_done(); continue
            is_streaming_request = request_data.stream
            logger.info(f"[{req_id}] (Worker) å–å‡ºè¯·æ±‚ã€‚æ¨¡å¼: {'æµå¼' if is_streaming_request else 'éæµå¼'}")
            current_time = time.time()
            if was_last_request_streaming and is_streaming_request and (current_time - last_request_completion_time < 1.0):
                delay_time = max(0.5, 1.0 - (current_time - last_request_completion_time))
                logger.info(f"[{req_id}] (Worker) è¿ç»­æµå¼è¯·æ±‚ï¼Œæ·»åŠ  {delay_time:.2f}s å»¶è¿Ÿ...")
                await asyncio.sleep(delay_time)
            if await http_request.is_disconnected():
                 logger.info(f"[{req_id}] (Worker) å®¢æˆ·ç«¯åœ¨ç­‰å¾…é”æ—¶æ–­å¼€ã€‚å–æ¶ˆã€‚")
                 if not result_future.done(): result_future.set_exception(HTTPException(status_code=499, detail=f"[{req_id}] å®¢æˆ·ç«¯å…³é—­äº†è¯·æ±‚"))
                 request_queue.task_done(); continue
            logger.info(f"[{req_id}] (Worker) ç­‰å¾…å¤„ç†é”...")
            async with processing_lock:
                logger.info(f"[{req_id}] (Worker) å·²è·å–å¤„ç†é”ã€‚å¼€å§‹æ ¸å¿ƒå¤„ç†...")
                if await http_request.is_disconnected():
                     logger.info(f"[{req_id}] (Worker) å®¢æˆ·ç«¯åœ¨è·å–é”åæ–­å¼€ã€‚å–æ¶ˆã€‚")
                     if not result_future.done(): result_future.set_exception(HTTPException(status_code=499, detail=f"[{req_id}] å®¢æˆ·ç«¯å…³é—­äº†è¯·æ±‚"))
                elif result_future.done():
                     logger.info(f"[{req_id}] (Worker) Future åœ¨å¤„ç†å‰å·²å®Œæˆ/å–æ¶ˆã€‚è·³è¿‡ã€‚")
                else:
                    returned_value = await _process_request_refactored(
                        req_id, request_data, http_request, result_future
                    )

                    completion_event, submit_btn_loc, client_disco_checker = None, None, None
                    current_request_was_streaming = False # Variable to track if the current request was streaming

                    if isinstance(returned_value, tuple) and len(returned_value) == 3:
                        completion_event, submit_btn_loc, client_disco_checker = returned_value
                        # A non-None completion_event signifies a streaming request
                        if completion_event is not None:
                            current_request_was_streaming = True
                            logger.info(f"[{req_id}] (Worker) _process_request_refactored returned stream info (event, locator, checker).")
                        else:
                            # This case (tuple of Nones) means it was likely a non-streaming path within _process_request_refactored
                            # or an early exit where stream-specific objects weren't fully initialized.
                            current_request_was_streaming = False # Explicitly false
                            logger.info(f"[{req_id}] (Worker) _process_request_refactored returned a tuple, but completion_event is None (likely non-stream or early exit).")
                    elif returned_value is None:
                        # Explicit None return is for non-streaming success from _process_request_refactored
                        current_request_was_streaming = False
                        logger.info(f"[{req_id}] (Worker) _process_request_refactored returned non-stream completion (None).")
                    else:
                        current_request_was_streaming = False
                        logger.warning(f"[{req_id}] (Worker) _process_request_refactored returned unexpected type: {type(returned_value)}")

                    if completion_event: # This implies current_request_was_streaming is True
                         logger.info(f"[{req_id}] (Worker) ç­‰å¾…æµå¼ç”Ÿæˆå™¨å®Œæˆä¿¡å·...")
                         try:
                              await asyncio.wait_for(completion_event.wait(), timeout=RESPONSE_COMPLETION_TIMEOUT/1000 + 60)
                              logger.info(f"[{req_id}] (Worker) âœ… æµå¼ç”Ÿæˆå™¨å®Œæˆä¿¡å·æ”¶åˆ°ã€‚")

                              if submit_btn_loc and client_disco_checker:
                                  logger.info(f"[{req_id}] (Worker) æµå¼å“åº”å®Œæˆï¼Œç­‰å¾…å‘é€æŒ‰é’®ç¦ç”¨...")
                                  wait_timeout_ms = 30000  # 30 seconds
                                  try:
                                      # Check disconnect before starting the potentially long wait
                                      client_disco_checker("æµå¼å“åº”åç­‰å¾…å‘é€æŒ‰é’®ç¦ç”¨ - å‰ç½®æ£€æŸ¥: ")
                                      await asyncio.sleep(0.5) # Give UI a moment to update after stream completion
                                      await expect_async(submit_btn_loc).to_be_disabled(timeout=wait_timeout_ms)
                                      logger.info(f"[{req_id}] âœ… å‘é€æŒ‰é’®å·²ç¦ç”¨ã€‚")
                                  except PlaywrightAsyncError as e_pw_disabled:
                                      logger.warning(f"[{req_id}] âš ï¸ æµå¼å“åº”åç­‰å¾…å‘é€æŒ‰é’®ç¦ç”¨è¶…æ—¶æˆ–é”™è¯¯: {e_pw_disabled}")
                                      await save_error_snapshot(f"stream_post_submit_button_disabled_timeout_{req_id}")
                                  except ClientDisconnectedError:
                                      logger.info(f"[{req_id}] å®¢æˆ·ç«¯åœ¨æµå¼å“åº”åç­‰å¾…å‘é€æŒ‰é’®ç¦ç”¨æ—¶æ–­å¼€è¿æ¥ã€‚")
                                      # This error will be caught by the outer try/except in the worker loop if it needs to propagate
                                  except Exception as e_disable_wait:
                                      logger.exception(f"[{req_id}] âŒ æµå¼å“åº”åç­‰å¾…å‘é€æŒ‰é’®ç¦ç”¨æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ã€‚")
                                      await save_error_snapshot(f"stream_post_submit_button_disabled_unexpected_{req_id}")
                              elif current_request_was_streaming: # Log if stream but no locators/checker
                                  logger.warning(f"[{req_id}] (Worker) æµå¼è¯·æ±‚ä½† submit_btn_loc æˆ– client_disco_checker æœªæä¾›ã€‚è·³è¿‡æŒ‰é’®ç¦ç”¨ç­‰å¾…ã€‚")

                         except asyncio.TimeoutError:
                              logger.warning(f"[{req_id}] (Worker) âš ï¸ ç­‰å¾…æµå¼ç”Ÿæˆå™¨å®Œæˆä¿¡å·è¶…æ—¶ã€‚")
                              if not result_future.done(): result_future.set_exception(HTTPException(status_code=504, detail=f"[{req_id}] Stream generation timed out waiting for completion signal."))
                         except ClientDisconnectedError as cd_err: # Catch disconnect during event.wait()
                              logger.info(f"[{req_id}] (Worker) å®¢æˆ·ç«¯åœ¨ç­‰å¾…æµå¼å®Œæˆäº‹ä»¶æ—¶æ–­å¼€: {cd_err}")
                              if not result_future.done(): result_future.set_exception(HTTPException(status_code=499, detail=f"[{req_id}] Client disconnected during stream event wait."))
                         except Exception as ev_wait_err:
                              logger.error(f"[{req_id}] (Worker) âŒ ç­‰å¾…æµå¼å®Œæˆäº‹ä»¶æ—¶å‡ºé”™: {ev_wait_err}")
                              if not result_future.done(): result_future.set_exception(HTTPException(status_code=500, detail=f"[{req_id}] Error waiting for stream completion: {ev_wait_err}"))
            # æ¸…ç©ºæµå¼é˜Ÿåˆ—ç¼“å­˜
            logger.info(f"[{req_id}] (Worker) å°è¯•æ¸…ç©ºæµå¼é˜Ÿåˆ—ç¼“å­˜...")
            await clear_stream_queue()   
            logger.info(f"[{req_id}] (Worker) é‡Šæ”¾å¤„ç†é”ã€‚")
            was_last_request_streaming = is_streaming_request
            last_request_completion_time = time.time()
        except asyncio.CancelledError:
            logger.info("--- é˜Ÿåˆ— Worker è¢«å–æ¶ˆ ---")
            if result_future and not result_future.done(): result_future.cancel("Worker cancelled")
            break
        except Exception as e:
            logger.error(f"[{req_id}] (Worker) âŒ å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
            if result_future and not result_future.done():
                result_future.set_exception(HTTPException(status_code=500, detail=f"[{req_id}] æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}"))
            await save_error_snapshot(f"worker_loop_error_{req_id}")
        finally:
             if request_item: request_queue.task_done()
    logger.info("--- é˜Ÿåˆ— Worker å·²åœæ­¢ ---")

# --- Helper function to use external helper service ---
async def use_helper_get_response(helper_endpoint, helper_sapisid) -> AsyncGenerator[str, None]:
    headers = {
        'Cookie': f'SAPISID={helper_sapisid}',
        'Accept': 'text/event-stream'
    }
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(helper_endpoint, headers=headers) as response:
                if response.status != 200:
                    logger.error(f"Helper Error: HTTP {response.status}")
                    yield "[ERROR]" # Indicate error to caller
                    return # Stop generation

                async for line in response.content:
                    decoded_line = line.decode('utf-8').strip()
                    if decoded_line and not decoded_line.startswith(':'):
                        if decoded_line.startswith('data:'):
                            data = decoded_line[5:].strip()
                            if data:
                                yield data
                        else: # Should not happen with SSE, but yield anyway
                            yield decoded_line
    except aiohttp.ClientError as e:
        logger.error(f"Error connecting to helper server: {e}")
        raise # Re-raise to be caught by caller
    except Exception as e:
        logger.error(f"Unexpected error in use_helper_get_response: {e}")
        raise # Re-raise


async def use_stream_response(req_id: str) -> AsyncGenerator[Any, None]: # æ·»åŠ  req_id
    total_empty = 0
    log_state = STREAM_TIMEOUT_LOG_STATE # Access global state

    while True:
        data_chunk = None
        try:
            if STREAM_QUEUE is None: # æ£€æŸ¥ STREAM_QUEUE æ˜¯å¦ä¸º None
                logger.error(f"[{req_id}] STREAM_QUEUE is None in use_stream_response.")
                yield {"done": True, "reason": "stream_system_error", "body": "Auxiliary stream not available.", "function": []}
                return
            data_chunk = await asyncio.to_thread(STREAM_QUEUE.get_nowait)

            if data_chunk is not None:
                total_empty = 0 # Reset counter on successful read
                if log_state["consecutive_timeouts"] > 0: # ä½¿ç”¨å­—å…¸è®¿é—®
                    logger.info(f"[{req_id}] Auxiliary stream data received after {log_state['consecutive_timeouts']} consecutive empty reads/timeouts. Resetting.")
                    log_state["consecutive_timeouts"] = 0
                    log_state["suppress_until_time"] = 0.0
                
                data = json.loads(data_chunk)
                yield data
                if data.get("done") is True:
                    return
        except queue.Empty: # æ›´å…·ä½“çš„å¼‚å¸¸
            total_empty += 1
        except json.JSONDecodeError as json_e: # æ›´å…·ä½“çš„å¼‚å¸¸
            logger.error(f"[{req_id}] JSONDecodeError in use_stream_response: {json_e}. Data: '{data_chunk}'")
            total_empty += 1
        except Exception as e_q_get: # é€šç”¨å¼‚å¸¸ä½œä¸ºåå¤‡
            logger.error(f"[{req_id}] Unexpected error getting from STREAM_QUEUE: {e_q_get}", exc_info=True)
            total_empty += 1

        if total_empty > 300: # Timeout condition
            log_state["consecutive_timeouts"] += 1
            current_time = time.monotonic() # ä½¿ç”¨ monotonic time

            if log_state["consecutive_timeouts"] <= log_state["max_initial_errors"]:
                logger.error(f"[{req_id}] Auxiliary stream data timeout (Attempt {total_empty}, Consecutive global: {log_state['consecutive_timeouts']}). Helper service might be down.")
                log_state["last_error_log_time"] = current_time
                # Set suppress_until_time only after the initial burst of errors.
                if log_state["consecutive_timeouts"] == log_state["max_initial_errors"]:
                    log_state["suppress_until_time"] = current_time + log_state["suppress_duration_after_initial_burst"]
            elif current_time >= log_state["suppress_until_time"]:
                logger.warning(f"[{req_id}] Auxiliary stream continues to time out (Global consecutive: {log_state['consecutive_timeouts']}). Last error log ~{current_time - log_state['last_error_log_time']:.0f}s ago. Next warning in {log_state['warning_interval_after_suppress']:.0f}s.")
                log_state["last_error_log_time"] = current_time
                log_state["suppress_until_time"] = current_time + log_state["warning_interval_after_suppress"]
            # Else: Log is suppressed

            yield {"done": True, "reason": "internal_timeout", "body": "", "function": []} # ç‰¹å®šè¶…æ—¶ä¿¡å·
            return
        
        await asyncio.sleep(0.1) # å¼‚æ­¥ä¼‘çœ 
async def clear_stream_queue():
    if STREAM_QUEUE is None:
        logger.info("æµé˜Ÿåˆ—æœªåˆå§‹åŒ–æˆ–å·²è¢«ç¦ç”¨ï¼Œè·³è¿‡æ¸…ç©ºæ“ä½œã€‚")
        return
    while True:
        try:
            data_chunk = await asyncio.to_thread(STREAM_QUEUE.get_nowait)
            # logger.info(f"æ¸…ç©ºæµå¼é˜Ÿåˆ—ç¼“å­˜ï¼Œä¸¢å¼ƒæ•°æ®: {data_chunk}")
        except queue.Empty:
            logger.info("æµå¼é˜Ÿåˆ—å·²æ¸…ç©º (æ•è·åˆ° queue.Empty)ã€‚")
            break
        except Exception as e:
            logger.error(f"æ¸…ç©ºæµå¼é˜Ÿåˆ—æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
            break
    logger.info("æµå¼é˜Ÿåˆ—ç¼“å­˜æ¸…ç©ºå®Œæ¯•ã€‚")


# --- Core Request Processing Logic ---
async def _process_request_refactored(
    req_id: str,
    request: ChatCompletionRequest,
    http_request: Request,
    result_future: Future
) -> Optional[Tuple[Event, Locator, Callable[[str], bool]]]:
    model_actually_switched_in_current_api_call = False
    logger.info(f"[{req_id}] (Refactored Process) å¼€å§‹å¤„ç†è¯·æ±‚...")
    logger.info(f"[{req_id}]   è¯·æ±‚å‚æ•° - Model: {request.model}, Stream: {request.stream}")
    logger.info(f"[{req_id}]   è¯·æ±‚å‚æ•° - Temperature: {request.temperature}")
    logger.info(f"[{req_id}]   è¯·æ±‚å‚æ•° - Max Output Tokens: {request.max_output_tokens}")
    logger.info(f"[{req_id}]   è¯·æ±‚å‚æ•° - Stop Sequences: {request.stop}")
    logger.info(f"[{req_id}]   è¯·æ±‚å‚æ•° - Top P: {request.top_p}")
    is_streaming = request.stream
    page: Optional[AsyncPage] = page_instance
    completion_event: Optional[Event] = None
    requested_model = request.model
    model_id_to_use = None
    needs_model_switching = False
    if requested_model and requested_model != MODEL_NAME:
        requested_model_parts = requested_model.split('/')
        requested_model_id = requested_model_parts[-1] if len(requested_model_parts) > 1 else requested_model
        logger.info(f"[{req_id}] è¯·æ±‚ä½¿ç”¨æ¨¡å‹: {requested_model_id}")
        if parsed_model_list:
            valid_model_ids = [m.get("id") for m in parsed_model_list]
            if requested_model_id not in valid_model_ids:
                logger.error(f"[{req_id}] âŒ æ— æ•ˆçš„æ¨¡å‹ID: {requested_model_id}ã€‚å¯ç”¨æ¨¡å‹: {valid_model_ids}")
                raise HTTPException(status_code=400, detail=f"[{req_id}] Invalid model '{requested_model_id}'. Available models: {', '.join(valid_model_ids)}")
        model_id_to_use = requested_model_id
        global current_ai_studio_model_id
        if current_ai_studio_model_id != model_id_to_use:
            needs_model_switching = True
            logger.info(f"[{req_id}] éœ€è¦åˆ‡æ¢æ¨¡å‹: å½“å‰={current_ai_studio_model_id} -> ç›®æ ‡={model_id_to_use}")
        else:
            logger.info(f"[{req_id}] è¯·æ±‚æ¨¡å‹ä¸å½“å‰æ¨¡å‹ç›¸åŒ ({model_id_to_use})ï¼Œæ— éœ€åˆ‡æ¢")
    else:
        logger.info(f"[{req_id}] æœªæŒ‡å®šå…·ä½“æ¨¡å‹æˆ–ä½¿ç”¨ä»£ç†æ¨¡å‹åç§°ï¼Œå°†ä½¿ç”¨å½“å‰æ¨¡å‹: {current_ai_studio_model_id or 'æœªçŸ¥'}")
    client_disconnected_event = Event()
    disconnect_check_task = None
    input_field_locator = page.locator(INPUT_SELECTOR) if page else None # Handle page=None
    submit_button_locator = page.locator(SUBMIT_BUTTON_SELECTOR) if page else None # Handle page=None

    async def check_disconnect_periodically():
        while not client_disconnected_event.is_set():
            try:
                if await http_request.is_disconnected():
                    logger.info(f"[{req_id}] (Disco Check Task) å®¢æˆ·ç«¯æ–­å¼€ã€‚è®¾ç½®äº‹ä»¶å¹¶å°è¯•åœæ­¢ã€‚")
                    client_disconnected_event.set()
                    try:
                        if submit_button_locator and await submit_button_locator.is_enabled(timeout=1500):
                             if input_field_locator and await input_field_locator.input_value(timeout=1500) == '':
                                 logger.info(f"[{req_id}] (Disco Check Task)   ç‚¹å‡»åœæ­¢...")
                                 await submit_button_locator.click(timeout=3000, force=True)
                    except Exception as click_err: logger.warning(f"[{req_id}] (Disco Check Task) åœæ­¢æŒ‰é’®ç‚¹å‡»å¤±è´¥: {click_err}")
                    if not result_future.done(): result_future.set_exception(HTTPException(status_code=499, detail=f"[{req_id}] å®¢æˆ·ç«¯åœ¨å¤„ç†æœŸé—´å…³é—­äº†è¯·æ±‚"))
                    break
                await asyncio.sleep(1.0)
            except asyncio.CancelledError: break
            except Exception as e:
                logger.error(f"[{req_id}] (Disco Check Task) é”™è¯¯: {e}")
                client_disconnected_event.set()
                if not result_future.done(): result_future.set_exception(HTTPException(status_code=500, detail=f"[{req_id}] Internal disconnect checker error: {e}"))
                break
    disconnect_check_task = asyncio.create_task(check_disconnect_periodically())
    def check_client_disconnected(*args):
        msg_to_log = ""
        if len(args) == 1 and isinstance(args[0], str):
            msg_to_log = args[0]

        if client_disconnected_event.is_set():
            # req_id is from the outer scope of _process_request_refactored
            logger.info(f"[{req_id}] {msg_to_log}æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥äº‹ä»¶ã€‚")
            raise ClientDisconnectedError(f"[{req_id}] Client disconnected event set.")
        return False
    try:
        if not page or page.is_closed() or not is_page_ready:
            raise HTTPException(status_code=503, detail=f"[{req_id}] AI Studio é¡µé¢ä¸¢å¤±æˆ–æœªå°±ç»ªã€‚", headers={"Retry-After": "30"})
        check_client_disconnected("Initial Page Check: ")
        if needs_model_switching and model_id_to_use:
            async with model_switching_lock:
                model_before_switch_attempt = current_ai_studio_model_id
                if current_ai_studio_model_id != model_id_to_use:
                    logger.info(f"[{req_id}] è·å–é”åå‡†å¤‡åˆ‡æ¢: å½“å‰å†…å­˜ä¸­æ¨¡å‹={current_ai_studio_model_id}, ç›®æ ‡={model_id_to_use}")
                    switch_success = await switch_ai_studio_model(page, model_id_to_use, req_id)
                    if switch_success:
                        current_ai_studio_model_id = model_id_to_use
                        model_actually_switched_in_current_api_call = True
                        logger.info(f"[{req_id}] âœ… æ¨¡å‹åˆ‡æ¢æˆåŠŸã€‚å…¨å±€æ¨¡å‹çŠ¶æ€å·²æ›´æ–°ä¸º: {current_ai_studio_model_id}")
                    else:
                        logger.warning(f"[{req_id}] âŒ æ¨¡å‹åˆ‡æ¢è‡³ {model_id_to_use} å¤±è´¥ (AI Studio æœªæ¥å—æˆ–è¦†ç›–äº†æ›´æ”¹)ã€‚")
                        active_model_id_after_fail = model_before_switch_attempt
                        try:
                            final_prefs_str_after_fail = await page.evaluate("() => localStorage.getItem('aiStudioUserPreference')")
                            if final_prefs_str_after_fail:
                                final_prefs_obj_after_fail = json.loads(final_prefs_str_after_fail)
                                model_path_in_final_prefs = final_prefs_obj_after_fail.get("promptModel")
                                if model_path_in_final_prefs and isinstance(model_path_in_final_prefs, str):
                                    active_model_id_after_fail = model_path_in_final_prefs.split('/')[-1]
                        except Exception as read_final_prefs_err:
                            logger.error(f"[{req_id}] åˆ‡æ¢å¤±è´¥åè¯»å–æœ€ç»ˆ localStorage å‡ºé”™: {read_final_prefs_err}")
                        current_ai_studio_model_id = active_model_id_after_fail
                        logger.info(f"[{req_id}] å…¨å±€æ¨¡å‹çŠ¶æ€åœ¨åˆ‡æ¢å¤±è´¥åè®¾ç½®ä¸º (æˆ–ä¿æŒä¸º): {current_ai_studio_model_id}")
                        actual_displayed_model_name = "æœªçŸ¥ (æ— æ³•è¯»å–)"
                        try:
                            model_wrapper_locator = page.locator('#mat-select-value-0 mat-select-trigger').first
                            actual_displayed_model_name = await model_wrapper_locator.inner_text(timeout=3000)
                        except Exception:
                            pass
                        raise HTTPException(
                            status_code=422,
                            detail=f"[{req_id}] AI Studio æœªèƒ½åº”ç”¨æ‰€è¯·æ±‚çš„æ¨¡å‹ '{model_id_to_use}' æˆ–è¯¥æ¨¡å‹ä¸å—æ”¯æŒã€‚è¯·é€‰æ‹© AI Studio ç½‘é¡µç•Œé¢ä¸­å¯ç”¨çš„æ¨¡å‹ã€‚å½“å‰å®é™…ç”Ÿæ•ˆçš„æ¨¡å‹ ID ä¸º '{current_ai_studio_model_id}', é¡µé¢æ˜¾ç¤ºä¸º '{actual_displayed_model_name}'."
                        )
                else:
                    logger.info(f"[{req_id}] è·å–é”åå‘ç°æ¨¡å‹å·²æ˜¯ç›®æ ‡æ¨¡å‹ {current_ai_studio_model_id}ï¼Œæ— éœ€åˆ‡æ¢")
        async with params_cache_lock:
            cached_model_for_params = page_params_cache.get("last_known_model_id_for_params")
            if model_actually_switched_in_current_api_call or \
               (current_ai_studio_model_id is not None and current_ai_studio_model_id != cached_model_for_params):
                action_taken = "Invalidating" if page_params_cache else "Initializing"
                logger.info(f"[{req_id}] {action_taken} parameter cache. Reason: Model context changed (switched this call: {model_actually_switched_in_current_api_call}, current model: {current_ai_studio_model_id}, cache model: {cached_model_for_params}).")
                page_params_cache.clear()
                if current_ai_studio_model_id:
                    page_params_cache["last_known_model_id_for_params"] = current_ai_studio_model_id
            else:
                logger.debug(f"[{req_id}] Parameter cache for model '{cached_model_for_params}' remains valid (current model: '{current_ai_studio_model_id}', switched this call: {model_actually_switched_in_current_api_call}).")
        try: validate_chat_request(request.messages, req_id)
        except ValueError as e: raise HTTPException(status_code=400, detail=f"[{req_id}] æ— æ•ˆè¯·æ±‚: {e}")
        prepared_prompt = prepare_combined_prompt(request.messages, req_id)
        check_client_disconnected("After Prompt Prep: ")
        logger.info(f"[{req_id}] (Refactored Process) å¼€å§‹æ¸…ç©ºèŠå¤©è®°å½•...")
        try:
            clear_chat_button_locator = page.locator(CLEAR_CHAT_BUTTON_SELECTOR)
            confirm_button_locator = page.locator(CLEAR_CHAT_CONFIRM_BUTTON_SELECTOR)
            overlay_locator = page.locator(OVERLAY_SELECTOR)

            can_attempt_clear = False
            try:
                await expect_async(clear_chat_button_locator).to_be_enabled(timeout=3000)
                can_attempt_clear = True
                logger.info(f"[{req_id}] â€œæ¸…ç©ºèŠå¤©â€æŒ‰é’®å¯ç”¨ï¼Œç»§ç»­æ¸…ç©ºæµç¨‹ã€‚")
            except Exception as e_enable:
                is_new_chat_url = '/prompts/new_chat' in page.url.rstrip('/')
                if is_new_chat_url:
                    logger.info(f"[{req_id}] â€œæ¸…ç©ºèŠå¤©â€æŒ‰é’®ä¸å¯ç”¨ (é¢„æœŸï¼Œå› ä¸ºåœ¨ new_chat é¡µé¢)ã€‚è·³è¿‡æ¸…ç©ºæ“ä½œã€‚")
                else:
                    logger.warning(f"[{req_id}] ç­‰å¾…â€œæ¸…ç©ºèŠå¤©â€æŒ‰é’®å¯ç”¨å¤±è´¥: {e_enable}ã€‚æ¸…ç©ºæ“ä½œå¯èƒ½æ— æ³•æ‰§è¡Œã€‚")
            
            check_client_disconnected("æ¸…ç©ºèŠå¤© - â€œæ¸…ç©ºèŠå¤©â€æŒ‰é’®å¯ç”¨æ€§æ£€æŸ¥å: ")

            if can_attempt_clear:
                overlay_initially_visible = False
                try:
                    if await overlay_locator.is_visible(timeout=1000): # Short timeout for initial check
                        overlay_initially_visible = True
                        logger.info(f"[{req_id}] æ¸…ç©ºèŠå¤©ç¡®è®¤é®ç½©å±‚å·²å¯è§ã€‚ç›´æ¥ç‚¹å‡»â€œç»§ç»­â€ã€‚")
                except TimeoutError:
                    logger.info(f"[{req_id}] æ¸…ç©ºèŠå¤©ç¡®è®¤é®ç½©å±‚åˆå§‹ä¸å¯è§ (æ£€æŸ¥è¶…æ—¶æˆ–æœªæ‰¾åˆ°)ã€‚")
                    overlay_initially_visible = False
                except Exception as e_vis_check:
                    logger.warning(f"[{req_id}] æ£€æŸ¥é®ç½©å±‚å¯è§æ€§æ—¶å‘ç”Ÿé”™è¯¯: {e_vis_check}ã€‚å‡å®šä¸å¯è§ã€‚")
                    overlay_initially_visible = False
                
                check_client_disconnected("æ¸…ç©ºèŠå¤© - åˆå§‹é®ç½©å±‚æ£€æŸ¥å (can_attempt_clear=True): ")

                if overlay_initially_visible:
                    logger.info(f"[{req_id}] ç‚¹å‡»â€œç»§ç»­â€æŒ‰é’® (é®ç½©å±‚å·²å­˜åœ¨): {CLEAR_CHAT_CONFIRM_BUTTON_SELECTOR}")
                    await confirm_button_locator.click(timeout=CLICK_TIMEOUT_MS)
                else:
                    logger.info(f"[{req_id}] ç‚¹å‡»â€œæ¸…ç©ºèŠå¤©â€æŒ‰é’®: {CLEAR_CHAT_BUTTON_SELECTOR}")
                    await clear_chat_button_locator.click(timeout=CLICK_TIMEOUT_MS)
                    check_client_disconnected("æ¸…ç©ºèŠå¤© - ç‚¹å‡»â€œæ¸…ç©ºèŠå¤©â€å: ")
                    try:
                        logger.info(f"[{req_id}] ç­‰å¾…æ¸…ç©ºèŠå¤©ç¡®è®¤é®ç½©å±‚å‡ºç°: {OVERLAY_SELECTOR}")
                        await expect_async(overlay_locator).to_be_visible(timeout=WAIT_FOR_ELEMENT_TIMEOUT_MS)
                        logger.info(f"[{req_id}] æ¸…ç©ºèŠå¤©ç¡®è®¤é®ç½©å±‚å·²å‡ºç°ã€‚")
                    except TimeoutError:
                        error_msg = f"ç­‰å¾…æ¸…ç©ºèŠå¤©ç¡®è®¤é®ç½©å±‚è¶…æ—¶ (ç‚¹å‡»æ¸…ç©ºæŒ‰é’®å)ã€‚è¯·æ±‚ ID: {req_id}"
                        logger.error(error_msg)
                        await save_error_snapshot(f"clear_chat_overlay_timeout_{req_id}")
                        raise PlaywrightAsyncError(error_msg)
                    
                    check_client_disconnected("æ¸…ç©ºèŠå¤© - é®ç½©å±‚å‡ºç°å: ")
                    logger.info(f"[{req_id}] ç‚¹å‡»â€œç»§ç»­â€æŒ‰é’® (åœ¨å¯¹è¯æ¡†ä¸­): {CLEAR_CHAT_CONFIRM_BUTTON_SELECTOR}")
                    await confirm_button_locator.click(timeout=CLICK_TIMEOUT_MS)
                
                check_client_disconnected("æ¸…ç©ºèŠå¤© - ç‚¹å‡»â€œç»§ç»­â€å: ")

                max_retries_disappear = 3
                for attempt_disappear in range(max_retries_disappear):
                    try:
                        logger.info(f"[{req_id}] ç­‰å¾…æ¸…ç©ºèŠå¤©ç¡®è®¤æŒ‰é’®/å¯¹è¯æ¡†æ¶ˆå¤± (å°è¯• {attempt_disappear + 1}/{max_retries_disappear})...")
                        await expect_async(confirm_button_locator).to_be_hidden(timeout=CLEAR_CHAT_VERIFY_TIMEOUT_MS)
                        await expect_async(overlay_locator).to_be_hidden(timeout=1000)
                        logger.info(f"[{req_id}] âœ… æ¸…ç©ºèŠå¤©ç¡®è®¤å¯¹è¯æ¡†å·²æˆåŠŸæ¶ˆå¤±ã€‚")
                        break
                    except TimeoutError:
                        logger.warning(f"[{req_id}] âš ï¸ ç­‰å¾…æ¸…ç©ºèŠå¤©ç¡®è®¤å¯¹è¯æ¡†æ¶ˆå¤±è¶…æ—¶ (å°è¯• {attempt_disappear + 1}/{max_retries_disappear})ã€‚")
                        if attempt_disappear < max_retries_disappear - 1:
                            confirm_still_visible = False; overlay_still_visible = False
                            try: confirm_still_visible = await confirm_button_locator.is_visible(timeout=200)
                            except: pass
                            try: overlay_still_visible = await overlay_locator.is_visible(timeout=200)
                            except: pass
                            if confirm_still_visible: logger.warning(f"[{req_id}] ç¡®è®¤æŒ‰é’®åœ¨ç‚¹å‡»å’Œç­‰å¾…åä»å¯è§ã€‚")
                            if overlay_still_visible: logger.warning(f"[{req_id}] é®ç½©å±‚åœ¨ç‚¹å‡»å’Œç­‰å¾…åä»å¯è§ã€‚")
                            await asyncio.sleep(1.0)
                            check_client_disconnected(f"æ¸…ç©ºèŠå¤© - é‡è¯•æ¶ˆå¤±æ£€æŸ¥ {attempt_disappear + 1} å‰: ")
                            continue
                        else:
                            error_msg = f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚æ¸…ç©ºèŠå¤©ç¡®è®¤å¯¹è¯æ¡†æœªæ¶ˆå¤±ã€‚è¯·æ±‚ ID: {req_id}"
                            logger.error(error_msg)
                            await save_error_snapshot(f"clear_chat_dialog_disappear_timeout_{req_id}")
                            raise PlaywrightAsyncError(error_msg)
                    except ClientDisconnectedError:
                        logger.info(f"[{req_id}] å®¢æˆ·ç«¯åœ¨ç­‰å¾…æ¸…ç©ºç¡®è®¤å¯¹è¯æ¡†æ¶ˆå¤±æ—¶æ–­å¼€è¿æ¥ã€‚")
                        raise
                    check_client_disconnected(f"æ¸…ç©ºèŠå¤© - æ¶ˆå¤±æ£€æŸ¥å°è¯• {attempt_disappear + 1} å: ")
                
                last_response_container = page.locator(RESPONSE_CONTAINER_SELECTOR).last
                await asyncio.sleep(0.5)
                check_client_disconnected("After Clear Post-Delay (New Logic): ")
                try:
                    await expect_async(last_response_container).to_be_hidden(timeout=CLEAR_CHAT_VERIFY_TIMEOUT_MS - 500)
                    logger.info(f"[{req_id}] âœ… èŠå¤©å·²æˆåŠŸæ¸…ç©º (éªŒè¯é€šè¿‡ - æœ€åå“åº”å®¹å™¨éšè—)ã€‚")
                except Exception as verify_err:
                    logger.warning(f"[{req_id}] âš ï¸ è­¦å‘Š: æ¸…ç©ºèŠå¤©éªŒè¯å¤±è´¥ (æœ€åå“åº”å®¹å™¨æœªéšè—): {verify_err}")
            else:
                # If can_attempt_clear is False and it wasn't a new_chat_url, it means clear button wasn't enabled.
                # Log this situation if not already handled by the e_enable exception logging.
                if not ('/prompts/new_chat' in page.url.rstrip('/')): # Avoid logging if it was expected on new_chat
                    logger.warning(f"[{req_id}] ç”±äºâ€œæ¸…ç©ºèŠå¤©â€æŒ‰é’®åˆå§‹ä¸å¯ç”¨ï¼Œæœªæ‰§è¡Œæ¸…ç©ºæ“ä½œã€‚")

            check_client_disconnected("After Clear Chat Logic (New): ")
        except (PlaywrightAsyncError, asyncio.TimeoutError, ClientDisconnectedError) as clear_err:
            if isinstance(clear_err, ClientDisconnectedError): raise
            logger.error(f"[{req_id}] âŒ é”™è¯¯: æ¸…ç©ºèŠå¤©é˜¶æ®µå‡ºé”™: {clear_err}")
            await save_error_snapshot(f"clear_chat_error_{req_id}")
        except Exception as clear_exc:
            logger.exception(f"[{req_id}] âŒ é”™è¯¯: æ¸…ç©ºèŠå¤©é˜¶æ®µæ„å¤–é”™è¯¯")
            await save_error_snapshot(f"clear_chat_unexpected_{req_id}")
        check_client_disconnected("After Clear Chat Logic: ")
        if request.temperature is not None and page and not page.is_closed():
            async with params_cache_lock:
                logger.info(f"[{req_id}] (Refactored Process) æ£€æŸ¥å¹¶è°ƒæ•´æ¸©åº¦è®¾ç½®...")
                requested_temp = request.temperature
                clamped_temp = max(0.0, min(2.0, requested_temp))
                if clamped_temp != requested_temp:
                    logger.warning(f"[{req_id}] è¯·æ±‚çš„æ¸©åº¦ {requested_temp} è¶…å‡ºèŒƒå›´ [0, 2]ï¼Œå·²è°ƒæ•´ä¸º {clamped_temp}")
                cached_temp = page_params_cache.get("temperature")
                if cached_temp is not None and abs(cached_temp - clamped_temp) < 0.001:
                    logger.info(f"[{req_id}] æ¸©åº¦ ({clamped_temp}) ä¸ç¼“å­˜å€¼ ({cached_temp}) ä¸€è‡´ã€‚è·³è¿‡é¡µé¢äº¤äº’ã€‚")
                else:
                    logger.info(f"[{req_id}] è¯·æ±‚æ¸©åº¦ ({clamped_temp}) ä¸ç¼“å­˜å€¼ ({cached_temp}) ä¸ä¸€è‡´æˆ–ç¼“å­˜ä¸­æ— å€¼ã€‚éœ€è¦ä¸é¡µé¢äº¤äº’ã€‚")
                    temp_input_locator = page.locator(TEMPERATURE_INPUT_SELECTOR)
                    try:
                        await expect_async(temp_input_locator).to_be_visible(timeout=5000)
                        check_client_disconnected("æ¸©åº¦è°ƒæ•´ - è¾“å…¥æ¡†å¯è§å: ")
                        current_temp_str = await temp_input_locator.input_value(timeout=3000)
                        check_client_disconnected("æ¸©åº¦è°ƒæ•´ - è¯»å–è¾“å…¥æ¡†å€¼å: ")
                        current_temp_float = float(current_temp_str)
                        logger.info(f"[{req_id}] é¡µé¢å½“å‰æ¸©åº¦: {current_temp_float}, è¯·æ±‚è°ƒæ•´åæ¸©åº¦: {clamped_temp}")
                        if abs(current_temp_float - clamped_temp) < 0.001:
                            logger.info(f"[{req_id}] é¡µé¢å½“å‰æ¸©åº¦ ({current_temp_float}) ä¸è¯·æ±‚æ¸©åº¦ ({clamped_temp}) ä¸€è‡´ã€‚æ›´æ–°ç¼“å­˜å¹¶è·³è¿‡å†™å…¥ã€‚")
                            page_params_cache["temperature"] = current_temp_float
                        else:
                            logger.info(f"[{req_id}] é¡µé¢æ¸©åº¦ ({current_temp_float}) ä¸è¯·æ±‚æ¸©åº¦ ({clamped_temp}) ä¸åŒï¼Œæ­£åœ¨æ›´æ–°...")
                            await temp_input_locator.fill(str(clamped_temp), timeout=5000)
                            check_client_disconnected("æ¸©åº¦è°ƒæ•´ - å¡«å……è¾“å…¥æ¡†å: ")
                            await asyncio.sleep(0.1)
                            new_temp_str = await temp_input_locator.input_value(timeout=3000)
                            new_temp_float = float(new_temp_str)
                            if abs(new_temp_float - clamped_temp) < 0.001:
                                logger.info(f"[{req_id}] âœ… æ¸©åº¦å·²æˆåŠŸæ›´æ–°ä¸º: {new_temp_float}ã€‚æ›´æ–°ç¼“å­˜ã€‚")
                                page_params_cache["temperature"] = new_temp_float
                            else:
                                logger.warning(f"[{req_id}] âš ï¸ æ¸©åº¦æ›´æ–°åéªŒè¯å¤±è´¥ã€‚é¡µé¢æ˜¾ç¤º: {new_temp_float}, æœŸæœ›: {clamped_temp}ã€‚æ¸…é™¤ç¼“å­˜ä¸­çš„æ¸©åº¦ã€‚")
                                page_params_cache.pop("temperature", None)
                                await save_error_snapshot(f"temperature_verify_fail_{req_id}")
                    except ValueError as ve:
                        logger.error(f"[{req_id}] è½¬æ¢æ¸©åº¦å€¼ä¸ºæµ®ç‚¹æ•°æ—¶å‡ºé”™: '{current_temp_str if 'current_temp_str' in locals() else 'æœªçŸ¥å€¼'}'. é”™è¯¯: {ve}ã€‚æ¸…é™¤ç¼“å­˜ä¸­çš„æ¸©åº¦ã€‚")
                        page_params_cache.pop("temperature", None)
                        await save_error_snapshot(f"temperature_value_error_{req_id}")
                    except PlaywrightAsyncError as pw_err:
                        logger.error(f"[{req_id}] âŒ æ“ä½œæ¸©åº¦è¾“å…¥æ¡†æ—¶å‘ç”ŸPlaywrighté”™è¯¯: {pw_err}ã€‚æ¸…é™¤ç¼“å­˜ä¸­çš„æ¸©åº¦ã€‚")
                        page_params_cache.pop("temperature", None)
                        await save_error_snapshot(f"temperature_playwright_error_{req_id}")
                    except ClientDisconnectedError:
                        logger.info(f"[{req_id}] å®¢æˆ·ç«¯åœ¨è°ƒæ•´æ¸©åº¦æ—¶æ–­å¼€è¿æ¥ã€‚")
                        raise
                    except Exception as e_temp:
                        logger.exception(f"[{req_id}] âŒ è°ƒæ•´æ¸©åº¦æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ã€‚æ¸…é™¤ç¼“å­˜ä¸­çš„æ¸©åº¦ã€‚")
                        page_params_cache.pop("temperature", None)
                        await save_error_snapshot(f"temperature_unknown_error_{req_id}")
            check_client_disconnected("æ¸©åº¦è°ƒæ•´ - é€»è¾‘å®Œæˆå: ")
        if request.max_output_tokens is not None and page and not page.is_closed():
            async with params_cache_lock:
                logger.info(f"[{req_id}] (Refactored Process) æ£€æŸ¥å¹¶è°ƒæ•´æœ€å¤§è¾“å‡º Token è®¾ç½®...")
                requested_max_tokens = request.max_output_tokens
                min_val_for_tokens = 1
                max_val_for_tokens_from_model = 65536
                if model_id_to_use and parsed_model_list:
                    current_model_data = next((m for m in parsed_model_list if m.get("id") == model_id_to_use), None)
                    if current_model_data and current_model_data.get("supported_max_output_tokens") is not None:
                        try:
                            supported_tokens = int(current_model_data["supported_max_output_tokens"])
                            if supported_tokens > 0: max_val_for_tokens_from_model = supported_tokens
                            else: logger.warning(f"[{req_id}] æ¨¡å‹ {model_id_to_use} supported_max_output_tokens æ— æ•ˆ: {supported_tokens}")
                        except (ValueError, TypeError): logger.warning(f"[{req_id}] æ¨¡å‹ {model_id_to_use} supported_max_output_tokens è§£æå¤±è´¥: {current_model_data['supported_max_output_tokens']}")
                    else: logger.warning(f"[{req_id}] æœªæ‰¾åˆ°æ¨¡å‹ {model_id_to_use} çš„ supported_max_output_tokens æ•°æ®ã€‚")
                else: logger.warning(f"[{req_id}] model_id_to_use ('{model_id_to_use}') æˆ– parsed_model_list ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤ tokens ä¸Šé™ã€‚")
                clamped_max_tokens = max(min_val_for_tokens, min(max_val_for_tokens_from_model, requested_max_tokens))
                if clamped_max_tokens != requested_max_tokens:
                    logger.warning(f"[{req_id}] è¯·æ±‚çš„æœ€å¤§è¾“å‡º Tokens {requested_max_tokens} è¶…å‡ºæ¨¡å‹èŒƒå›´ [{min_val_for_tokens}, {max_val_for_tokens_from_model}]ï¼Œå·²è°ƒæ•´ä¸º {clamped_max_tokens}")
                cached_max_tokens = page_params_cache.get("max_output_tokens")
                if cached_max_tokens is not None and cached_max_tokens == clamped_max_tokens:
                    logger.info(f"[{req_id}] æœ€å¤§è¾“å‡º Tokens ({clamped_max_tokens}) ä¸ç¼“å­˜å€¼ ({cached_max_tokens}) ä¸€è‡´ã€‚è·³è¿‡é¡µé¢äº¤äº’ã€‚")
                else:
                    logger.info(f"[{req_id}] è¯·æ±‚æœ€å¤§è¾“å‡º Tokens ({clamped_max_tokens}) ä¸ç¼“å­˜å€¼ ({cached_max_tokens}) ä¸ä¸€è‡´æˆ–ç¼“å­˜ä¸­æ— å€¼ã€‚éœ€è¦ä¸é¡µé¢äº¤äº’ã€‚")
                    max_tokens_input_locator = page.locator(MAX_OUTPUT_TOKENS_SELECTOR)
                    try:
                        await expect_async(max_tokens_input_locator).to_be_visible(timeout=5000)
                        check_client_disconnected("æœ€å¤§è¾“å‡ºTokenè°ƒæ•´ - è¾“å…¥æ¡†å¯è§å: ")
                        current_max_tokens_str = await max_tokens_input_locator.input_value(timeout=3000)
                        check_client_disconnected("æœ€å¤§è¾“å‡ºTokenè°ƒæ•´ - è¯»å–è¾“å…¥æ¡†å€¼å: ")
                        current_max_tokens_int = int(current_max_tokens_str)
                        logger.info(f"[{req_id}] é¡µé¢å½“å‰æœ€å¤§è¾“å‡º Tokens: {current_max_tokens_int}, è¯·æ±‚è°ƒæ•´åæœ€å¤§è¾“å‡º Tokens: {clamped_max_tokens}")
                        if current_max_tokens_int == clamped_max_tokens:
                            logger.info(f"[{req_id}] é¡µé¢å½“å‰æœ€å¤§è¾“å‡º Tokens ({current_max_tokens_int}) ä¸è¯·æ±‚å€¼ ({clamped_max_tokens}) ä¸€è‡´ã€‚æ›´æ–°ç¼“å­˜å¹¶è·³è¿‡å†™å…¥ã€‚")
                            page_params_cache["max_output_tokens"] = current_max_tokens_int
                        else:
                            logger.info(f"[{req_id}] é¡µé¢æœ€å¤§è¾“å‡º Tokens ({current_max_tokens_int}) ä¸è¯·æ±‚å€¼ ({clamped_max_tokens}) ä¸åŒï¼Œæ­£åœ¨æ›´æ–°...")
                            await max_tokens_input_locator.fill(str(clamped_max_tokens), timeout=5000)
                            check_client_disconnected("æœ€å¤§è¾“å‡ºTokenè°ƒæ•´ - å¡«å……è¾“å…¥æ¡†å: ")
                            await asyncio.sleep(0.1)
                            new_max_tokens_str = await max_tokens_input_locator.input_value(timeout=3000)
                            new_max_tokens_int = int(new_max_tokens_str)
                            if new_max_tokens_int == clamped_max_tokens:
                                logger.info(f"[{req_id}] âœ… æœ€å¤§è¾“å‡º Tokens å·²æˆåŠŸæ›´æ–°ä¸º: {new_max_tokens_int}ã€‚æ›´æ–°ç¼“å­˜ã€‚")
                                page_params_cache["max_output_tokens"] = new_max_tokens_int
                            else:
                                logger.warning(f"[{req_id}] âš ï¸ æœ€å¤§è¾“å‡º Tokens æ›´æ–°åéªŒè¯å¤±è´¥ã€‚é¡µé¢æ˜¾ç¤º: {new_max_tokens_int}, æœŸæœ›: {clamped_max_tokens}ã€‚æ¸…é™¤ç¼“å­˜ä¸­çš„æ­¤å‚æ•°ã€‚")
                                page_params_cache.pop("max_output_tokens", None)
                                await save_error_snapshot(f"max_tokens_verify_fail_{req_id}")
                    except ValueError as ve:
                        logger.error(f"[{req_id}] è½¬æ¢æœ€å¤§è¾“å‡º Tokens å€¼ä¸ºæ•´æ•°æ—¶å‡ºé”™: '{current_max_tokens_str if 'current_max_tokens_str' in locals() else 'æœªçŸ¥å€¼'}'. é”™è¯¯: {ve}ã€‚æ¸…é™¤ç¼“å­˜ä¸­çš„æ­¤å‚æ•°ã€‚")
                        page_params_cache.pop("max_output_tokens", None)
                        await save_error_snapshot(f"max_tokens_value_error_{req_id}")
                    except PlaywrightAsyncError as pw_err:
                        logger.error(f"[{req_id}] âŒ æ“ä½œæœ€å¤§è¾“å‡º Tokens è¾“å…¥æ¡†æ—¶å‘ç”ŸPlaywrighté”™è¯¯: {pw_err}ã€‚æ¸…é™¤ç¼“å­˜ä¸­çš„æ­¤å‚æ•°ã€‚")
                        page_params_cache.pop("max_output_tokens", None)
                        await save_error_snapshot(f"max_tokens_playwright_error_{req_id}")
                    except ClientDisconnectedError:
                        logger.info(f"[{req_id}] å®¢æˆ·ç«¯åœ¨è°ƒæ•´æœ€å¤§è¾“å‡º Tokens æ—¶æ–­å¼€è¿æ¥ã€‚")
                        raise
                    except Exception as e_max_tokens:
                        logger.exception(f"[{req_id}] âŒ è°ƒæ•´æœ€å¤§è¾“å‡º Tokens æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ã€‚æ¸…é™¤ç¼“å­˜ä¸­çš„æ­¤å‚æ•°ã€‚")
                        page_params_cache.pop("max_output_tokens", None)
                        await save_error_snapshot(f"max_tokens_unknown_error_{req_id}")
            check_client_disconnected("æœ€å¤§è¾“å‡ºTokenè°ƒæ•´ - é€»è¾‘å®Œæˆå: ")
        if request.stop is not None and page and not page.is_closed():
            async with params_cache_lock:
                logger.info(f"[{req_id}] (Refactored Process) æ£€æŸ¥å¹¶è®¾ç½®åœæ­¢åºåˆ—...")
                requested_stop_sequences_raw = []
                if isinstance(request.stop, str):
                    requested_stop_sequences_raw = [request.stop]
                elif isinstance(request.stop, list):
                    requested_stop_sequences_raw = [s for s in request.stop if isinstance(s, str) and s.strip()]
                normalized_requested_stops = set(s.strip() for s in requested_stop_sequences_raw if s.strip())
                cached_stops_set = page_params_cache.get("stop_sequences")
                if cached_stops_set is not None and cached_stops_set == normalized_requested_stops:
                    logger.info(f"[{req_id}] è¯·æ±‚çš„åœæ­¢åºåˆ— ({normalized_requested_stops}) ä¸ç¼“å­˜å€¼ ({cached_stops_set}) ä¸€è‡´ã€‚è·³è¿‡é¡µé¢äº¤äº’ã€‚")
                else:
                    logger.info(f"[{req_id}] è¯·æ±‚åœæ­¢åºåˆ— ({normalized_requested_stops}) ä¸ç¼“å­˜å€¼ ({cached_stops_set}) ä¸ä¸€è‡´æˆ–ç¼“å­˜ä¸­æ— å€¼ã€‚éœ€è¦ä¸é¡µé¢äº¤äº’ã€‚")
                    stop_input_locator = page.locator(STOP_SEQUENCE_INPUT_SELECTOR)
                    remove_chip_buttons_locator = page.locator(MAT_CHIP_REMOVE_BUTTON_SELECTOR)
                    interaction_successful = False
                    try:
                        logger.info(f"[{req_id}] å°è¯•æ¸…ç©ºå·²æœ‰çš„åœæ­¢åºåˆ—...")
                        initial_chip_count = await remove_chip_buttons_locator.count()
                        removed_count = 0
                        max_removals = initial_chip_count + 5
                        while await remove_chip_buttons_locator.count() > 0 and removed_count < max_removals:
                            check_client_disconnected("åœæ­¢åºåˆ—æ¸…é™¤ - å¾ªç¯å¼€å§‹: ")
                            try:
                                await remove_chip_buttons_locator.first.click(timeout=2000)
                                removed_count += 1; await asyncio.sleep(0.15)
                            except Exception: break
                        logger.info(f"[{req_id}] å·²æœ‰åœæ­¢åºåˆ—æ¸…ç©ºå°è¯•å®Œæˆã€‚ç§»é™¤ {removed_count} ä¸ªã€‚")
                        check_client_disconnected("åœæ­¢åºåˆ—æ¸…é™¤ - å®Œæˆå: ")
                        if normalized_requested_stops:
                            logger.info(f"[{req_id}] æ·»åŠ æ–°çš„åœæ­¢åºåˆ—: {normalized_requested_stops}")
                            await expect_async(stop_input_locator).to_be_visible(timeout=5000)
                            for seq in normalized_requested_stops:
                                await stop_input_locator.fill(seq, timeout=3000)
                                await stop_input_locator.press("Enter", timeout=3000)
                                await asyncio.sleep(0.2)
                                current_input_val = await stop_input_locator.input_value(timeout=1000)
                                if current_input_val:
                                     logger.warning(f"[{req_id}] æ·»åŠ åœæ­¢åºåˆ— '{seq}' åè¾“å…¥æ¡†æœªæ¸…ç©º (å€¼ä¸º: '{current_input_val}')ã€‚")
                            logger.info(f"[{req_id}] âœ… æ–°åœæ­¢åºåˆ—æ·»åŠ æ“ä½œå®Œæˆã€‚")
                        else:
                            logger.info(f"[{req_id}] æ²¡æœ‰æä¾›æ–°çš„æœ‰æ•ˆåœæ­¢åºåˆ—æ¥æ·»åŠ  (è¯·æ±‚æ¸…ç©º)ã€‚")
                        interaction_successful = True
                        page_params_cache["stop_sequences"] = normalized_requested_stops
                        logger.info(f"[{req_id}] åœæ­¢åºåˆ—ç¼“å­˜å·²æ›´æ–°ä¸º: {normalized_requested_stops}")
                    except PlaywrightAsyncError as pw_err:
                        logger.error(f"[{req_id}] âŒ æ“ä½œåœæ­¢åºåˆ—æ—¶å‘ç”ŸPlaywrighté”™è¯¯: {pw_err}ã€‚æ¸…é™¤ç¼“å­˜ä¸­çš„æ­¤å‚æ•°ã€‚")
                        page_params_cache.pop("stop_sequences", None)
                        await save_error_snapshot(f"stop_sequence_playwright_error_{req_id}")
                    except ClientDisconnectedError:
                        logger.info(f"[{req_id}] å®¢æˆ·ç«¯åœ¨è°ƒæ•´åœæ­¢åºåˆ—æ—¶æ–­å¼€è¿æ¥ã€‚")
                        raise
                    except Exception as e_stop_seq:
                        logger.exception(f"[{req_id}] âŒ è®¾ç½®åœæ­¢åºåˆ—æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ã€‚æ¸…é™¤ç¼“å­˜ä¸­çš„æ­¤å‚æ•°ã€‚")
                        page_params_cache.pop("stop_sequences", None)
                        await save_error_snapshot(f"stop_sequence_unknown_error_{req_id}")
            check_client_disconnected("åœæ­¢åºåˆ—è°ƒæ•´ - é€»è¾‘å®Œæˆå: ")
        if request.top_p is not None and page and not page.is_closed():
            logger.info(f"[{req_id}] (Refactored Process) æ£€æŸ¥å¹¶è°ƒæ•´ Top P è®¾ç½®...")
            requested_top_p = request.top_p
            clamped_top_p = max(0.0, min(1.0, requested_top_p))
            if abs(clamped_top_p - requested_top_p) > 1e-9:
                logger.warning(f"[{req_id}] è¯·æ±‚çš„ Top P {requested_top_p} è¶…å‡ºèŒƒå›´ [0, 1]ï¼Œå·²è°ƒæ•´ä¸º {clamped_top_p}")
            top_p_input_locator = page.locator(TOP_P_INPUT_SELECTOR)
            try:
                await expect_async(top_p_input_locator).to_be_visible(timeout=5000)
                check_client_disconnected("Top P è°ƒæ•´ - è¾“å…¥æ¡†å¯è§å: ")
                current_top_p_str = await top_p_input_locator.input_value(timeout=3000)
                check_client_disconnected("Top P è°ƒæ•´ - è¯»å–è¾“å…¥æ¡†å€¼å: ")
                current_top_p_float = float(current_top_p_str)
                logger.info(f"[{req_id}] é¡µé¢å½“å‰ Top P: {current_top_p_float}, è¯·æ±‚è°ƒæ•´å Top P: {clamped_top_p}")
                if abs(current_top_p_float - clamped_top_p) > 1e-9:
                    logger.info(f"[{req_id}] é¡µé¢ Top P ({current_top_p_float}) ä¸è¯·æ±‚ Top P ({clamped_top_p}) ä¸åŒï¼Œæ­£åœ¨æ›´æ–°...")
                    await top_p_input_locator.fill(str(clamped_top_p), timeout=5000)
                    check_client_disconnected("Top P è°ƒæ•´ - å¡«å……è¾“å…¥æ¡†å: ")
                    await asyncio.sleep(0.1)
                    new_top_p_str = await top_p_input_locator.input_value(timeout=3000)
                    new_top_p_float = float(new_top_p_str)
                    if abs(new_top_p_float - clamped_top_p) < 1e-9:
                        logger.info(f"[{req_id}] âœ… Top P å·²æˆåŠŸæ›´æ–°ä¸º: {new_top_p_float}")
                    else:
                        logger.warning(f"[{req_id}] âš ï¸ Top P æ›´æ–°åéªŒè¯å¤±è´¥ã€‚é¡µé¢æ˜¾ç¤º: {new_top_p_float}, æœŸæœ›: {clamped_top_p}")
                else:
                    logger.info(f"[{req_id}] é¡µé¢ Top P ({current_top_p_float}) ä¸è¯·æ±‚ Top P ({clamped_top_p}) ä¸€è‡´æˆ–åœ¨å®¹å·®èŒƒå›´å†…ï¼Œæ— éœ€æ›´æ”¹ã€‚")
            except ValueError as ve:
                logger.error(f"[{req_id}] è½¬æ¢ Top P å€¼ä¸ºæµ®ç‚¹æ•°æ—¶å‡ºé”™: '{current_top_p_str if 'current_top_p_str' in locals() else 'æœªçŸ¥å€¼'}'. é”™è¯¯: {ve}")
                await save_error_snapshot(f"top_p_value_error_{req_id}")
            except PlaywrightAsyncError as pw_err:
                logger.error(f"[{req_id}] âŒ æ“ä½œ Top P è¾“å…¥æ¡†æ—¶å‘ç”ŸPlaywrighté”™è¯¯: {pw_err}")
                await save_error_snapshot(f"top_p_playwright_error_{req_id}")
            except ClientDisconnectedError:
                logger.info(f"[{req_id}] å®¢æˆ·ç«¯åœ¨è°ƒæ•´ Top P æ—¶æ–­å¼€è¿æ¥ã€‚")
                raise
            except Exception as e_top_p:
                logger.exception(f"[{req_id}] âŒ è°ƒæ•´ Top P æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯")
                await save_error_snapshot(f"top_p_unknown_error_{req_id}")
            check_client_disconnected("Top P è°ƒæ•´ - é€»è¾‘å®Œæˆå: ")
        logger.info(f"[{req_id}] (Refactored Process) å¡«å……å¹¶æäº¤æç¤º ({len(prepared_prompt)} chars)...")
        prompt_textarea_locator = page.locator(PROMPT_TEXTAREA_SELECTOR)
        autosize_wrapper_locator = page.locator('ms-prompt-input-wrapper ms-autosize-textarea')
        try:
            await expect_async(prompt_textarea_locator).to_be_visible(timeout=5000)
            check_client_disconnected("After Input Visible: ")
            logger.info(f"[{req_id}]   - ä½¿ç”¨ JavaScript evaluate å¡«å……æç¤ºæ–‡æœ¬...")
            await prompt_textarea_locator.evaluate(
                '''
                (element, text) => {
                    element.value = text;
                    element.dispatchEvent(new Event('input', { bubbles: true, cancelable: true }));
                    element.dispatchEvent(new Event('change', { bubbles: true, cancelable: true }));
                }
                ''',
                prepared_prompt
            )
            await autosize_wrapper_locator.evaluate('(element, text) => { element.setAttribute("data-value", text); }', prepared_prompt)
            logger.info(f"[{req_id}]   - JavaScript evaluate å¡«å……å®Œæˆï¼Œdata-value å·²å°è¯•æ›´æ–°ã€‚")
            check_client_disconnected("After Input Fill (evaluate): ")

            logger.info(f"[{req_id}]   - ç­‰å¾…å‘é€æŒ‰é’®å¯ç”¨ (å¡«å……æç¤ºå)...")
            wait_timeout_ms_submit_enabled = 40000 # 40 seconds
            try:
                # Check disconnect before starting the potentially long wait
                check_client_disconnected("å¡«å……æç¤ºåç­‰å¾…å‘é€æŒ‰é’®å¯ç”¨ - å‰ç½®æ£€æŸ¥: ")
                await expect_async(submit_button_locator).to_be_enabled(timeout=wait_timeout_ms_submit_enabled)
                logger.info(f"[{req_id}]   - âœ… å‘é€æŒ‰é’®å·²å¯ç”¨ã€‚")
            except PlaywrightAsyncError as e_pw_enabled:
                logger.error(f"[{req_id}]   - âŒ ç­‰å¾…å‘é€æŒ‰é’®å¯ç”¨è¶…æ—¶æˆ–é”™è¯¯: {e_pw_enabled}")
                await save_error_snapshot(f"submit_button_enable_timeout_{req_id}")
                raise # Re-raise to be caught by the main try-except block for prompt submission
            except ClientDisconnectedError:
                logger.info(f"[{req_id}] å®¢æˆ·ç«¯åœ¨ç­‰å¾…å‘é€æŒ‰é’®å¯ç”¨æ—¶æ–­å¼€è¿æ¥ã€‚")
                raise
            except Exception as e_enable_wait:
                logger.exception(f"[{req_id}]   - âŒ ç­‰å¾…å‘é€æŒ‰é’®å¯ç”¨æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ã€‚")
                await save_error_snapshot(f"submit_button_enable_unexpected_{req_id}")
                raise

            check_client_disconnected("After Submit Button Enabled (Post-Wait): ")
            await asyncio.sleep(0.3) # Small delay after button is enabled, before pressing shortcut
            check_client_disconnected("After Submit Pre-Shortcut-Delay: ")
            submitted_successfully_via_shortcut = False
            user_prompt_autosize_locator = page.locator('ms-prompt-input-wrapper ms-autosize-textarea').nth(1)
            logger.info(f"[{req_id}]   - ç”¨äºå¿«æ·é”®åéªŒè¯çš„ç”¨æˆ·è¾“å…¥åŒºåŸŸé€‰æ‹©å™¨: nth(1) of 'ms-prompt-input-wrapper ms-autosize-textarea'")
            try:
                host_os_from_launcher = os.environ.get('HOST_OS_FOR_SHORTCUT')
                is_mac_determined = False
                if host_os_from_launcher:
                    logger.info(f"[{req_id}]   - ä»å¯åŠ¨å™¨ç¯å¢ƒå˜é‡ HOST_OS_FOR_SHORTCUT è·å–åˆ°æ“ä½œç³»ç»Ÿæç¤º: '{host_os_from_launcher}'")
                    if host_os_from_launcher == "Darwin":
                        is_mac_determined = True
                    elif host_os_from_launcher in ["Windows", "Linux"]:
                        is_mac_determined = False
                    else:
                        logger.warning(f"[{req_id}]   - æœªçŸ¥çš„ HOST_OS_FOR_SHORTCUT å€¼: '{host_os_from_launcher}'ã€‚å°†å›é€€åˆ°æµè§ˆå™¨æ£€æµ‹ã€‚")
                        host_os_from_launcher = None
                if not host_os_from_launcher:
                    if host_os_from_launcher is None:
                        logger.info(f"[{req_id}]   - HOST_OS_FOR_SHORTCUT æœªè®¾ç½®æˆ–å€¼æœªçŸ¥ï¼Œå°†è¿›è¡Œæµè§ˆå™¨å†…éƒ¨æ“ä½œç³»ç»Ÿæ£€æµ‹ã€‚")
                    user_agent_data_platform = None
                    try:
                        user_agent_data_platform = await page.evaluate("() => navigator.userAgentData?.platform || ''")
                    except Exception as e_ua_data:
                        logger.warning(f"[{req_id}]   - navigator.userAgentData.platform è¯»å–å¤±è´¥ ({e_ua_data})ï¼Œå°è¯• navigator.userAgentã€‚")
                        user_agent_string = await page.evaluate("() => navigator.userAgent || ''")
                        user_agent_string_lower = user_agent_string.lower()
                        if "macintosh" in user_agent_string_lower or "mac os x" in user_agent_string_lower or "macintel" in user_agent_string_lower:
                            user_agent_data_platform = "macOS"
                        elif "windows" in user_agent_string_lower:
                            user_agent_data_platform = "Windows"
                        elif "linux" in user_agent_string_lower:
                            user_agent_data_platform = "Linux"
                        else:
                            user_agent_data_platform = "Other"
                    if user_agent_data_platform and user_agent_data_platform != "Other":
                        user_agent_data_platform_lower = user_agent_data_platform.lower()
                        is_mac_determined = "mac" in user_agent_data_platform_lower or "macos" in user_agent_data_platform_lower or "macintel" in user_agent_data_platform_lower
                        logger.info(f"[{req_id}]   - æµè§ˆå™¨å†…éƒ¨æ£€æµ‹åˆ°å¹³å°: '{user_agent_data_platform}', æ¨æ–­ is_mac: {is_mac_determined}")
                    else:
                        logger.warning(f"[{req_id}]   - æµè§ˆå™¨å¹³å°ä¿¡æ¯è·å–å¤±è´¥ã€ä¸ºç©ºæˆ–ä¸º'Other' ('{user_agent_data_platform}')ã€‚é»˜è®¤ä½¿ç”¨éMacå¿«æ·é”®ã€‚")
                        is_mac_determined = False
                shortcut_modifier = "Meta" if is_mac_determined else "Control"
                shortcut_key = "Enter"
                logger.info(f"[{req_id}]   - æœ€ç»ˆé€‰æ‹©å¿«æ·é”®: {shortcut_modifier}+{shortcut_key} (åŸºäº is_mac_determined: {is_mac_determined})")
                logger.info(f"[{req_id}]   - å°è¯•å°†ç„¦ç‚¹è®¾ç½®åˆ°è¾“å…¥æ¡†...")
                await prompt_textarea_locator.focus(timeout=5000)
                check_client_disconnected("After Input Focus (Shortcut): ")
                await asyncio.sleep(0.1)
                logger.info(f"[{req_id}]   - ç„¦ç‚¹è®¾ç½®å®Œæˆï¼Œå‡†å¤‡æŒ‰ä¸‹å¿«æ·é”®...")
                try:
                    await page.keyboard.press(f'{shortcut_modifier}+{shortcut_key}')
                    logger.info(f"[{req_id}]   - å·²ä½¿ç”¨ç»„åˆé”®æ–¹å¼æ¨¡æ‹ŸæŒ‰ä¸‹: {shortcut_modifier}+{shortcut_key}")
                except Exception as combo_err:
                    logger.warning(f"[{req_id}]   - ç»„åˆé”®æ–¹å¼å¤±è´¥: {combo_err}ï¼Œå°è¯•åˆ†æ­¥æŒ‰é”®...")
                    try:
                        await page.keyboard.down(shortcut_modifier)
                        await asyncio.sleep(0.05)
                        await page.keyboard.down(shortcut_key)
                        await asyncio.sleep(0.05)
                        await page.keyboard.up(shortcut_key)
                        await asyncio.sleep(0.05)
                        await page.keyboard.up(shortcut_modifier)
                        logger.info(f"[{req_id}]   - å·²ä½¿ç”¨åˆ†æ­¥æŒ‰é”®æ–¹å¼æ¨¡æ‹Ÿ: {shortcut_modifier}+{shortcut_key}")
                    except Exception as step_err:
                        logger.error(f"[{req_id}]   - åˆ†æ­¥æŒ‰é”®ä¹Ÿå¤±è´¥: {step_err}")
                check_client_disconnected("After Keyboard Press: ")
                await asyncio.sleep(0.75) # <--- æ–°å¢æ­¤è¡Œä»¥æä¾›UIååº”æ—¶é—´
                check_client_disconnected("After Keyboard Press Post-Delay: ") # <--- æ–°å¢æ­¤è¡Œæ—¥å¿—
                user_prompt_actual_textarea_locator = page.locator(
                    'ms-prompt-input-wrapper textarea[aria-label="Start typing a prompt"]'
                )
                selector_string = 'ms-prompt-input-wrapper textarea[aria-label="Start typing a prompt"]'
                logger.info(f"[{req_id}]   - ç”¨äºå¿«æ·é”®åéªŒè¯çš„ç”¨æˆ·è¾“å…¥ textarea é€‰æ‹©å™¨: '{selector_string}'")
                validation_attempts = 7
                validation_interval = 0.2
                for i in range(validation_attempts):
                    try:
                        current_value = await user_prompt_actual_textarea_locator.input_value(timeout=500)
                        if current_value == "":
                            submitted_successfully_via_shortcut = True
                            logger.info(f"[{req_id}]   - âœ… å¿«æ·é”®æäº¤æˆåŠŸç¡®è®¤ (ç”¨æˆ·è¾“å…¥ textarea value å·²æ¸…ç©º after {i+1} attempts)ã€‚")
                            break
                        else:
                            if DEBUG_LOGS_ENABLED:
                                logger.debug(f"[{req_id}]   - ç”¨æˆ·è¾“å…¥ textarea value éªŒè¯å°è¯• {i+1}/{validation_attempts}: å½“å‰='{current_value}', æœŸæœ›=''")
                    except PlaywrightAsyncError as e_val:
                        if DEBUG_LOGS_ENABLED:
                            logger.debug(f"[{req_id}]   - è·å–ç”¨æˆ·è¾“å…¥ textarea value æ—¶å‡ºé”™ (å°è¯• {i+1}): {e_val.message.splitlines()[0]}")
                        if "timeout" in e_val.message.lower():
                            pass
                        else:
                            logger.warning(f"[{req_id}]   - è·å–ç”¨æˆ·è¾“å…¥ textarea value æ—¶ Playwright é”™è¯¯ (å°è¯• {i+1}): {e_val.message.splitlines()[0]}")
                            if "strict mode violation" in e_val.message.lower():
                                await save_error_snapshot(f"shortcut_submit_textarea_value_strict_error_{req_id}")
                                break
                            break
                    except Exception as e_gen:
                        logger.warning(f"[{req_id}]   - è·å–ç”¨æˆ·è¾“å…¥ textarea value æ—¶å‘ç”Ÿå…¶ä»–é”™è¯¯ (å°è¯• {i+1}): {e_gen}")
                        break
                    if i < validation_attempts - 1:
                        await asyncio.sleep(validation_interval)
                if not submitted_successfully_via_shortcut:
                    final_value_for_log = "(æ— æ³•è·å–æˆ–æœªæ¸…ç©º)"
                    try:
                        final_value_for_log = await user_prompt_actual_textarea_locator.input_value(timeout=300)
                    except:
                        pass
                    logger.warning(f"[{req_id}]   - âš ï¸ å¿«æ·é”®æäº¤åç”¨æˆ·è¾“å…¥ textarea value ('{final_value_for_log}') æœªåœ¨é¢„æœŸæ—¶é—´å†… ({validation_attempts * validation_interval:.1f}s) æ¸…ç©ºã€‚")
            except Exception as shortcut_err:
                logger.error(f"[{req_id}]   - âŒ å¿«æ·é”®æäº¤è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {shortcut_err}", exc_info=True)
                await save_error_snapshot(f"shortcut_submit_error_{req_id}")
                raise PlaywrightAsyncError(f"Failed to submit prompt via keyboard shortcut: {shortcut_err}") from shortcut_err
            if not submitted_successfully_via_shortcut:
                 logger.error(f"[{req_id}] ä¸¥é‡é”™è¯¯: æœªèƒ½é€šè¿‡å¿«æ·é”®ç¡®è®¤æäº¤ã€‚")
                 raise PlaywrightAsyncError("Failed to confirm prompt submission via shortcut.")
        except (PlaywrightAsyncError, asyncio.TimeoutError, ClientDisconnectedError) as submit_err:
            if isinstance(submit_err, ClientDisconnectedError): raise
            logger.error(f"[{req_id}] âŒ é”™è¯¯: å¡«å……æˆ–æäº¤æç¤ºæ—¶å‡ºé”™: {submit_err}", exc_info=True)
            await save_error_snapshot(f"submit_prompt_error_{req_id}")
            raise HTTPException(status_code=502, detail=f"[{req_id}] Failed to submit prompt to AI Studio: {submit_err}")
        except Exception as submit_exc:
            logger.exception(f"[{req_id}] âŒ é”™è¯¯: å¡«å……æˆ–æäº¤æç¤ºæ—¶æ„å¤–é”™è¯¯")
            await save_error_snapshot(f"submit_prompt_unexpected_{req_id}")
            raise HTTPException(status_code=500, detail=f"[{req_id}] Unexpected error during prompt submission: {submit_exc}")
        check_client_disconnected("After Submit Logic: ")

        stream_port = os.environ.get('STREAM_PORT')
        use_stream = stream_port != '0' # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ä½ çš„è¾…åŠ©æµ

        if use_stream:
            # ç¡®ä¿ generate_random_string å‡½æ•°å·²å®šä¹‰æˆ–å¯è®¿é—®
            def generate_random_string(length):
                charset = "abcdefghijklmnopqrstuvwxyz0123456789"
                return ''.join(random.choice(charset) for _ in range(length))

            if is_streaming:
                try:
                    completion_event = Event()
                    # ç¡®ä¿ create_stream_generator_from_helper å‡½æ•°å·²å®šä¹‰æˆ–å¯è®¿é—®
                    async def create_stream_generator_from_helper(event_to_set: Event) -> AsyncGenerator[str, None]:
                        last_reason_pos = 0
                        last_body_pos = 0
                        # ä½¿ç”¨å½“å‰AI Studioæ¨¡å‹IDæˆ–é»˜è®¤æ¨¡å‹åç§°
                        model_name_for_stream = current_ai_studio_model_id or MODEL_NAME
                        chat_completion_id = f"{CHAT_COMPLETION_ID_PREFIX}{req_id}-{int(time.time())}-{random.randint(100, 999)}"
                        created_timestamp = int(time.time())

                        async for data in use_stream_response(req_id): # ç¡®ä¿ use_stream_response æ˜¯å¼‚æ­¥ç”Ÿæˆå™¨
                            # --- å¼€å§‹å¤„ç†ä» use_stream_response è·å–çš„ data ---
                            # (è¿™é‡Œæ˜¯ä½ ç°æœ‰çš„è§£æ data å¹¶ç”Ÿæˆ SSE å—çš„é€»è¾‘)
                            # ä¾‹å¦‚:
                            if len(data["reason"]) > last_reason_pos:
                                output = {
                                    "id": chat_completion_id,
                                    "object": "chat.completion.chunk",
                                    "model": model_name_for_stream,
                                    "created": created_timestamp,
                                    "choices":[{
                                        "delta":{
                                            "role": "assistant",
                                            "content": None,
                                            "reasoning_content": data["reason"][last_reason_pos:],
                                        },
                                        "finish_reason": None,
                                        "native_finish_reason": None, # ä¿æŒä¸OpenAIå…¼å®¹
                                    }]
                                }
                                last_reason_pos = len(data["reason"])
                                yield f"data: {json.dumps(output, ensure_ascii=False, separators=(',', ':'))}\n\n"
                            elif len(data["body"]) > last_body_pos:
                                finish_reason_val = None
                                if data["done"]:
                                    finish_reason_val = "stop"
                                
                                delta_content = {"role": "assistant", "content": data["body"][last_body_pos:]}
                                choice_item = {
                                    "delta": delta_content,
                                    "finish_reason": finish_reason_val,
                                    "native_finish_reason": finish_reason_val,
                                }

                                if data["done"] and data.get("function") and len(data["function"]) > 0:
                                    tool_calls_list = []
                                    for func_idx, function_call_data in enumerate(data["function"]):
                                        tool_calls_list.append({
                                            "id": f"call_{generate_random_string(24)}", # ç¡®ä¿IDå”¯ä¸€
                                            "index": func_idx, # ä½¿ç”¨å®é™…ç´¢å¼•
                                            "type": "function",
                                            "function": {
                                                "name": function_call_data["name"],
                                                "arguments": json.dumps(function_call_data["params"]),
                                            },
                                        })
                                    delta_content["tool_calls"] = tool_calls_list
                                    # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œfinish_reason åº”è¯¥æ˜¯ tool_calls
                                    choice_item["finish_reason"] = "tool_calls"
                                    choice_item["native_finish_reason"] = "tool_calls"
                                    # æ ¹æ®OpenAIè§„èŒƒï¼Œå½“æœ‰tool_callsæ—¶ï¼Œcontenté€šå¸¸ä¸ºnull
                                    delta_content["content"] = None


                                output = {
                                    "id": chat_completion_id,
                                    "object": "chat.completion.chunk",
                                    "model": model_name_for_stream,
                                    "created": created_timestamp,
                                    "choices": [choice_item]
                                }
                                last_body_pos = len(data["body"])
                                yield f"data: {json.dumps(output, ensure_ascii=False, separators=(',', ':'))}\n\n"
                            elif data["done"]: # å¤„ç†ä»… 'done' ä¸º true çš„æƒ…å†µï¼Œå¯èƒ½åŒ…å«å‡½æ•°è°ƒç”¨ä½†æ— æ–°å†…å®¹
                                delta_content = {"role": "assistant"} # è‡³å°‘éœ€è¦ role
                                choice_item = {
                                    "delta": delta_content,
                                    "finish_reason": "stop",
                                    "native_finish_reason": "stop",
                                }

                                if data.get("function") and len(data["function"]) > 0:
                                    tool_calls_list = []
                                    for func_idx, function_call_data in enumerate(data["function"]):
                                        tool_calls_list.append({
                                            "id": f"call_{generate_random_string(24)}",
                                            "index": func_idx,
                                            "type": "function",
                                            "function": {
                                                "name": function_call_data["name"],
                                                "arguments": json.dumps(function_call_data["params"]),
                                            },
                                        })
                                    delta_content["tool_calls"] = tool_calls_list
                                    choice_item["finish_reason"] = "tool_calls"
                                    choice_item["native_finish_reason"] = "tool_calls"
                                    delta_content["content"] = None # æœ‰ tool_calls æ—¶ content ä¸º null

                                output = {
                                    "id": chat_completion_id,
                                    "object": "chat.completion.chunk",
                                    "model": model_name_for_stream,
                                    "created": created_timestamp,
                                    "choices": [choice_item]
                                }
                                yield f"data: {json.dumps(output, ensure_ascii=False, separators=(',', ':'))}\n\n"
                        # --- ç»“æŸå¤„ç†ä» use_stream_response è·å–çš„ data ---
                        
                        yield "data: [DONE]\n\n" # ç¡®ä¿å‘é€æœ€ç»ˆçš„ [DONE] æ ‡è®°

                        if not event_to_set.is_set():
                            event_to_set.set()

                    stream_gen_func = create_stream_generator_from_helper(completion_event)
                    if not result_future.done():
                        result_future.set_result(StreamingResponse(stream_gen_func, media_type="text/event-stream"))
                    else: # å¦‚æœ future å·²ç»å®Œæˆï¼ˆä¾‹å¦‚ï¼Œè¢«å–æ¶ˆï¼‰
                        if not completion_event.is_set(): completion_event.set() # ç¡®ä¿äº‹ä»¶è¢«è®¾ç½®
                    
                    # ä¿®æ”¹åçš„è¿”å›è¯­å¥:
                    return completion_event, submit_button_locator, check_client_disconnected

                except Exception as e:
                    logger.error(f"[{req_id}] (Stream Gen) ä»é˜Ÿåˆ—è·å–æµå¼æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True) # æ·»åŠ  exc_info
                    # å¦‚æœåœ¨æµç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™ï¼Œç¡®ä¿ completion_event è¢«è®¾ç½®ï¼Œä»¥é˜² worker å¡ä½
                    if completion_event and not completion_event.is_set():
                        completion_event.set()
                    # æ­¤å¤„é”™è¯¯å¤„ç†ï¼šå½“å‰ä»£ç ä¼šå°† use_stream è®¾ä¸º False å¹¶å°è¯•å›é€€åˆ° Playwright äº¤äº’ã€‚
                    # å¦‚æœè¾…åŠ©æµæ˜¯ä¸»è¦æ–¹å¼ä¸”å¤±è´¥ï¼Œå¯èƒ½ç›´æ¥æŠ›å‡ºé”™è¯¯æ›´åˆé€‚ï¼Œè€Œä¸æ˜¯é™é»˜å›é€€ã€‚
                    # ä½†æ ¹æ®ç°æœ‰é€»è¾‘ï¼Œæˆ‘ä»¬ä¿æŒå›é€€ã€‚
                    use_stream = False
                    logger.warning(f"[{req_id}] è¾…åŠ©æµå¤„ç†å¤±è´¥ï¼Œå°†å°è¯•å›é€€åˆ° Playwright é¡µé¢äº¤äº’ï¼ˆå¦‚æœé€‚ç”¨ï¼‰ã€‚")


            else: # éæµå¼è¾…åŠ©è·¯å¾„ (use_stream ä¸º True, is_streaming ä¸º False)
                content = None
                reasoning_content = None
                functions = None
                final_data_from_aux_stream = None # åˆå§‹åŒ–

                # ç¡®ä¿ use_stream_response æ˜¯å¼‚æ­¥è¿­ä»£å™¨
                async for data in use_stream_response(req_id): # ä¼ é€’ req_id
                    check_client_disconnected(f"éæµå¼è¾…åŠ©æµ - å¾ªç¯ä¸­ ({req_id}): ")

                    final_data_from_aux_stream = data # å­˜å‚¨æœ€åæ”¶åˆ°çš„æ•°æ®
                    if data.get("done"): # å¯¹äºéæµå¼ï¼Œæˆ‘ä»¬æœŸæœ›ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ•°æ®çš„ "done" æ¶ˆæ¯
                        content = data.get("body") # ä½¿ç”¨ .get() é¿å… KeyError
                        reasoning_content = data.get("reason")
                        functions = data.get("function")
                        break # è·å–åˆ°æ•°æ®åå³ä¸­æ–­
                
                if final_data_from_aux_stream and final_data_from_aux_stream.get("reason") == "internal_timeout":
                    logger.error(f"[{req_id}] Non-streaming request via auxiliary stream failed: Internal Timeout from aux stream.")
                    #ç¡®ä¿ HTTPException å·²å¯¼å…¥: from fastapi import HTTPException
                    raise HTTPException(status_code=502, detail=f"[{req_id}] Auxiliary stream processing error for non-streaming request (internal_timeout).")

                if final_data_from_aux_stream and final_data_from_aux_stream.get("done") is True and content is None and final_data_from_aux_stream.get("reason") != "internal_timeout":
                     logger.error(f"[{req_id}] Non-streaming request via auxiliary stream finished but provided no content. Reason: {final_data_from_aux_stream.get('reason')}.")
                     raise HTTPException(status_code=502, detail=f"[{req_id}] Auxiliary stream finished for non-streaming request but provided no content (Reason: {final_data_from_aux_stream.get('reason')}).")

                model_name_for_json = current_ai_studio_model_id or MODEL_NAME
                message_payload = {"role": "assistant", "content": content}
                finish_reason_val = "stop"

                if functions and len(functions) > 0:
                    tool_calls_list = []
                    for func_idx, function_call_data in enumerate(functions):
                        tool_calls_list.append({
                            "id": f"call_{generate_random_string(24)}",
                            "index": func_idx,
                            "type": "function",
                            "function": {
                                "name": function_call_data["name"],
                                "arguments": json.dumps(function_call_data["params"]),
                            },
                        })
                    message_payload["tool_calls"] = tool_calls_list
                    finish_reason_val = "tool_calls"
                    # å½“æœ‰ tool_calls æ—¶ï¼ŒOpenAI è§„èŒƒé€šå¸¸å°† content è®¾ä¸º null
                    message_payload["content"] = None
                
                if reasoning_content: # å¦‚æœæœ‰æ€è€ƒè¿‡ç¨‹å†…å®¹ï¼Œä¹ŸåŠ å…¥åˆ° message ä¸­
                    message_payload["reasoning_content"] = reasoning_content


                response_payload = {
                    "id": f"{CHAT_COMPLETION_ID_PREFIX}{req_id}-{int(time.time())}",
                    "object": "chat.completion", "created": int(time.time()),
                    "model": model_name_for_json,
                    "choices": [{
                        "index": 0,
                        "message": message_payload,
                        "finish_reason": finish_reason_val,
                        "native_finish_reason": finish_reason_val, # æ·»åŠ  native_finish_reason
                    }],
                    "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0} # ä¼ªä½¿ç”¨æ•°æ®
                }

                if not result_future.done():
                    result_future.set_result(JSONResponse(content=response_payload))
                return None # éæµå¼è¯·æ±‚è¿”å› None


        if not use_stream:
            logger.info(f"[{req_id}] (Refactored Process) å®šä½å“åº”å…ƒç´ ...")
            response_container = page.locator(RESPONSE_CONTAINER_SELECTOR).last
            response_element = response_container.locator(RESPONSE_TEXT_SELECTOR)
            try:
                await expect_async(response_container).to_be_attached(timeout=20000)
                check_client_disconnected("After Response Container Attached: ")
                await expect_async(response_element).to_be_attached(timeout=90000)
                logger.info(f"[{req_id}]   - å“åº”å…ƒç´ å·²å®šä½ã€‚")
            except (PlaywrightAsyncError, asyncio.TimeoutError, ClientDisconnectedError) as locate_err:
                if isinstance(locate_err, ClientDisconnectedError): raise
                logger.error(f"[{req_id}] âŒ é”™è¯¯: å®šä½å“åº”å…ƒç´ å¤±è´¥æˆ–è¶…æ—¶: {locate_err}")
                await save_error_snapshot(f"response_locate_error_{req_id}")
                raise HTTPException(status_code=502, detail=f"[{req_id}] Failed to locate AI Studio response element: {locate_err}")
            except Exception as locate_exc:
                logger.exception(f"[{req_id}] âŒ é”™è¯¯: å®šä½å“åº”å…ƒç´ æ—¶æ„å¤–é”™è¯¯")
                await save_error_snapshot(f"response_locate_unexpected_{req_id}")
                raise HTTPException(status_code=500, detail=f"[{req_id}] Unexpected error locating response element: {locate_exc}")
            check_client_disconnected("After Locate Response: ")

            # --- MERGED: Helper logic integration ---
            use_helper = False
            helper_endpoint = os.environ.get('HELPER_ENDPOINT')
            helper_sapisid = os.environ.get('HELPER_SAPISID')
            if helper_endpoint and helper_sapisid:
                logger.info(f"[{req_id}] æ£€æµ‹åˆ° Helper é…ç½®ï¼Œå°†å°è¯•ä½¿ç”¨ Helper æœåŠ¡è·å–å“åº”ã€‚")
                use_helper = True
            else:
                logger.info(f"[{req_id}] æœªæ£€æµ‹åˆ°å®Œæ•´çš„ Helper é…ç½®ï¼Œå°†ä½¿ç”¨ Playwright é¡µé¢äº¤äº’è·å–å“åº”ã€‚")

            if use_helper and (not use_stream):
                try:
                    if is_streaming:
                        completion_event = Event()
                        async def create_stream_generator_from_helper(event_to_set: Event) -> AsyncGenerator[str, None]:
                            try:
                                async for data_chunk in use_helper_get_response(helper_endpoint, helper_sapisid):
                                    if client_disconnected_event.is_set():
                                        logger.info(f"[{req_id}] (Helper Stream Gen) å®¢æˆ·ç«¯æ–­å¼€ï¼Œåœæ­¢ã€‚")
                                        break
                                    if data_chunk == "[ERROR]": # Helper indicated an error
                                        logger.error(f"[{req_id}] (Helper Stream Gen) Helper æœåŠ¡è¿”å›é”™è¯¯ä¿¡å·ã€‚")
                                        yield generate_sse_error_chunk("Helper service reported an error.", req_id, "helper_error")
                                        break
                                    if data_chunk == "[DONE]": # Helper indicated completion
                                        logger.info(f"[{req_id}] (Helper Stream Gen) Helper æœåŠ¡æŒ‡ç¤ºå®Œæˆã€‚")
                                        break
                                    yield f"data: {data_chunk}\n\n" # Assume helper sends pre-formatted SSE data chunks
                                yield "data: [DONE]\n\n" # Ensure final DONE is sent
                            except Exception as e_helper_stream:
                                logger.error(f"[{req_id}] (Helper Stream Gen) ä» Helper è·å–æµå¼æ•°æ®æ—¶å‡ºé”™: {e_helper_stream}", exc_info=True)
                                yield generate_sse_error_chunk(f"Error streaming from helper: {e_helper_stream}", req_id)
                                yield "data: [DONE]\n\n"
                            finally:
                                if not event_to_set.is_set(): event_to_set.set()

                        stream_gen_func = create_stream_generator_from_helper(completion_event)
                        if not result_future.done():
                            result_future.set_result(StreamingResponse(stream_gen_func, media_type="text/event-stream"))
                        else:
                            if not completion_event.is_set(): completion_event.set() # Ensure event is set if future already done
                        return completion_event # Return the event for the worker to wait on
                    else: # Non-streaming with helper
                        full_response_content = ""
                        think_content = ""
                        body_content = ""
                        async for data_chunk in use_helper_get_response(helper_endpoint, helper_sapisid):
                            if data_chunk == "[ERROR]":
                                raise HTTPException(status_code=502, detail=f"[{req_id}] Helper service reported an error during non-streaming fetch.")
                            if data_chunk == "[DONE]":
                                break
                            try:
                                # Assuming helper sends OpenAI-like delta chunks even for non-streaming,
                                # and we need to aggregate them.
                                stream_data = json.loads(data_chunk)
                                if "choices" in stream_data and stream_data["choices"]:
                                    delta = stream_data["choices"][0].get("delta", {})
                                    if "reasoning_content" in delta: # Example for structured content
                                        think_content += delta["reasoning_content"]
                                    elif "content" in delta:
                                        body_content += delta["content"]
                            except json.JSONDecodeError:
                                logger.warning(f"[{req_id}] (Helper Non-Stream) æ— æ³•è§£ææ¥è‡ª Helper çš„ JSON æ•°æ®å—: {data_chunk}")
                                body_content += data_chunk # Fallback: append raw if not JSON

                        if think_content:
                            full_response_content = f"<think>{think_content}</think>\n{body_content}"
                        else:
                            full_response_content = body_content

                        response_payload = {
                            "id": f"{CHAT_COMPLETION_ID_PREFIX}{req_id}-{int(time.time())}",
                            "object": "chat.completion", "created": int(time.time()), "model": MODEL_NAME,
                            "choices": [{"index": 0, "message": {"role": "assistant", "content": full_response_content}, "finish_reason": "stop"}],
                            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
                        }
                        if not result_future.done():
                            result_future.set_result(JSONResponse(content=response_payload))
                        return None # No event for non-streaming
                except Exception as e_helper:
                    logger.error(f"[{req_id}] ä½¿ç”¨ Helper æœåŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e_helper}ã€‚å°†å›é€€åˆ° Playwright é¡µé¢äº¤äº’ã€‚", exc_info=True)
                    use_helper = False # Fallback to Playwright

            # --- Fallback to Playwright page interaction if helper is not used or failed ---
            if (not use_helper) and (not use_stream):
                logger.info(f"[{req_id}] (Refactored Process) ç­‰å¾…å“åº”ç”Ÿæˆå®Œæˆæˆ–æ£€æµ‹æ¨¡å‹é”™è¯¯...")
                MODEL_ERROR_CONTAINER_SELECTOR = 'ms-chat-turn:last-child div.model-error'
                completion_detected_via_edit_button = False
                page_model_error_message: Optional[str] = None
                completion_detected_via_edit_button = await _wait_for_response_completion(
                    page,
                    input_field_locator,
                    submit_button_locator,
                    page.locator(EDIT_MESSAGE_BUTTON_SELECTOR), # edit_button_locator
                    req_id, # req_id for _wait_for_response_completion
                    check_client_disconnected, # check_client_disconnected_func
                    req_id, # current_chat_id
                )
                check_client_disconnected("After _wait_for_response_completion attempt: ")
                if not completion_detected_via_edit_button:
                    logger.info(f"[{req_id}] _wait_for_response_completion æœªé€šè¿‡ç¼–è¾‘æŒ‰é’®ç¡®è®¤å®Œæˆï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨æ¨¡å‹é”™è¯¯...")
                    try:
                        error_container_locator = page.locator(MODEL_ERROR_CONTAINER_SELECTOR)
                        await expect_async(error_container_locator).to_be_visible(timeout=2000)
                        specific_error_text_locator = error_container_locator.locator('*:not(mat-icon)')
                        try:
                            page_model_error_message = await specific_error_text_locator.first.text_content(timeout=500)
                            if page_model_error_message: page_model_error_message = page_model_error_message.strip()
                        except PlaywrightAsyncError:
                            page_model_error_message = await error_container_locator.text_content(timeout=500)
                            if page_model_error_message: page_model_error_message = page_model_error_message.strip()
                        if page_model_error_message:
                            logger.error(f"[{req_id}] âŒ æ£€æµ‹åˆ° AI Studio æ¨¡å‹è¿”å›çš„é”™è¯¯ä¿¡æ¯: {page_model_error_message}")
                            await save_error_snapshot(f"model_returned_error_{req_id}")
                            raise HTTPException(status_code=502, detail=f"[{req_id}] AI Studio Model Error: {page_model_error_message}")
                        else:
                            logger.warning(f"[{req_id}] æ£€æµ‹åˆ° model-error å®¹å™¨ï¼Œä½†æœªèƒ½æå–å…·ä½“é”™è¯¯æ–‡æœ¬ã€‚")
                            await save_error_snapshot(f"model_error_container_no_text_{req_id}")
                            raise HTTPException(status_code=502, detail=f"[{req_id}] AI Studio returned an unspecified model error (error container found).")
                    except (PlaywrightAsyncError, asyncio.TimeoutError) as e_model_err_check:
                        logger.info(f"[{req_id}] æœªæ£€æµ‹åˆ°æ˜ç¡®çš„ model-error å®¹å™¨ (æˆ–æ£€æŸ¥è¶…æ—¶: {type(e_model_err_check).__name__})ã€‚ç»§ç»­æŒ‰åŸè¶…æ—¶é€»è¾‘å¤„ç†ã€‚")
                        if not completion_detected_via_edit_button:
                             raise HTTPException(status_code=504, detail=f"[{req_id}] AI Studio response generation timed out (and no specific model error detected).")
                if not completion_detected_via_edit_button:
                    logger.info(f"[{req_id}] (Refactored Process) æ£€æŸ¥é¡µé¢ Toast é”™è¯¯æç¤º...")
                    page_toast_error = await detect_and_extract_page_error(page, req_id)
                    if page_toast_error:
                        logger.error(f"[{req_id}] âŒ é”™è¯¯: AI Studio é¡µé¢è¿”å› Toast é”™è¯¯: {page_toast_error}")
                        await save_error_snapshot(f"page_toast_error_detected_{req_id}")
                        raise HTTPException(status_code=502, detail=f"[{req_id}] AI Studio Page Error: {page_toast_error}")
                    check_client_disconnected("After Page Toast Error Check: ")
                else:
                    logger.info(f"[{req_id}] å·²é€šè¿‡ç¼–è¾‘æŒ‰é’®ç¡®è®¤å®Œæˆï¼Œè·³è¿‡ Toast é”™è¯¯æ£€æŸ¥ã€‚")
                if not completion_detected_via_edit_button:
                    logger.error(f"[{req_id}] é€»è¾‘å¼‚å¸¸ï¼šå“åº”æœªå®Œæˆï¼Œä¹Ÿæœªæ£€æµ‹åˆ°æ¨¡å‹é”™è¯¯ï¼Œä½†ä¸åº”åˆ°è¾¾æ­¤å¤„è·å–å†…å®¹ã€‚")
                    raise HTTPException(status_code=500, detail=f"[{req_id}] Internal logic error in response processing.")
                logger.info(f"[{req_id}] (Refactored Process) è·å–æœ€ç»ˆå“åº”å†…å®¹...")
                final_content = await _get_final_response_content(
                    page, req_id, check_client_disconnected
                )
                if final_content is None:
                    try:
                        error_container_locator = page.locator(MODEL_ERROR_CONTAINER_SELECTOR)
                        if await error_container_locator.is_visible(timeout=500):
                            late_error_message = await error_container_locator.text_content(timeout=300) or "Unknown model error after content fetch attempt."
                            logger.error(f"[{req_id}] è·å–å†…å®¹å¤±è´¥åï¼Œæ£€æµ‹åˆ°å»¶è¿Ÿå‡ºç°çš„æ¨¡å‹é”™è¯¯: {late_error_message.strip()}")
                            raise HTTPException(status_code=502, detail=f"[{req_id}] AI Studio Model Error (detected after content fetch failure): {late_error_message.strip()}")
                    except:
                        pass
                    raise HTTPException(status_code=500, detail=f"[{req_id}] Failed to extract final response content from AI Studio.")
                check_client_disconnected("After Get Content: ")
                logger.info(f"[{req_id}] (Refactored Process) æ ¼å¼åŒ–å¹¶è®¾ç½®ç»“æœ (æ¨¡å¼: {'æµå¼' if is_streaming else 'éæµå¼'})...")
                if is_streaming:
                    completion_event = Event()
                    async def create_stream_generator(event_to_set: Event, content_to_stream: str) -> AsyncGenerator[str, None]:
                        logger.info(f"[{req_id}] (Stream Gen) å¼€å§‹ä¼ªæµå¼è¾“å‡º ({len(content_to_stream)} chars)...")
                        try:
                            total_chars = len(content_to_stream)
                            chunk_size = 5
                            for i in range(0, total_chars, chunk_size):
                                if client_disconnected_event.is_set():
                                    logger.info(f"[{req_id}] (Stream Gen) æ–­å¼€è¿æ¥ï¼Œåœæ­¢ã€‚")
                                    break
                                chunk = content_to_stream[i:i + chunk_size]
                                if not chunk:
                                    continue
                                yield generate_sse_chunk(chunk, req_id, MODEL_NAME)
                                await asyncio.sleep(PSEUDO_STREAM_DELAY)
                            yield generate_sse_stop_chunk(req_id, MODEL_NAME)
                            yield "data: [DONE]\n\n"
                            logger.info(f"[{req_id}] (Stream Gen) âœ… ä¼ªæµå¼å“åº”å‘é€å®Œæ¯•ã€‚")
                        except asyncio.CancelledError:
                            logger.info(f"[{req_id}] (Stream Gen) æµç”Ÿæˆå™¨è¢«å–æ¶ˆã€‚")
                        except Exception as e:
                            logger.exception(f"[{req_id}] (Stream Gen) âŒ ä¼ªæµå¼ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™")
                            try: yield generate_sse_error_chunk(f"Stream generation error: {e}", req_id); yield "data: [DONE]\n\n"
                            except: pass
                        finally:
                            logger.info(f"[{req_id}] (Stream Gen) è®¾ç½®å®Œæˆäº‹ä»¶ã€‚")
                            if not event_to_set.is_set(): event_to_set.set()
                    stream_generator_func = create_stream_generator(completion_event, final_content)
                    if not result_future.done():
                        result_future.set_result(StreamingResponse(stream_generator_func, media_type="text/event-stream"))
                        logger.info(f"[{req_id}] (Refactored Process) æµå¼å“åº”ç”Ÿæˆå™¨å·²è®¾ç½®ã€‚")
                    else:
                        logger.warning(f"[{req_id}] (Refactored Process) Future å·²å®Œæˆ/å–æ¶ˆï¼Œæ— æ³•è®¾ç½®æµå¼ç»“æœã€‚")
                        if not completion_event.is_set(): completion_event.set()
                    return completion_event
                else:
                    response_payload = {
                        "id": f"{CHAT_COMPLETION_ID_PREFIX}{req_id}-{int(time.time())}",
                        "object": "chat.completion", "created": int(time.time()), "model": MODEL_NAME,
                        "choices": [{"index": 0, "message": {"role": "assistant", "content": final_content}, "finish_reason": "stop"}],
                        "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
                    }
                    if not result_future.done():
                        result_future.set_result(JSONResponse(content=response_payload))
                        logger.info(f"[{req_id}] (Refactored Process) éæµå¼ JSON å“åº”å·²è®¾ç½®ã€‚")
                    else:
                        logger.warning(f"[{req_id}] (Refactored Process) Future å·²å®Œæˆ/å–æ¶ˆï¼Œæ— æ³•è®¾ç½® JSON ç»“æœã€‚")
                    return None
    except ClientDisconnectedError as disco_err:
        logger.info(f"[{req_id}] (Refactored Process) æ•è·åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ä¿¡å·: {disco_err}")
        if not result_future.done():
             result_future.set_exception(HTTPException(status_code=499, detail=f"[{req_id}] Client disconnected during processing."))
    except HTTPException as http_err:
        logger.warning(f"[{req_id}] (Refactored Process) æ•è·åˆ° HTTP å¼‚å¸¸: {http_err.status_code} - {http_err.detail}")
        if not result_future.done(): result_future.set_exception(http_err)
    except PlaywrightAsyncError as pw_err:
        logger.error(f"[{req_id}] (Refactored Process) æ•è·åˆ° Playwright é”™è¯¯: {pw_err}")
        await save_error_snapshot(f"process_playwright_error_{req_id}")
        if not result_future.done(): result_future.set_exception(HTTPException(status_code=502, detail=f"[{req_id}] Playwright interaction failed: {pw_err}"))
    except asyncio.TimeoutError as timeout_err:
        logger.error(f"[{req_id}] (Refactored Process) æ•è·åˆ°æ“ä½œè¶…æ—¶: {timeout_err}")
        await save_error_snapshot(f"process_timeout_error_{req_id}")
        if not result_future.done(): result_future.set_exception(HTTPException(status_code=504, detail=f"[{req_id}] Operation timed out: {timeout_err}"))
    except asyncio.CancelledError:
        logger.info(f"[{req_id}] (Refactored Process) ä»»åŠ¡è¢«å–æ¶ˆã€‚")
        if not result_future.done(): result_future.cancel("Processing task cancelled")
    except Exception as e:
        logger.exception(f"[{req_id}] (Refactored Process) æ•è·åˆ°æ„å¤–é”™è¯¯")
        await save_error_snapshot(f"process_unexpected_error_{req_id}")
        if not result_future.done(): result_future.set_exception(HTTPException(status_code=500, detail=f"[{req_id}] Unexpected server error: {e}"))
    finally:
        if disconnect_check_task and not disconnect_check_task.done():
            disconnect_check_task.cancel()
            try: await disconnect_check_task
            except asyncio.CancelledError: pass
            except Exception as task_clean_err: logger.error(f"[{req_id}] æ¸…ç†ä»»åŠ¡æ—¶å‡ºé”™: {task_clean_err}")
        logger.info(f"[{req_id}] (Refactored Process) å¤„ç†å®Œæˆã€‚")
        if is_streaming and completion_event and not completion_event.is_set() and (result_future.done() and result_future.exception() is not None):
             logger.warning(f"[{req_id}] (Refactored Process) æµå¼è¯·æ±‚å¼‚å¸¸ï¼Œç¡®ä¿å®Œæˆäº‹ä»¶å·²è®¾ç½®ã€‚")
             completion_event.set()
        return completion_event, submit_button_locator, check_client_disconnected

# --- Main Chat Endpoint ---
@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest, http_request: Request):
    req_id = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=7))
    logger.info(f"[{req_id}] æ”¶åˆ° /v1/chat/completions è¯·æ±‚ (Stream={request.stream})")
    logger.debug(f"[{req_id}] å®Œæ•´è¯·æ±‚å‚æ•°: {request.model_dump_json(indent=2)}")
    launch_mode = os.environ.get('LAUNCH_MODE', 'unknown')
    browser_page_critical = launch_mode != "direct_debug_no_browser"
    service_unavailable = is_initializing or \
                          not is_playwright_ready or \
                          (browser_page_critical and (not is_page_ready or not is_browser_connected)) or \
                          not worker_task or worker_task.done()
    if service_unavailable:
        status_code = 503
        error_details = []
        if is_initializing: error_details.append("åˆå§‹åŒ–è¿›è¡Œä¸­")
        if not is_playwright_ready: error_details.append("Playwright æœªå°±ç»ª")
        if browser_page_critical:
            if not is_browser_connected: error_details.append("æµè§ˆå™¨æœªè¿æ¥")
            if not is_page_ready: error_details.append("é¡µé¢æœªå°±ç»ª")
        if not worker_task or worker_task.done(): error_details.append("Worker æœªè¿è¡Œ")
        detail = f"[{req_id}] æœåŠ¡å½“å‰ä¸å¯ç”¨ ({', '.join(error_details)}). è¯·ç¨åé‡è¯•."
        logger.error(f"[{req_id}] æœåŠ¡ä¸å¯ç”¨è¯¦æƒ…: {detail}")
        raise HTTPException(status_code=status_code, detail=detail, headers={"Retry-After": "30"})
    result_future = Future()
    request_item = {
        "req_id": req_id, "request_data": request, "http_request": http_request,
        "result_future": result_future, "enqueue_time": time.time(), "cancelled": False
    }
    await request_queue.put(request_item)
    logger.info(f"[{req_id}] è¯·æ±‚å·²åŠ å…¥é˜Ÿåˆ— (å½“å‰é˜Ÿåˆ—é•¿åº¦: {request_queue.qsize()})")
    try:
        timeout_seconds = RESPONSE_COMPLETION_TIMEOUT / 1000 + 120
        result = await asyncio.wait_for(result_future, timeout=timeout_seconds)
        logger.info(f"[{req_id}] Worker å¤„ç†å®Œæˆï¼Œè¿”å›ç»“æœã€‚")
        return result
    except asyncio.TimeoutError:
        logger.error(f"[{req_id}] âŒ ç­‰å¾… Worker å“åº”è¶…æ—¶ ({timeout_seconds}s)ã€‚")
        raise HTTPException(status_code=504, detail=f"[{req_id}] Request processing timed out waiting for worker response.")
    except asyncio.CancelledError:
        logger.info(f"[{req_id}] è¯·æ±‚ Future è¢«å–æ¶ˆ (å¯èƒ½ç”±å®¢æˆ·ç«¯æ–­å¼€è¿æ¥è§¦å‘)ã€‚")
        if not result_future.done() or result_future.exception() is None:
             raise HTTPException(status_code=499, detail=f"[{req_id}] Request cancelled by client or server.")
        else:
             raise result_future.exception()
    except HTTPException as http_err:
        raise http_err
    except Exception as e:
        logger.exception(f"[{req_id}] âŒ ç­‰å¾… Worker å“åº”æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯")
        raise HTTPException(status_code=500, detail=f"[{req_id}] Unexpected error waiting for worker response: {e}")

# --- Cancel Request Endpoint ---
async def cancel_queued_request(req_id: str) -> bool:
    cancelled = False
    items_to_requeue = []
    found = False
    try:
        while True:
            item = request_queue.get_nowait()
            if item.get("req_id") == req_id and not item.get("cancelled", False):
                logger.info(f"[{req_id}] åœ¨é˜Ÿåˆ—ä¸­æ‰¾åˆ°è¯·æ±‚ï¼Œæ ‡è®°ä¸ºå·²å–æ¶ˆã€‚")
                item["cancelled"] = True
                item_future = item.get("result_future")
                if item_future and not item_future.done():
                    item_future.set_exception(HTTPException(status_code=499, detail=f"[{req_id}] Request cancelled by API call."))
                items_to_requeue.append(item)
                cancelled = True
                found = True
            else:
                items_to_requeue.append(item)
    except asyncio.QueueEmpty:
        pass
    finally:
        for item in items_to_requeue:
            await request_queue.put(item)
    return cancelled

@app.post("/v1/cancel/{req_id}")
async def cancel_request(req_id: str):
    logger.info(f"[{req_id}] æ”¶åˆ°å–æ¶ˆè¯·æ±‚ã€‚")
    cancelled = await cancel_queued_request(req_id)
    if cancelled:
        return JSONResponse(content={"success": True, "message": f"Request {req_id} marked as cancelled in queue."})
    else:
        return JSONResponse(
            content={"success": False, "message": f"Request {req_id} not found in queue (it might be processing or already finished)."},
            status_code=404
        )

@app.get("/v1/queue")
async def get_queue_status():
    queue_items = []
    items_to_requeue = []
    try:
        while True:
            item = request_queue.get_nowait()
            items_to_requeue.append(item)
            req_id = item.get("req_id", "unknown")
            timestamp = item.get("enqueue_time", 0)
            is_streaming = item.get("request_data").stream if hasattr(item.get("request_data", {}), "stream") else False
            cancelled = item.get("cancelled", False)
            queue_items.append({
                "req_id": req_id, "enqueue_time": timestamp,
                "wait_time_seconds": round(time.time() - timestamp, 2) if timestamp else None,
                "is_streaming": is_streaming, "cancelled": cancelled
            })
    except asyncio.QueueEmpty:
        pass
    finally:
        for item in items_to_requeue:
            await request_queue.put(item)
    return JSONResponse(content={
        "queue_length": len(queue_items),
        "is_processing_locked": processing_lock.locked(),
        "items": sorted(queue_items, key=lambda x: x.get("enqueue_time", 0))
    })

@app.websocket("/ws/logs")
async def websocket_log_endpoint(websocket: WebSocket):
    if not log_ws_manager:
        try:
            await websocket.accept()
            await websocket.send_text(json.dumps({
                "type": "error", "status": "disconnected",
                "message": "æ—¥å¿—æœåŠ¡å†…éƒ¨é”™è¯¯ (ç®¡ç†å™¨æœªåˆå§‹åŒ–)ã€‚",
                "timestamp": datetime.datetime.now().isoformat()}))
            await websocket.close(code=1011)
        except Exception: pass
        return
    client_id = str(uuid.uuid4())
    try:
        await log_ws_manager.connect(client_id, websocket)
        while True:
            data = await websocket.receive_text()
            if data.lower() == "ping":
                 await websocket.send_text(json.dumps({"type": "pong", "timestamp": datetime.datetime.now().isoformat()}))
    except WebSocketDisconnect:
        pass
    except Exception as e:
        logger.error(f"æ—¥å¿— WebSocket (å®¢æˆ·ç«¯ {client_id}) å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
    finally:
        if log_ws_manager:
            log_ws_manager.disconnect(client_id)

# --- Main Guard ---
if __name__ == "__main__":
    print("é”™è¯¯: server.py ä¸åº”ç›´æ¥ä½œä¸ºä¸»è„šæœ¬è¿è¡Œã€‚", file=sys.stderr)
    print("è¯·ä½¿ç”¨ launch_camoufox.py (ç”¨äºè°ƒè¯•) æˆ– start.py (ç”¨äºåå°æœåŠ¡) æ¥å¯åŠ¨ã€‚", file=sys.stderr)
    print("\nå¦‚æœç¡®å®éœ€è¦ç›´æ¥è¿è¡Œ server.py è¿›è¡Œåº•å±‚æµ‹è¯• (ä¸æ¨è):", file=sys.stderr)
    print("  1. ç¡®ä¿å·²è®¾ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡ï¼Œå¦‚ CAMOUFOX_WS_ENDPOINT, LAUNCH_MODE, SERVER_REDIRECT_PRINT, SERVER_LOG_LEVELã€‚", file=sys.stderr)
    print("  2. ç„¶åå¯ä»¥å°è¯•: python -m uvicorn server:app --host 0.0.0.0 --port <ç«¯å£å·>", file=sys.stderr)
    print("     ä¾‹å¦‚: LAUNCH_MODE=direct_debug_no_browser SERVER_REDIRECT_PRINT=false python -m uvicorn server:app --port 8000", file=sys.stderr)
    sys.exit(1)

# --- Model Switching Helper ---
async def switch_ai_studio_model(page: AsyncPage, model_id: str, req_id: str) -> bool:
    logger.info(f"[{req_id}] å¼€å§‹åˆ‡æ¢æ¨¡å‹åˆ°: {model_id}")
    original_prefs_str: Optional[str] = None
    original_prompt_model: Optional[str] = None
    new_chat_url = f"https://{AI_STUDIO_URL_PATTERN}prompts/new_chat"
    try:
        original_prefs_str = await page.evaluate("() => localStorage.getItem('aiStudioUserPreference')")
        if original_prefs_str:
            try:
                original_prefs_obj = json.loads(original_prefs_str)
                original_prompt_model = original_prefs_obj.get("promptModel")
                logger.info(f"[{req_id}] åˆ‡æ¢å‰ localStorage.promptModel ä¸º: {original_prompt_model or 'æœªè®¾ç½®'}")
            except json.JSONDecodeError:
                logger.warning(f"[{req_id}] æ— æ³•è§£æåŸå§‹çš„ aiStudioUserPreference JSON å­—ç¬¦ä¸²ã€‚")
                original_prefs_str = None
        current_prefs_for_modification = json.loads(original_prefs_str) if original_prefs_str else {}
        full_model_path = f"models/{model_id}"
        if current_prefs_for_modification.get("promptModel") == full_model_path:
            logger.info(f"[{req_id}] æ¨¡å‹å·²ç»è®¾ç½®ä¸º {model_id} (localStorage ä¸­å·²æ˜¯ç›®æ ‡å€¼)ï¼Œæ— éœ€åˆ‡æ¢")
            if page.url != new_chat_url:
                 logger.info(f"[{req_id}] å½“å‰ URL ä¸æ˜¯ new_chat ({page.url})ï¼Œå¯¼èˆªåˆ° {new_chat_url}")
                 await page.goto(new_chat_url, wait_until="domcontentloaded", timeout=30000)
                 await expect_async(page.locator(INPUT_SELECTOR)).to_be_visible(timeout=30000)
            return True
        logger.info(f"[{req_id}] ä» {current_prefs_for_modification.get('promptModel', 'æœªçŸ¥')} æ›´æ–° localStorage.promptModel ä¸º {full_model_path}")
        current_prefs_for_modification["promptModel"] = full_model_path
        await page.evaluate("(prefsStr) => localStorage.setItem('aiStudioUserPreference', prefsStr)", json.dumps(current_prefs_for_modification))
        logger.info(f"[{req_id}] localStorage å·²æ›´æ–°ï¼Œå¯¼èˆªåˆ° '{new_chat_url}' åº”ç”¨æ–°æ¨¡å‹...")
        await page.goto(new_chat_url, wait_until="domcontentloaded", timeout=30000)
        input_field = page.locator(INPUT_SELECTOR)
        await expect_async(input_field).to_be_visible(timeout=30000)
        logger.info(f"[{req_id}] é¡µé¢å·²å¯¼èˆªåˆ°æ–°èŠå¤©å¹¶åŠ è½½å®Œæˆï¼Œè¾“å…¥æ¡†å¯è§")
        final_prefs_str = await page.evaluate("() => localStorage.getItem('aiStudioUserPreference')")
        final_prompt_model_in_storage: Optional[str] = None
        if final_prefs_str:
            try:
                final_prefs_obj = json.loads(final_prefs_str)
                final_prompt_model_in_storage = final_prefs_obj.get("promptModel")
            except json.JSONDecodeError:
                logger.warning(f"[{req_id}] æ— æ³•è§£æåˆ·æ–°åçš„ aiStudioUserPreference JSON å­—ç¬¦ä¸²ã€‚")
        if final_prompt_model_in_storage == full_model_path:
            logger.info(f"[{req_id}] âœ… AI Studio localStorage ä¸­æ¨¡å‹å·²æˆåŠŸè®¾ç½®ä¸º: {full_model_path}")
            page_display_match = False
            expected_display_name_for_target_id = None
            actual_displayed_model_name_on_page = "æ— æ³•è¯»å–"
            if parsed_model_list:
                for m_obj in parsed_model_list:
                    if m_obj.get("id") == model_id:
                        expected_display_name_for_target_id = m_obj.get("display_name")
                        break
            if not expected_display_name_for_target_id:
                logger.warning(f"[{req_id}] æ— æ³•åœ¨parsed_model_listä¸­æ‰¾åˆ°ç›®æ ‡ID '{model_id}' çš„æ˜¾ç¤ºåç§°ï¼Œè·³è¿‡é¡µé¢æ˜¾ç¤ºåç§°éªŒè¯ã€‚è¿™å¯èƒ½ä¸å‡†ç¡®ã€‚")
                page_display_match = True
            else:
                try:
                    model_name_locator = page.locator('mat-select[data-test-ms-model-selector] div.model-option-content span.gmat-body-medium')
                    actual_displayed_model_name_on_page_raw = await model_name_locator.first.inner_text(timeout=5000)
                    actual_displayed_model_name_on_page = actual_displayed_model_name_on_page_raw.strip()
                    normalized_actual_display = actual_displayed_model_name_on_page.lower()
                    normalized_expected_display = expected_display_name_for_target_id.strip().lower()
                    if normalized_actual_display == normalized_expected_display:
                        page_display_match = True
                        logger.info(f"[{req_id}] âœ… é¡µé¢æ˜¾ç¤ºæ¨¡å‹ ('{actual_displayed_model_name_on_page}') ä¸æœŸæœ› ('{expected_display_name_for_target_id}') ä¸€è‡´ã€‚")
                    else:
                        logger.error(f"[{req_id}] âŒ é¡µé¢æ˜¾ç¤ºæ¨¡å‹ ('{actual_displayed_model_name_on_page}') ä¸æœŸæœ› ('{expected_display_name_for_target_id}') ä¸ä¸€è‡´ã€‚(Raw page: '{actual_displayed_model_name_on_page_raw}')")
                except Exception as e_disp:
                    logger.warning(f"[{req_id}] è¯»å–é¡µé¢æ˜¾ç¤ºçš„å½“å‰æ¨¡å‹åç§°æ—¶å‡ºé”™: {e_disp}ã€‚å°†æ— æ³•éªŒè¯é¡µé¢æ˜¾ç¤ºã€‚")
            if page_display_match:
                return True
            else:
                logger.error(f"[{req_id}] âŒ æ¨¡å‹åˆ‡æ¢å¤±è´¥ï¼Œå› ä¸ºé¡µé¢æ˜¾ç¤ºçš„æ¨¡å‹ä¸æœŸæœ›ä¸ç¬¦ (å³ä½¿localStorageå¯èƒ½å·²æ›´æ”¹)ã€‚")
        else:
            logger.error(f"[{req_id}] âŒ AI Studio æœªæ¥å—æ¨¡å‹æ›´æ”¹ (localStorage)ã€‚æœŸæœ›='{full_model_path}', å®é™…='{final_prompt_model_in_storage or 'æœªè®¾ç½®æˆ–æ— æ•ˆ'}'.")
        logger.info(f"[{req_id}] æ¨¡å‹åˆ‡æ¢å¤±è´¥ã€‚å°è¯•æ¢å¤åˆ°é¡µé¢å½“å‰å®é™…æ˜¾ç¤ºçš„æ¨¡å‹çš„çŠ¶æ€...")
        current_displayed_name_for_revert_raw = "æ— æ³•è¯»å–"
        current_displayed_name_for_revert_stripped = "æ— æ³•è¯»å–"
        try:
            model_name_locator_revert = page.locator('mat-select[data-test-ms-model-selector] div.model-option-content span.gmat-body-medium')
            current_displayed_name_for_revert_raw = await model_name_locator_revert.first.inner_text(timeout=5000)
            current_displayed_name_for_revert_stripped = current_displayed_name_for_revert_raw.strip()
            logger.info(f"[{req_id}] æ¢å¤ï¼šé¡µé¢å½“å‰æ˜¾ç¤ºçš„æ¨¡å‹åç§° (åŸå§‹: '{current_displayed_name_for_revert_raw}', æ¸…ç†å: '{current_displayed_name_for_revert_stripped}')")
        except Exception as e_read_disp_revert:
            logger.warning(f"[{req_id}] æ¢å¤ï¼šè¯»å–é¡µé¢å½“å‰æ˜¾ç¤ºæ¨¡å‹åç§°å¤±è´¥: {e_read_disp_revert}ã€‚å°†å°è¯•å›é€€åˆ°åŸå§‹localStorageã€‚")
            if original_prefs_str:
                logger.info(f"[{req_id}] æ¢å¤ï¼šç”±äºæ— æ³•è¯»å–å½“å‰é¡µé¢æ˜¾ç¤ºï¼Œå°è¯•å°† localStorage æ¢å¤åˆ°åŸå§‹çŠ¶æ€: '{original_prompt_model or 'æœªè®¾ç½®'}'")
                await page.evaluate("(origPrefs) => localStorage.setItem('aiStudioUserPreference', origPrefs)", original_prefs_str)
                logger.info(f"[{req_id}] æ¢å¤ï¼šå¯¼èˆªåˆ° '{new_chat_url}' ä»¥åº”ç”¨æ¢å¤çš„åŸå§‹ localStorage è®¾ç½®...")
                await page.goto(new_chat_url, wait_until="domcontentloaded", timeout=20000)
                await expect_async(page.locator(INPUT_SELECTOR)).to_be_visible(timeout=20000)
                logger.info(f"[{req_id}] æ¢å¤ï¼šé¡µé¢å·²å¯¼èˆªåˆ°æ–°èŠå¤©å¹¶åŠ è½½ï¼Œå·²å°è¯•åº”ç”¨åŸå§‹ localStorageã€‚")
            else:
                logger.warning(f"[{req_id}] æ¢å¤ï¼šæ— æœ‰æ•ˆçš„åŸå§‹ localStorage çŠ¶æ€å¯æ¢å¤ï¼Œä¹Ÿæ— æ³•è¯»å–å½“å‰é¡µé¢æ˜¾ç¤ºã€‚")
            return False
        model_id_to_revert_to = None
        if parsed_model_list and current_displayed_name_for_revert_stripped != "æ— æ³•è¯»å–":
            normalized_current_display_for_revert = current_displayed_name_for_revert_stripped.lower()
            for m_obj in parsed_model_list:
                parsed_list_display_name = m_obj.get("display_name", "").strip().lower()
                if parsed_list_display_name == normalized_current_display_for_revert:
                    model_id_to_revert_to = m_obj.get("id")
                    logger.info(f"[{req_id}] æ¢å¤ï¼šé¡µé¢æ˜¾ç¤ºåç§° '{current_displayed_name_for_revert_stripped}' å¯¹åº”æ¨¡å‹ID: {model_id_to_revert_to}")
                    break
            if not model_id_to_revert_to:
                logger.warning(f"[{req_id}] æ¢å¤ï¼šæ— æ³•åœ¨ parsed_model_list ä¸­æ‰¾åˆ°ä¸é¡µé¢æ˜¾ç¤ºåç§° '{current_displayed_name_for_revert_stripped}' åŒ¹é…çš„æ¨¡å‹IDã€‚")
        else:
            if current_displayed_name_for_revert_stripped == "æ— æ³•è¯»å–":
                 logger.warning(f"[{req_id}] æ¢å¤ï¼šå› æ— æ³•è¯»å–é¡µé¢æ˜¾ç¤ºåç§°ï¼Œæ•…ä¸èƒ½ä» parsed_model_list è½¬æ¢IDã€‚")
            else:
                 logger.warning(f"[{req_id}] æ¢å¤ï¼šparsed_model_list ä¸ºç©ºï¼Œæ— æ³•ä»æ˜¾ç¤ºåç§° '{current_displayed_name_for_revert_stripped}' è½¬æ¢æ¨¡å‹IDã€‚")
        if model_id_to_revert_to:
            base_prefs_for_final_revert = {}
            try:
                current_ls_content_str = await page.evaluate("() => localStorage.getItem('aiStudioUserPreference')")
                if current_ls_content_str:
                    base_prefs_for_final_revert = json.loads(current_ls_content_str)
                elif original_prefs_str:
                    base_prefs_for_final_revert = json.loads(original_prefs_str)
            except json.JSONDecodeError:
                logger.warning(f"[{req_id}] æ¢å¤ï¼šè§£æç°æœ‰ localStorage ä»¥æ„å»ºæ¢å¤åå¥½å¤±è´¥ã€‚")
            path_to_revert_to = f"models/{model_id_to_revert_to}"
            base_prefs_for_final_revert["promptModel"] = path_to_revert_to
            logger.info(f"[{req_id}] æ¢å¤ï¼šå‡†å¤‡å°† localStorage.promptModel è®¾ç½®å›é¡µé¢å®é™…æ˜¾ç¤ºçš„æ¨¡å‹çš„è·¯å¾„: '{path_to_revert_to}'")
            await page.evaluate("(prefsStr) => localStorage.setItem('aiStudioUserPreference', prefsStr)", json.dumps(base_prefs_for_final_revert))
            logger.info(f"[{req_id}] æ¢å¤ï¼šå¯¼èˆªåˆ° '{new_chat_url}' ä»¥åº”ç”¨æ¢å¤åˆ° '{model_id_to_revert_to}' çš„ localStorage è®¾ç½®...")
            await page.goto(new_chat_url, wait_until="domcontentloaded", timeout=30000)
            await expect_async(page.locator(INPUT_SELECTOR)).to_be_visible(timeout=30000)
            logger.info(f"[{req_id}] æ¢å¤ï¼šé¡µé¢å·²å¯¼èˆªåˆ°æ–°èŠå¤©å¹¶åŠ è½½ã€‚localStorage åº”å·²è®¾ç½®ä¸ºåæ˜ æ¨¡å‹ '{model_id_to_revert_to}'ã€‚")
        else:
            logger.error(f"[{req_id}] æ¢å¤ï¼šæ— æ³•å°†æ¨¡å‹æ¢å¤åˆ°é¡µé¢æ˜¾ç¤ºçš„çŠ¶æ€ï¼Œå› ä¸ºæœªèƒ½ä»æ˜¾ç¤ºåç§° '{current_displayed_name_for_revert_stripped}' ç¡®å®šæœ‰æ•ˆæ¨¡å‹IDã€‚")
            if original_prefs_str:
                logger.warning(f"[{req_id}] æ¢å¤ï¼šä½œä¸ºæœ€ç»ˆåå¤‡ï¼Œå°è¯•æ¢å¤åˆ°åŸå§‹ localStorage: '{original_prompt_model or 'æœªè®¾ç½®'}'")
                await page.evaluate("(origPrefs) => localStorage.setItem('aiStudioUserPreference', origPrefs)", original_prefs_str)
                logger.info(f"[{req_id}] æ¢å¤ï¼šå¯¼èˆªåˆ° '{new_chat_url}' ä»¥åº”ç”¨æœ€ç»ˆåå¤‡çš„åŸå§‹ localStorageã€‚")
                await page.goto(new_chat_url, wait_until="domcontentloaded", timeout=20000)
                await expect_async(page.locator(INPUT_SELECTOR)).to_be_visible(timeout=20000)
                logger.info(f"[{req_id}] æ¢å¤ï¼šé¡µé¢å·²å¯¼èˆªåˆ°æ–°èŠå¤©å¹¶åŠ è½½ï¼Œå·²åº”ç”¨æœ€ç»ˆåå¤‡çš„åŸå§‹ localStorageã€‚")
            else:
                logger.warning(f"[{req_id}] æ¢å¤ï¼šæ— æœ‰æ•ˆçš„åŸå§‹ localStorage çŠ¶æ€å¯ä½œä¸ºæœ€ç»ˆåå¤‡ã€‚")
        return False
    except Exception as e:
        logger.exception(f"[{req_id}] âŒ åˆ‡æ¢æ¨¡å‹è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯")
        await save_error_snapshot(f"model_switch_error_{req_id}")
        try:
            if original_prefs_str:
                logger.info(f"[{req_id}] å‘ç”Ÿå¼‚å¸¸ï¼Œå°è¯•æ¢å¤ localStorage è‡³: {original_prompt_model or 'æœªè®¾ç½®'}")
                await page.evaluate("(origPrefs) => localStorage.setItem('aiStudioUserPreference', origPrefs)", original_prefs_str)
                logger.info(f"[{req_id}] å¼‚å¸¸æ¢å¤ï¼šå¯¼èˆªåˆ° '{new_chat_url}' ä»¥åº”ç”¨æ¢å¤çš„ localStorageã€‚")
                await page.goto(new_chat_url, wait_until="domcontentloaded", timeout=15000)
                await expect_async(page.locator(INPUT_SELECTOR)).to_be_visible(timeout=15000)
        except Exception as recovery_err:
            logger.error(f"[{req_id}] å¼‚å¸¸åæ¢å¤ localStorage å¤±è´¥: {recovery_err}")
        return False

# --- Load Excluded Models ---
def load_excluded_models(filename: str):
    global excluded_model_ids, logger
    excluded_file_path = os.path.join(os.path.dirname(__file__), filename)
    try:
        if os.path.exists(excluded_file_path):
            with open(excluded_file_path, 'r', encoding='utf-8') as f:
                loaded_ids = {line.strip() for line in f if line.strip()}
            if loaded_ids:
                excluded_model_ids.update(loaded_ids)
                logger.info(f"âœ… ä» '{filename}' åŠ è½½äº† {len(loaded_ids)} ä¸ªæ¨¡å‹åˆ°æ’é™¤åˆ—è¡¨: {excluded_model_ids}")
            else:
                logger.info(f"'{filename}' æ–‡ä»¶ä¸ºç©ºæˆ–ä¸åŒ…å«æœ‰æ•ˆçš„æ¨¡å‹ IDï¼Œæ’é™¤åˆ—è¡¨æœªæ›´æ”¹ã€‚")
        else:
            logger.info(f"æ¨¡å‹æ’é™¤åˆ—è¡¨æ–‡ä»¶ '{filename}' æœªæ‰¾åˆ°ï¼Œæ’é™¤åˆ—è¡¨ä¸ºç©ºã€‚")
    except Exception as e:
        logger.error(f"âŒ ä» '{filename}' åŠ è½½æ’é™¤æ¨¡å‹åˆ—è¡¨æ—¶å‡ºé”™: {e}", exc_info=True)

# --- Handle Initial Model State and Storage ---
async def _handle_initial_model_state_and_storage(page: AsyncPage):
    global current_ai_studio_model_id, logger, parsed_model_list, model_list_fetch_event, INPUT_SELECTOR
    logger.info("--- (æ–°) å¤„ç†åˆå§‹æ¨¡å‹çŠ¶æ€, localStorage å’Œ isAdvancedOpen ---")
    needs_reload_and_storage_update = False
    reason_for_reload = ""
    try:
        initial_prefs_str = await page.evaluate("() => localStorage.getItem('aiStudioUserPreference')")
        if not initial_prefs_str:
            needs_reload_and_storage_update = True
            reason_for_reload = "localStorage.aiStudioUserPreference æœªæ‰¾åˆ°ã€‚"
            logger.info(f"   åˆ¤å®šéœ€è¦åˆ·æ–°å’Œå­˜å‚¨æ›´æ–°: {reason_for_reload}")
        else:
            logger.info("   localStorage ä¸­æ‰¾åˆ° 'aiStudioUserPreference'ã€‚æ­£åœ¨è§£æ...")
            try:
                pref_obj = json.loads(initial_prefs_str)
                prompt_model_path = pref_obj.get("promptModel")
                is_advanced_open_in_storage = pref_obj.get("isAdvancedOpen")
                is_prompt_model_valid = isinstance(prompt_model_path, str) and prompt_model_path.strip()
                if not is_prompt_model_valid:
                    needs_reload_and_storage_update = True
                    reason_for_reload = "localStorage.promptModel æ— æ•ˆæˆ–æœªè®¾ç½®ã€‚"
                    logger.info(f"   åˆ¤å®šéœ€è¦åˆ·æ–°å’Œå­˜å‚¨æ›´æ–°: {reason_for_reload}")
                elif is_advanced_open_in_storage is not True:
                    needs_reload_and_storage_update = True
                    reason_for_reload = f"localStorage.isAdvancedOpen ({is_advanced_open_in_storage}) ä¸ä¸º Trueã€‚"
                    logger.info(f"   åˆ¤å®šéœ€è¦åˆ·æ–°å’Œå­˜å‚¨æ›´æ–°: {reason_for_reload}")
                else:
                    current_ai_studio_model_id = prompt_model_path.split('/')[-1]
                    logger.info(f"   âœ… localStorage æœ‰æ•ˆä¸” isAdvancedOpen=trueã€‚åˆå§‹æ¨¡å‹ ID ä» localStorage è®¾ç½®ä¸º: {current_ai_studio_model_id}")
            except json.JSONDecodeError:
                needs_reload_and_storage_update = True
                reason_for_reload = "è§£æ localStorage.aiStudioUserPreference JSON å¤±è´¥ã€‚"
                logger.error(f"   åˆ¤å®šéœ€è¦åˆ·æ–°å’Œå­˜å‚¨æ›´æ–°: {reason_for_reload}")
        if needs_reload_and_storage_update:
            logger.info(f"   æ‰§è¡Œåˆ·æ–°å’Œå­˜å‚¨æ›´æ–°æµç¨‹ï¼ŒåŸå› : {reason_for_reload}")
            logger.info("   æ­¥éª¤ 1: è°ƒç”¨ _set_model_from_page_display(set_storage=True) æ›´æ–° localStorage å’Œå…¨å±€æ¨¡å‹ ID...")
            await _set_model_from_page_display(page, set_storage=True)
            current_page_url = page.url
            logger.info(f"   æ­¥éª¤ 2: é‡æ–°åŠ è½½é¡µé¢ ({current_page_url}) ä»¥åº”ç”¨ isAdvancedOpen=true...")
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    logger.info(f"   å°è¯•é‡æ–°åŠ è½½é¡µé¢ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡): {current_page_url}")
                    await page.goto(current_page_url, wait_until="domcontentloaded", timeout=40000)
                    await expect_async(page.locator(INPUT_SELECTOR)).to_be_visible(timeout=30000)
                    logger.info(f"   âœ… é¡µé¢å·²æˆåŠŸé‡æ–°åŠ è½½åˆ°: {page.url}")
                    break  # æˆåŠŸåˆ™è·³å‡ºå¾ªç¯
                except Exception as reload_err:
                    logger.warning(f"   âš ï¸ é¡µé¢é‡æ–°åŠ è½½å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {reload_err}")
                    if attempt < max_retries - 1:
                        logger.info(f"   å°†åœ¨5ç§’åé‡è¯•...")
                        await asyncio.sleep(5)
                    else:
                        logger.error(f"   âŒ é¡µé¢é‡æ–°åŠ è½½åœ¨ {max_retries} æ¬¡å°è¯•åæœ€ç»ˆå¤±è´¥: {reload_err}. åç»­æ¨¡å‹çŠ¶æ€å¯èƒ½ä¸å‡†ç¡®ã€‚", exc_info=True)
                        await save_error_snapshot(f"initial_storage_reload_fail_attempt_{attempt+1}")
                        # Consider re-raising or handling more gracefully if critical
                        # ä¾‹å¦‚ï¼Œå¦‚æœè¿™æ˜¯å…³é”®æ­¥éª¤ï¼Œå¯èƒ½éœ€è¦: raise reload_err
            logger.info("   æ­¥éª¤ 3: é‡æ–°åŠ è½½åï¼Œå†æ¬¡è°ƒç”¨ _set_model_from_page_display(set_storage=False) ä»¥åŒæ­¥å…¨å±€æ¨¡å‹ ID...")
            await _set_model_from_page_display(page, set_storage=False)
            logger.info(f"   âœ… åˆ·æ–°å’Œå­˜å‚¨æ›´æ–°æµç¨‹å®Œæˆã€‚æœ€ç»ˆå…¨å±€æ¨¡å‹ ID: {current_ai_studio_model_id}")
        else:
            logger.info("   localStorage çŠ¶æ€è‰¯å¥½ (isAdvancedOpen=true, promptModelæœ‰æ•ˆ)ï¼Œæ— éœ€åˆ·æ–°é¡µé¢ã€‚")
    except Exception as e:
        logger.error(f"âŒ (æ–°) å¤„ç†åˆå§‹æ¨¡å‹çŠ¶æ€å’Œ localStorage æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        try:
            logger.warning("   ç”±äºå‘ç”Ÿé”™è¯¯ï¼Œå°è¯•å›é€€ä»…ä»é¡µé¢æ˜¾ç¤ºè®¾ç½®å…¨å±€æ¨¡å‹ ID (ä¸å†™å…¥localStorage)...")
            await _set_model_from_page_display(page, set_storage=False)
        except Exception as fallback_err:
            logger.error(f"   å›é€€è®¾ç½®æ¨¡å‹IDä¹Ÿå¤±è´¥: {fallback_err}")

async def _set_model_from_page_display(page: AsyncPage, set_storage: bool = False):
    global current_ai_studio_model_id, logger, parsed_model_list, model_list_fetch_event
    try:
        logger.info("   å°è¯•ä»é¡µé¢æ˜¾ç¤ºå…ƒç´ è¯»å–å½“å‰æ¨¡å‹åç§°...")
        model_name_locator = page.locator('mat-select[data-test-ms-model-selector] div.model-option-content span.gmat-body-medium')
        displayed_model_name_from_page_raw = await model_name_locator.first.inner_text(timeout=7000)
        displayed_model_name = displayed_model_name_from_page_raw.strip()
        logger.info(f"   é¡µé¢å½“å‰æ˜¾ç¤ºæ¨¡å‹åç§° (åŸå§‹: '{displayed_model_name_from_page_raw}', æ¸…ç†å: '{displayed_model_name}')")
        found_model_id_from_display = None
        if not model_list_fetch_event.is_set():
            logger.info("   ç­‰å¾…æ¨¡å‹åˆ—è¡¨æ•°æ® (æœ€å¤š5ç§’) ä»¥ä¾¿è½¬æ¢æ˜¾ç¤ºåç§°...")
            try: await asyncio.wait_for(model_list_fetch_event.wait(), timeout=5.0)
            except asyncio.TimeoutError: logger.warning("   ç­‰å¾…æ¨¡å‹åˆ—è¡¨è¶…æ—¶ï¼Œå¯èƒ½æ— æ³•å‡†ç¡®è½¬æ¢æ˜¾ç¤ºåç§°ä¸ºIDã€‚")
        if parsed_model_list:
            for model_obj in parsed_model_list:
                if model_obj.get("display_name") and model_obj.get("display_name").strip() == displayed_model_name:
                    found_model_id_from_display = model_obj.get("id")
                    logger.info(f"   æ˜¾ç¤ºåç§° '{displayed_model_name}' å¯¹åº”æ¨¡å‹ ID: {found_model_id_from_display}")
                    break
            if not found_model_id_from_display:
                 logger.warning(f"   æœªåœ¨å·²çŸ¥æ¨¡å‹åˆ—è¡¨ä¸­æ‰¾åˆ°ä¸æ˜¾ç¤ºåç§° '{displayed_model_name}' åŒ¹é…çš„ IDã€‚")
        else:
            logger.warning("   æ¨¡å‹åˆ—è¡¨å°šä¸å¯ç”¨ï¼Œæ— æ³•å°†æ˜¾ç¤ºåç§°è½¬æ¢ä¸ºIDã€‚")
        new_model_value = found_model_id_from_display if found_model_id_from_display else displayed_model_name
        if current_ai_studio_model_id != new_model_value:
            current_ai_studio_model_id = new_model_value
            logger.info(f"   å…¨å±€ current_ai_studio_model_id å·²æ›´æ–°ä¸º: {current_ai_studio_model_id}")
        else:
            logger.info(f"   å…¨å±€ current_ai_studio_model_id ('{current_ai_studio_model_id}') ä¸ä»é¡µé¢è·å–çš„å€¼ä¸€è‡´ï¼Œæœªæ›´æ”¹ã€‚")
        if set_storage:
            logger.info(f"   å‡†å¤‡ä¸ºé¡µé¢çŠ¶æ€è®¾ç½® localStorage (ç¡®ä¿ isAdvancedOpen=true)...")
            existing_prefs_for_update_str = await page.evaluate("() => localStorage.getItem('aiStudioUserPreference')")
            prefs_to_set = {}
            if existing_prefs_for_update_str:
                try:
                    prefs_to_set = json.loads(existing_prefs_for_update_str)
                except json.JSONDecodeError:
                    logger.warning("   è§£æç°æœ‰ localStorage.aiStudioUserPreference å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°çš„åå¥½è®¾ç½®ã€‚")
            prefs_to_set["isAdvancedOpen"] = True
            logger.info(f"     å¼ºåˆ¶ isAdvancedOpen: true")
            prefs_to_set["areToolsOpen"] = False
            logger.info(f"     å¼ºåˆ¶ areToolsOpen: false")
            if found_model_id_from_display:
                new_prompt_model_path = f"models/{found_model_id_from_display}"
                prefs_to_set["promptModel"] = new_prompt_model_path
                logger.info(f"     è®¾ç½® promptModel ä¸º: {new_prompt_model_path} (åŸºäºæ‰¾åˆ°çš„ID)")
            elif "promptModel" not in prefs_to_set:
                logger.warning(f"     æ— æ³•ä»é¡µé¢æ˜¾ç¤º '{displayed_model_name}' æ‰¾åˆ°æ¨¡å‹IDï¼Œä¸” localStorage ä¸­æ— ç°æœ‰ promptModelã€‚promptModel å°†ä¸ä¼šè¢«ä¸»åŠ¨è®¾ç½®ä»¥é¿å…æ½œåœ¨é—®é¢˜ã€‚")
            default_keys_if_missing = {
                "bidiModel": "models/gemini-1.0-pro-001",
                "isSafetySettingsOpen": False,
                "hasShownSearchGroundingTos": False,
                "autosaveEnabled": True,
                "theme": "system",
                "bidiOutputFormat": 3,
                "isSystemInstructionsOpen": False,
                "warmWelcomeDisplayed": True,
                "getCodeLanguage": "Node.js",
                "getCodeHistoryToggle": False,
                "fileCopyrightAcknowledged": True
            }
            for key, val_default in default_keys_if_missing.items():
                if key not in prefs_to_set:
                    prefs_to_set[key] = val_default
            await page.evaluate("(prefsStr) => localStorage.setItem('aiStudioUserPreference', prefsStr)", json.dumps(prefs_to_set))
            logger.info(f"   âœ… localStorage.aiStudioUserPreference å·²æ›´æ–°ã€‚isAdvancedOpen: {prefs_to_set.get('isAdvancedOpen')}, areToolsOpen: {prefs_to_set.get('areToolsOpen')}, promptModel: '{prefs_to_set.get('promptModel', 'æœªè®¾ç½®/ä¿ç•™åŸæ ·')}'ã€‚")
    except Exception as e_set_disp:
        logger.error(f"   å°è¯•ä»é¡µé¢æ˜¾ç¤ºè®¾ç½®æ¨¡å‹æ—¶å‡ºé”™: {e_set_disp}", exc_info=True)
